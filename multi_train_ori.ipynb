{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Train Gradient Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import threading\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from stable_baselines3 import A2C as ALGO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "NUM_CLIENT_MODELS = 2\n",
    "NUM_TRAINING_STEPS = 10_000\n",
    "NUM_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init. ENV and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "global_model = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "client_models = [ALGO(\"MlpPolicy\", gym.make('LunarLander-v2')) for i in range(NUM_CLIENT_MODELS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Evaluate Model and Train Model within Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, message = '', verbose = 0):\n",
    "    fitnesses = []\n",
    "    iterations = 2\n",
    "    for i in range(iterations):\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        if verbose == 1:\n",
    "            print(i, fitness, end=\" \")\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    mean_fitness = np.mean(sorted(fitnesses))\n",
    "    print(f'Type {message} Mean reward: {mean_fitness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, timesteps):\n",
    "    # print('Starting Training')\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    # print('Completed Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_eval(client_models):\n",
    "    # Create Threads\n",
    "    client_threads = [] \n",
    "    for ci in range(NUM_CLIENT_MODELS):\n",
    "        thread = threading.Thread(target=evaluate, args=(client_models[ci], gym.make('LunarLander-v2'), f'Trained Model {ci}'))\n",
    "        client_threads.append(thread)\n",
    "\n",
    "    # Start Threads\n",
    "    for thread in client_threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Join Threads (wait until thread is completely executed)\n",
    "    for thread in client_threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: -223.39775323503417\n",
      "Type Trained Model 0 Mean reward: -276.58416300378155\n",
      "Type Trained Model 1 Mean reward: -253.46882942284282\n"
     ]
    }
   ],
   "source": [
    "for model in client_models:\n",
    "    model.set_parameters(global_model.get_parameters())\n",
    "\n",
    "evaluate(global_model, env)\n",
    "\n",
    "multithread_eval(client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 1K Steps and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: -296.88150507626125\n",
      "Type Trained Model 1 Mean reward: -296.35482030900585\n",
      "Type Trained Model 0 Mean reward: -1733.7059948234921\n"
     ]
    }
   ],
   "source": [
    "# Create Threads\n",
    "client_threads = [] \n",
    "for i in range(NUM_CLIENT_MODELS):\n",
    "    thread = threading.Thread(target=train, args=(client_models[i], NUM_TRAINING_STEPS))\n",
    "    client_threads.append(thread)\n",
    "\n",
    "\n",
    "# Start Threads\n",
    "for thread in client_threads:\n",
    "    thread.start()\n",
    "\n",
    "# Join Threads (wait until thread is completely executed)\n",
    "for thread in client_threads:\n",
    "    thread.join()\n",
    "\n",
    "evaluate(global_model, env)\n",
    "\n",
    "multithread_eval(client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Gradient and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: -1990.1841961945347\n"
     ]
    }
   ],
   "source": [
    "global_dict = global_model.policy.state_dict()\n",
    "\n",
    "# Accumulate Client Parameters / Weights\n",
    "for k in global_dict.keys():\n",
    "    global_dict[k] = torch.stack([client_models[i].policy.state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "\n",
    "# Load New Parameters to Global Model\n",
    "global_model.policy.load_state_dict(global_dict)\n",
    "\n",
    "# Load New Parameters to clients\n",
    "for model in client_models:\n",
    "    model.policy.load_state_dict(global_model.policy.state_dict())\n",
    "\n",
    "evaluate(global_model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Global Initial Model Mean reward: -1770.0712671525544\n",
      "Train Iter:  0\n",
      "Type Trained Model 1 Mean reward: -3.6711915621364577\n",
      "Type Trained Model 0 Mean reward: -59.250647715966466\n",
      "Type Global Updated Model Mean reward: 3.0320234181764576\n",
      "Train Iter:  1\n",
      "Type Trained Model 1 Mean reward: -20.633241837677932\n",
      "Type Trained Model 0 Mean reward: 7.087361509526905\n",
      "Type Global Updated Model Mean reward: -5.288396007391956\n",
      "Train Iter:  2\n",
      "Type Trained Model 0 Mean reward: 37.9332150891186\n",
      "Type Trained Model 1 Mean reward: -50.09303499287578\n",
      "Type Global Updated Model Mean reward: -4.3426747398871965\n",
      "Train Iter:  3\n",
      "Type Trained Model 0 Mean reward: -16.676327334890093\n",
      "Type Trained Model 1 Mean reward: 31.439973291876456\n",
      "Type Global Updated Model Mean reward: 120.63703586988098\n",
      "Train Iter:  4\n",
      "Type Trained Model 1 Mean reward: 71.98086525731351\n",
      "Type Trained Model 0 Mean reward: 26.816471511568338\n",
      "Type Global Updated Model Mean reward: 25.73925326213365\n",
      "Train Iter:  5\n",
      "Type Trained Model 1 Mean reward: -75.26652213522175\n",
      "Type Trained Model 0 Mean reward: -50.77994227038871\n",
      "Type Global Updated Model Mean reward: -67.27942236643575\n",
      "Train Iter:  6\n",
      "Type Trained Model 1 Mean reward: -13.833352997674956\n",
      "Type Trained Model 0 Mean reward: -243.23013180880787\n",
      "Type Global Updated Model Mean reward: 149.91578571208865\n",
      "Train Iter:  7\n",
      "Type Trained Model 1 Mean reward: 84.00166936119392\n",
      "Type Trained Model 0 Mean reward: -1421.0123735248126\n",
      "Type Global Updated Model Mean reward: -95.5490719728603\n",
      "Train Iter:  8\n",
      "Type Trained Model 0 Mean reward: 4.403753015075921\n",
      "Type Trained Model 1 Mean reward: 57.760293372539124\n",
      "Type Global Updated Model Mean reward: 1.9603813943590644\n",
      "Train Iter:  9\n",
      "Type Trained Model 1 Mean reward: 205.24821022785244\n",
      "Type Trained Model 0 Mean reward: 13.54911163623316\n",
      "Type Global Updated Model Mean reward: 171.38390120984434\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Before Iterated Training\n",
    "evaluate(global_model, env, \"Global Initial Model\")\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    print('Train Iter: ', i)\n",
    "\n",
    "    # Create Threads\n",
    "    client_threads = [] \n",
    "    for ci in range(NUM_CLIENT_MODELS):\n",
    "        thread = threading.Thread(target=train, args=(client_models[ci], NUM_TRAINING_STEPS))\n",
    "        client_threads.append(thread)\n",
    "\n",
    "\n",
    "    # Start Threads\n",
    "    for thread in client_threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Join Threads (wait until thread is completely executed)\n",
    "    for thread in client_threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Evaluation after Training\n",
    "    multithread_eval(client_models)\n",
    "\n",
    "    # Accumulate Client Parameters / Weights\n",
    "    global_dict = global_model.policy.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].policy.state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "\n",
    "    # Load New Parameters to Global Model\n",
    "    global_model.policy.load_state_dict(global_dict)\n",
    "\n",
    "    # Load New Parameters to clients\n",
    "    for model in client_models:\n",
    "        model.policy.load_state_dict(global_model.policy.state_dict())\n",
    "\n",
    "    # Evaluate Updated Global Model\n",
    "    evaluate(model, env, 'Global Updated Model', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': OrderedDict([('mlp_extractor.policy_net.0.weight',\n",
       "               tensor([[ 1.0718e-01, -1.3142e-01,  8.1531e-02, -6.2127e-01, -3.0137e-01,\n",
       "                        -7.7027e-03,  1.3792e-01,  2.3970e-01],\n",
       "                       [-6.8553e-02, -5.0693e-02,  2.5027e-01,  6.0018e-01, -1.7604e-01,\n",
       "                        -2.1755e-01,  1.6782e-01,  3.9031e-01],\n",
       "                       [-6.4633e-02, -1.4975e-02,  3.6865e-01,  4.1997e-01, -6.1053e-01,\n",
       "                        -3.7117e-01,  1.4363e-01,  1.9571e-01],\n",
       "                       [-1.0289e-01,  1.4346e-01,  5.5555e-01,  1.1888e-01, -4.0846e-01,\n",
       "                        -3.1693e-01, -2.6111e-03, -1.6400e-01],\n",
       "                       [-1.2851e-01,  1.7242e-01, -4.8431e-01,  6.7283e-01,  2.5170e-02,\n",
       "                         4.5872e-02, -3.0708e-01,  5.8815e-02],\n",
       "                       [ 1.5102e-01,  1.4502e-01, -4.5250e-01, -1.7336e-01,  2.6811e-01,\n",
       "                         1.1459e-01, -4.4270e-01, -2.1618e-01],\n",
       "                       [-1.2058e-02, -7.9647e-02, -4.5351e-01, -6.1375e-01,  2.3873e-01,\n",
       "                        -2.0066e-01, -3.5942e-01, -7.6844e-02],\n",
       "                       [-1.1896e-01, -4.2339e-02, -7.9924e-02,  4.6969e-01,  4.1172e-02,\n",
       "                         1.6528e-01,  1.7080e-01,  5.0879e-01],\n",
       "                       [ 4.0682e-01,  9.1290e-02,  7.9299e-02, -5.6084e-01,  1.6222e-01,\n",
       "                        -1.7099e-01,  2.2000e-01, -5.9443e-02],\n",
       "                       [ 5.7889e-02,  2.2431e-01,  3.3968e-01, -3.3152e-01, -4.6052e-01,\n",
       "                        -3.1978e-01, -1.2336e-01,  5.5267e-02],\n",
       "                       [ 2.1757e-01,  7.3852e-02,  1.9907e-01, -4.5657e-01, -5.2951e-01,\n",
       "                        -1.5471e-01, -3.4833e-01, -9.2257e-02],\n",
       "                       [-5.4983e-02, -2.7521e-01,  3.4370e-01,  4.1144e-01, -2.4563e-01,\n",
       "                        -1.1148e-01, -1.1451e-01,  1.7806e-01],\n",
       "                       [-1.2081e-02, -4.3508e-02,  3.4399e-01, -5.6490e-01, -4.1638e-01,\n",
       "                        -3.5191e-02,  8.0716e-02, -6.6730e-02],\n",
       "                       [-1.9375e-01,  2.2265e-01,  4.0153e-01, -4.1247e-01, -4.6601e-01,\n",
       "                         8.5140e-03,  1.3993e-01,  1.8452e-02],\n",
       "                       [ 3.2367e-01, -1.6935e-01,  3.7778e-01, -6.2488e-01, -1.6378e-01,\n",
       "                        -1.6725e-01, -2.0763e-01, -1.8795e-02],\n",
       "                       [-1.0660e-01, -8.7001e-02,  4.7112e-01,  3.3593e-01, -1.5929e-01,\n",
       "                        -3.0457e-01, -8.5091e-02,  8.5943e-02],\n",
       "                       [ 2.2791e-01, -1.9346e-01, -4.5278e-01,  1.7751e-01,  1.0741e-01,\n",
       "                         3.4111e-01, -2.6662e-02, -6.1587e-03],\n",
       "                       [ 1.6146e-01,  2.0105e-02, -1.2602e-01, -7.6508e-01,  3.5044e-01,\n",
       "                         1.6815e-02, -1.3871e-02, -4.2316e-02],\n",
       "                       [-4.0582e-01, -5.8830e-02, -2.4173e-01,  6.6339e-01, -4.0359e-02,\n",
       "                        -4.8022e-02, -1.7791e-02, -1.1930e-01],\n",
       "                       [-2.9660e-01, -5.2772e-02, -3.7232e-01,  6.2957e-01,  2.4254e-01,\n",
       "                         1.4704e-01, -6.9672e-02,  2.0627e-03],\n",
       "                       [ 1.7867e-01, -5.0406e-02, -3.0856e-01, -3.7832e-01,  5.0160e-01,\n",
       "                         1.7474e-02, -2.2175e-01,  2.2365e-01],\n",
       "                       [ 1.3071e-01,  4.7981e-02,  4.5054e-01, -1.4606e-01, -4.5531e-01,\n",
       "                         5.6100e-02,  1.0131e-01, -4.4575e-01],\n",
       "                       [ 1.8082e-01,  2.1003e-01, -4.5208e-02, -5.8469e-01, -9.9296e-02,\n",
       "                        -1.2298e-01, -3.4833e-01, -1.1972e-03],\n",
       "                       [-2.4007e-01,  2.5381e-01, -1.6942e-02, -2.7819e-01, -5.1510e-02,\n",
       "                        -4.7923e-02, -2.7860e-01, -1.0223e-01],\n",
       "                       [ 4.5264e-01,  2.7015e-02, -3.5865e-01,  2.0457e-01,  3.2360e-01,\n",
       "                         6.1213e-01,  2.1309e-01,  1.1999e-02],\n",
       "                       [ 3.4716e-01,  6.6413e-02,  3.2259e-01, -5.4785e-01, -2.0335e-01,\n",
       "                        -3.1311e-01, -2.7435e-01, -1.6429e-03],\n",
       "                       [ 1.3489e-01,  1.2890e-01, -5.5403e-01, -5.4037e-01,  7.4434e-02,\n",
       "                         7.8033e-02,  3.3777e-02, -2.9873e-02],\n",
       "                       [-5.4384e-02, -9.1975e-02,  8.1558e-01,  4.5621e-01, -4.2939e-01,\n",
       "                        -4.9652e-02, -1.0725e-01,  2.7465e-01],\n",
       "                       [ 2.4264e-01,  2.6098e-01,  1.6507e-01, -7.0672e-01,  1.0991e-01,\n",
       "                        -2.0663e-01,  2.3800e-02,  3.3076e-02],\n",
       "                       [ 2.9207e-01, -2.3255e-01, -3.6720e-01, -6.0303e-01,  7.3223e-02,\n",
       "                         1.8437e-02, -1.3378e-01, -1.7865e-01],\n",
       "                       [ 4.1863e-01,  1.9803e-01, -3.1415e-01, -4.8621e-01,  1.7023e-01,\n",
       "                        -1.1590e-01,  5.8722e-02, -6.9106e-03],\n",
       "                       [ 1.1019e-01,  1.9690e-01, -4.1579e-01, -4.9177e-01,  2.6041e-01,\n",
       "                        -1.1123e-01,  1.5903e-01, -2.3131e-01],\n",
       "                       [-1.5813e-01,  9.1940e-04, -7.8464e-01, -3.3363e-01,  3.8654e-01,\n",
       "                         3.8315e-02,  1.7169e-01,  4.1132e-02],\n",
       "                       [-6.8105e-01, -1.9295e-01, -6.6948e-02,  7.1167e-01,  1.2265e-01,\n",
       "                         1.0106e-01, -4.2248e-02, -8.9876e-02],\n",
       "                       [ 2.5795e-01,  5.6338e-02,  2.1522e-01,  5.8032e-01, -3.9876e-01,\n",
       "                        -3.7537e-01, -9.7901e-03, -1.7238e-01],\n",
       "                       [-1.9367e-01, -3.9275e-03, -3.3494e-01,  6.7016e-01,  2.2697e-01,\n",
       "                         8.7911e-02, -2.1147e-01, -1.4422e-01],\n",
       "                       [-3.7334e-01, -3.3953e-01, -5.5212e-01,  3.8591e-01,  2.6996e-01,\n",
       "                        -1.5927e-01,  1.0334e-01,  1.1200e-01],\n",
       "                       [ 8.4135e-02, -1.3425e-01,  2.5196e-01,  1.1553e-01,  2.9839e-01,\n",
       "                         3.3725e-01, -9.6370e-02,  5.0981e-03],\n",
       "                       [-9.7650e-02, -1.7031e-01, -1.9885e-01,  2.0567e-01,  4.7661e-01,\n",
       "                         1.4393e-01,  3.0293e-01,  1.6726e-01],\n",
       "                       [ 9.8820e-02, -2.9036e-01,  3.2559e-01,  9.2078e-02,  1.5518e-02,\n",
       "                         4.2222e-02,  2.9327e-01, -2.6253e-01],\n",
       "                       [ 2.9083e-01, -5.1745e-02,  2.6959e-01,  2.5832e-01, -4.7438e-01,\n",
       "                        -3.0631e-01, -5.9647e-02, -2.0435e-01],\n",
       "                       [-3.8575e-01, -4.1771e-02,  1.2955e-01,  2.0501e-01, -3.0035e-01,\n",
       "                         3.6832e-02,  2.1224e-01,  1.0097e-01],\n",
       "                       [-3.9744e-01,  9.1702e-02, -2.5496e-01,  3.5372e-01,  3.9250e-01,\n",
       "                        -7.5061e-02,  1.0065e-01, -7.2281e-02],\n",
       "                       [ 1.1757e-01,  5.1514e-03, -4.2228e-01,  3.8175e-01,  2.8181e-01,\n",
       "                         2.0330e-01,  7.3351e-02, -8.6185e-02],\n",
       "                       [ 5.1173e-02,  5.1750e-02,  1.6564e-01, -2.9626e-01, -5.0121e-01,\n",
       "                        -7.0132e-03,  3.2394e-02, -6.9735e-02],\n",
       "                       [ 2.3046e-01, -5.7541e-02, -1.1295e-01, -7.6005e-01, -1.0035e-01,\n",
       "                         2.1440e-01, -1.1555e-01, -4.4491e-01],\n",
       "                       [ 8.9147e-03, -1.0244e-01, -4.6595e-01, -3.9429e-01,  2.5591e-01,\n",
       "                         4.3808e-01, -1.8552e-01,  7.6037e-02],\n",
       "                       [-7.1641e-02, -1.5270e-01, -3.7022e-01,  3.1825e-01,  3.5777e-01,\n",
       "                        -1.5381e-01, -1.2968e-02, -1.3331e-01],\n",
       "                       [ 4.3494e-02, -3.0688e-02,  5.1708e-01, -3.0401e-01, -5.0099e-01,\n",
       "                         1.1252e-01, -3.6769e-01,  1.9637e-01],\n",
       "                       [ 1.1344e-01, -8.0539e-02, -1.5283e-01, -9.3602e-01,  2.9309e-02,\n",
       "                         4.3492e-02, -1.2661e-01,  6.1419e-02],\n",
       "                       [-1.7407e-01,  4.9843e-02, -5.2420e-01,  2.9821e-03,  6.1023e-02,\n",
       "                         5.6489e-02, -2.3907e-01,  1.9879e-01],\n",
       "                       [-2.0548e-01,  1.2843e-01, -2.1638e-01, -2.0746e-01,  8.1907e-01,\n",
       "                         3.3384e-01, -2.3897e-01, -2.9191e-01],\n",
       "                       [ 4.1180e-02,  1.1644e-01,  5.0952e-01,  4.0589e-01, -5.8575e-01,\n",
       "                        -1.8168e-01,  2.5178e-02, -1.6287e-01],\n",
       "                       [-3.5198e-02,  1.8713e-01, -3.7924e-01, -6.5084e-01,  2.4738e-01,\n",
       "                         1.4018e-01, -1.6674e-01,  1.0229e-01],\n",
       "                       [-1.7732e-02,  2.0983e-02, -3.4593e-01, -1.3956e-01,  5.3988e-01,\n",
       "                         3.0370e-01,  3.1631e-01, -1.8282e-01],\n",
       "                       [ 1.2974e-01, -1.8714e-02,  1.6883e-02, -5.1345e-01,  4.1142e-01,\n",
       "                         1.0788e-01, -1.4743e-01, -6.2114e-02],\n",
       "                       [-3.2281e-01, -1.4580e-01, -2.2772e-01,  5.0823e-01,  3.5265e-01,\n",
       "                         1.6037e-01, -2.0229e-01,  9.7428e-02],\n",
       "                       [ 5.5264e-01,  2.7455e-03,  4.3555e-01, -4.3568e-01,  9.6674e-03,\n",
       "                         4.9187e-02, -3.5642e-01, -1.2490e-01],\n",
       "                       [ 1.9349e-01, -2.7063e-01,  4.3119e-01, -3.1591e-01, -3.1499e-01,\n",
       "                        -5.7103e-02, -3.1400e-01, -2.4267e-01],\n",
       "                       [-9.2769e-02, -6.9777e-02, -8.5344e-01,  2.0882e-01,  1.9696e-01,\n",
       "                         3.7257e-01,  1.7822e-02,  7.5440e-02],\n",
       "                       [-2.7592e-02, -1.2747e-01, -9.1541e-02, -5.2161e-01, -2.9602e-01,\n",
       "                        -3.4805e-03,  1.0423e-01, -3.9973e-01],\n",
       "                       [-2.0928e-01, -3.9366e-01,  7.1292e-02,  3.4034e-01, -1.1315e-01,\n",
       "                         2.0375e-01,  1.7121e-01,  5.6173e-02],\n",
       "                       [-3.6285e-01, -2.0157e-01, -4.0464e-01,  2.5694e-01,  3.3962e-01,\n",
       "                         2.4944e-01, -1.3018e-02, -1.2863e-01],\n",
       "                       [ 5.4621e-02,  3.8998e-02,  2.5181e-02, -3.1983e-01, -2.7927e-01,\n",
       "                         1.0352e-01,  1.5646e-01,  3.9369e-02]])),\n",
       "              ('mlp_extractor.policy_net.0.bias',\n",
       "               tensor([-0.1257,  0.1030,  0.0694,  0.0247,  0.0532, -0.0763, -0.0775,  0.0569,\n",
       "                       -0.0828, -0.0617, -0.0936,  0.0975, -0.0919,  0.0109, -0.0990,  0.0394,\n",
       "                        0.0105, -0.0724,  0.0770,  0.0896, -0.0744,  0.0728, -0.1105, -0.1170,\n",
       "                        0.1426, -0.1063, -0.0261,  0.0274, -0.1202, -0.1290, -0.0771, -0.0467,\n",
       "                       -0.0148,  0.1350,  0.0600,  0.0838,  0.0314,  0.0645,  0.0733, -0.1811,\n",
       "                        0.0146,  0.1434,  0.0616,  0.0701, -0.0489, -0.0957, -0.0662, -0.0704,\n",
       "                       -0.0459, -0.0863,  0.0782, -0.0518,  0.0549, -0.0611, -0.0260, -0.1142,\n",
       "                        0.0699, -0.0772, -0.1047,  0.0732, -0.1388,  0.1180,  0.0421, -0.0809])),\n",
       "              ('mlp_extractor.policy_net.2.weight',\n",
       "               tensor([[-0.0785,  0.4115, -0.0730,  ...,  0.1634,  0.4028, -0.2576],\n",
       "                       [ 0.2126, -0.6895, -0.5650,  ...,  0.0668, -0.1730,  0.2591],\n",
       "                       [-0.1859,  0.4128,  0.8016,  ...,  0.1950,  0.0578,  0.0568],\n",
       "                       ...,\n",
       "                       [-0.1460,  0.5708,  0.3942,  ..., -0.1568, -0.1548, -0.2152],\n",
       "                       [ 0.0030, -0.2153, -0.0456,  ..., -0.0375, -0.0562,  0.1696],\n",
       "                       [ 0.1284,  0.4430,  0.1779,  ...,  0.1548, -0.2028,  0.1137]])),\n",
       "              ('mlp_extractor.policy_net.2.bias',\n",
       "               tensor([ 0.1466, -0.0328,  0.0320, -0.0227, -0.1018, -0.0390, -0.0704, -0.0177,\n",
       "                        0.0933,  0.0705,  0.0114, -0.1217, -0.0303, -0.1558, -0.1406,  0.1021,\n",
       "                        0.0029,  0.1043, -0.0961, -0.0159,  0.0035,  0.1077, -0.1903, -0.0355,\n",
       "                        0.0539,  0.0439, -0.1388, -0.0482,  0.1726, -0.0267, -0.0696,  0.0371,\n",
       "                       -0.0675,  0.0672, -0.1335,  0.0184,  0.0119,  0.0244, -0.0463,  0.0846,\n",
       "                       -0.0340,  0.0432,  0.1180, -0.0764,  0.0509,  0.0571,  0.1259, -0.0433,\n",
       "                        0.0396, -0.0135, -0.0608,  0.0490, -0.1252, -0.0498, -0.0325, -0.1041,\n",
       "                       -0.1038,  0.1747,  0.0153,  0.0193, -0.0723, -0.0012, -0.2028,  0.0267])),\n",
       "              ('mlp_extractor.value_net.0.weight',\n",
       "               tensor([[-2.3524e-01,  4.8186e-01,  7.3088e-02,  2.0586e-01, -1.7521e-01,\n",
       "                        -7.3978e-02, -1.9151e-01,  1.2073e-01],\n",
       "                       [ 2.3274e-01, -2.2702e-01,  9.4340e-02, -4.1679e-01,  3.9939e-01,\n",
       "                        -2.3905e-01,  2.6286e-02, -7.3200e-02],\n",
       "                       [-1.7546e-01,  1.0957e-01,  1.0205e-01,  3.8844e-01, -4.0389e-01,\n",
       "                         2.5609e-01, -2.6241e-01,  4.0846e-01],\n",
       "                       [-4.2039e-01, -1.8002e-02,  2.9897e-01, -2.6280e-01, -7.3232e-01,\n",
       "                         3.0822e-01,  8.8549e-02, -1.7449e-01],\n",
       "                       [-1.7112e-01,  4.7899e-01, -4.4828e-02, -4.3291e-01, -5.3885e-02,\n",
       "                        -2.4543e-01,  2.4736e-01,  2.3530e-01],\n",
       "                       [ 2.6915e-01,  7.8736e-01,  2.1407e-01, -9.8019e-02,  1.6375e-01,\n",
       "                         4.1290e-01, -1.6437e-01,  2.4535e-02],\n",
       "                       [ 4.5545e-01, -2.9444e-02,  2.0116e-01, -3.1849e-01,  3.7113e-01,\n",
       "                         3.3534e-02, -1.0387e-02, -2.3562e-01],\n",
       "                       [-1.9600e-01,  9.7724e-02,  2.1491e-01, -9.5565e-02, -5.3672e-01,\n",
       "                         4.6743e-01,  1.5479e-01, -3.3440e-02],\n",
       "                       [-1.0885e-01, -1.0052e-01, -3.0640e-01, -5.0872e-01, -3.2100e-01,\n",
       "                        -8.2170e-02,  2.7453e-01,  4.5016e-02],\n",
       "                       [-1.6321e-01, -3.4307e-01,  4.2121e-01, -7.1011e-02, -1.8036e-01,\n",
       "                         7.0050e-01,  1.9839e-01, -2.2779e-01],\n",
       "                       [ 1.5103e-01,  4.5122e-01, -3.7109e-01, -2.6145e-01, -1.4514e-01,\n",
       "                         1.0471e-01,  2.7834e-01, -6.2241e-02],\n",
       "                       [-6.9373e-02,  2.0560e-01,  1.7369e-03, -5.5819e-01, -3.9781e-02,\n",
       "                        -1.4103e-02, -3.2635e-01, -1.2528e-01],\n",
       "                       [ 8.3257e-02,  1.2034e-01, -4.3625e-01, -7.1384e-01, -4.1343e-01,\n",
       "                         2.9199e-02, -3.4771e-01, -4.6371e-01],\n",
       "                       [-1.6572e-01, -3.2000e-01, -4.5640e-01,  2.8873e-01, -6.6559e-02,\n",
       "                        -5.6071e-01, -1.3885e-01, -9.5886e-02],\n",
       "                       [ 3.3483e-01,  8.7302e-02,  2.0334e-02,  6.2382e-02,  5.7283e-01,\n",
       "                        -3.3776e-01,  1.3065e-01, -1.2868e-01],\n",
       "                       [-4.3631e-01, -1.8079e-01, -4.5771e-02, -3.0514e-01, -1.4355e+00,\n",
       "                        -2.0089e-01,  9.7274e-02,  4.0680e-02],\n",
       "                       [-2.6996e-01, -2.2668e-02,  5.1036e-01, -2.9840e-01, -4.6608e-01,\n",
       "                         8.2803e-02, -1.8590e-02,  4.5053e-01],\n",
       "                       [-9.5614e-02,  1.8497e-01, -2.0554e-02, -5.7892e-01, -6.4897e-02,\n",
       "                         3.0847e-02, -5.4128e-01,  1.0073e-01],\n",
       "                       [ 2.5193e-01, -1.3838e-01, -6.3601e-01,  3.1784e-01,  2.5363e-01,\n",
       "                        -2.5546e-01,  1.8248e-01,  6.5164e-01],\n",
       "                       [-5.8970e-01,  1.8278e-01,  3.5517e-01,  5.0669e-01, -3.3138e-01,\n",
       "                         3.9396e-01,  3.5635e-01,  4.2172e-01],\n",
       "                       [ 3.4590e-02,  7.0533e-02, -1.3403e-01,  5.2521e-01,  5.2635e-01,\n",
       "                        -2.3967e-01,  6.3550e-02,  1.1783e-01],\n",
       "                       [ 6.8644e-02,  2.4128e-01,  3.7010e-01,  6.6772e-01,  8.9194e-01,\n",
       "                         5.2010e-02, -1.3975e-01,  3.4668e-01],\n",
       "                       [ 7.4220e-02, -3.0509e-01,  5.5724e-01,  1.9233e-01, -2.4681e-02,\n",
       "                         5.1555e-01,  3.1214e-01,  2.0106e-01],\n",
       "                       [-2.2439e-01,  8.1029e-02, -1.1056e-01, -5.4855e-01, -1.5746e-01,\n",
       "                         6.8614e-02, -4.3007e-01, -8.5427e-01],\n",
       "                       [-1.0619e-01, -4.9852e-01, -6.7797e-02,  3.0004e-01, -2.2429e-01,\n",
       "                        -9.1330e-03,  4.3552e-01,  1.8631e-01],\n",
       "                       [ 5.6079e-01, -4.7041e-02, -8.2230e-02, -6.3915e-01,  2.5793e-01,\n",
       "                        -4.2075e-02, -1.4204e-01, -3.4913e-02],\n",
       "                       [-3.9070e-02, -3.7408e-01,  7.9365e-02,  4.9200e-01,  6.4533e-01,\n",
       "                         7.8270e-01, -3.0644e-01,  6.0732e-02],\n",
       "                       [ 2.8551e-01, -2.7270e-01, -3.3689e-01, -5.8407e-01, -5.4804e-01,\n",
       "                        -5.8446e-02, -1.8649e-01, -2.7059e-01],\n",
       "                       [ 4.8905e-01, -2.1108e-01,  1.7955e-01, -8.4488e-02,  1.4938e-01,\n",
       "                        -3.6972e-01, -3.2521e-01, -3.8778e-01],\n",
       "                       [-5.1562e-01,  2.7002e-01, -2.3749e-01,  9.0621e-01,  9.4447e-02,\n",
       "                         3.1771e-01,  4.1995e-01,  4.8880e-01],\n",
       "                       [-1.0393e-01,  1.7914e-02,  1.4685e-01, -1.0794e+00, -2.2163e-01,\n",
       "                        -1.1448e-01, -4.1595e-01, -1.2174e-01],\n",
       "                       [ 1.4190e-01,  1.7982e-01, -9.3527e-02, -3.1658e-01, -6.9501e-01,\n",
       "                        -1.0044e-01, -2.2727e-01, -8.0342e-02],\n",
       "                       [-3.7442e-01,  8.4275e-02,  2.8584e-01,  2.8468e-01, -6.3045e-01,\n",
       "                         3.4637e-01,  3.3738e-01,  2.7185e-01],\n",
       "                       [ 2.5233e-01,  1.2353e-01, -3.5144e-01,  2.2602e-01,  2.7756e-01,\n",
       "                        -8.1002e-01, -3.5731e-01,  6.4378e-02],\n",
       "                       [ 1.6663e-01,  1.0305e-01, -3.4692e-02, -3.4080e-01, -4.2429e-01,\n",
       "                        -1.3677e-01,  1.6894e-01,  7.3566e-02],\n",
       "                       [-1.0000e-01, -1.3244e-01, -1.0438e-01,  9.4677e-01, -2.2698e-01,\n",
       "                        -3.9493e-01,  8.2444e-02, -5.2104e-03],\n",
       "                       [-1.8727e-02,  1.5237e-01, -1.4386e-01, -1.1031e-02, -6.0005e-01,\n",
       "                        -6.4630e-02,  2.8435e-01,  3.5241e-01],\n",
       "                       [-7.0177e-01, -2.6870e-02,  3.3997e-01, -6.1691e-02, -2.6287e-01,\n",
       "                        -1.2732e-01,  1.5148e-01, -4.0066e-02],\n",
       "                       [-1.9559e-01, -5.0926e-01, -2.0535e-01,  3.9705e-01, -4.6898e-01,\n",
       "                        -1.0015e+00, -1.1291e-01, -1.9855e-01],\n",
       "                       [-2.5910e-01,  1.4114e-01, -1.3030e-01,  3.2696e-01, -6.1844e-01,\n",
       "                         3.1929e-01,  1.6280e-01,  3.4134e-01],\n",
       "                       [ 1.9664e-01, -3.4841e-01,  1.8292e-01,  1.6013e-01,  1.3179e-01,\n",
       "                         4.0911e-01, -4.9824e-02,  1.9987e-01],\n",
       "                       [-1.4287e-03, -8.9682e-02, -2.7544e-01, -7.1802e-01, -4.1594e-01,\n",
       "                        -2.2007e-02, -2.1878e-01, -2.3806e-01],\n",
       "                       [ 1.3448e-01, -1.9029e-01, -3.3068e-03,  3.4543e-01,  1.4013e-01,\n",
       "                         2.1627e-01,  2.0010e-01,  1.2678e-01],\n",
       "                       [-3.7864e-01,  3.0068e-02,  2.0480e-02, -7.3826e-01, -1.3990e-01,\n",
       "                        -2.8147e-01,  1.3723e-01, -2.5193e-01],\n",
       "                       [ 7.3473e-01, -2.4385e-01, -2.0384e-01, -2.4606e-01,  4.0944e-01,\n",
       "                        -5.4177e-03, -3.8095e-01, -2.8499e-01],\n",
       "                       [ 4.5516e-01, -1.3720e-02, -1.8344e-01, -4.9325e-01,  2.5810e-02,\n",
       "                         3.6727e-02, -3.4229e-01, -1.4229e-01],\n",
       "                       [ 4.4710e-02,  1.3729e-01, -1.8369e-02, -8.3226e-01, -2.0322e-01,\n",
       "                         2.8277e-01, -4.2151e-01, -4.2565e-01],\n",
       "                       [ 5.8458e-02, -2.5792e-01, -3.6977e-02,  2.6508e-01,  4.4969e-02,\n",
       "                        -1.4556e-01,  2.1335e-02, -2.7513e-02],\n",
       "                       [ 2.4657e-01, -5.3397e-01,  1.6818e-01,  3.1140e-01,  8.3264e-01,\n",
       "                        -5.1339e-01, -1.9341e-01, -2.9607e-01],\n",
       "                       [-2.7821e-01, -2.7214e-02,  2.4544e-02,  6.8907e-01, -3.0955e-01,\n",
       "                         5.2268e-02,  7.1508e-02,  9.1263e-02],\n",
       "                       [-2.2990e-01,  1.4019e-01, -1.9683e-01,  2.2615e-01, -6.7252e-01,\n",
       "                         3.3594e-01,  6.7156e-01,  4.8793e-02],\n",
       "                       [ 1.6617e-01,  3.7586e-01,  1.0096e-01, -1.1557e-01, -1.6952e-01,\n",
       "                        -2.7236e-01, -2.7075e-01,  3.4679e-01],\n",
       "                       [ 3.6241e-02, -1.6008e-01, -6.1910e-02, -2.2495e-01, -7.5838e-01,\n",
       "                         1.7477e-01, -2.5719e-01, -3.0947e-01],\n",
       "                       [-6.7612e-02, -2.2875e-01, -1.6485e-01,  3.5203e-01,  5.2518e-01,\n",
       "                         2.8702e-01, -2.3533e-01,  2.2850e-02],\n",
       "                       [-4.0867e-01, -4.2301e-01, -1.1115e-01,  4.8160e-01, -5.1778e-01,\n",
       "                        -7.0024e-01,  9.0124e-02, -1.0708e-01],\n",
       "                       [ 4.5560e-03, -9.2598e-02,  4.2159e-01,  5.1602e-01,  4.6260e-01,\n",
       "                         4.9812e-01,  2.9057e-01,  3.6455e-01],\n",
       "                       [-4.2733e-01,  8.6508e-02,  3.8934e-01, -2.0402e-01, -1.0874e+00,\n",
       "                         2.4983e-01,  2.9111e-01, -1.6976e-01],\n",
       "                       [-5.1621e-01, -6.0983e-03, -1.0497e-01,  5.6373e-01, -2.3861e-01,\n",
       "                         1.2110e-02,  1.4170e-01,  1.6475e-01],\n",
       "                       [-2.4733e-01,  7.6694e-02, -8.7196e-03, -3.6835e-02, -9.8288e-01,\n",
       "                         8.6825e-02,  1.6754e-01,  8.6717e-02],\n",
       "                       [-1.9034e-01, -9.3169e-02, -2.7718e-02,  7.5090e-01,  1.3628e-01,\n",
       "                        -1.6549e-01,  2.5586e-01,  8.3616e-01],\n",
       "                       [ 2.6796e-01,  1.6887e-01,  5.4247e-02, -2.7660e-01,  3.1848e-01,\n",
       "                         1.3593e-02, -2.4189e-01, -4.1086e-01],\n",
       "                       [ 1.0615e-01, -2.1911e-01,  2.8256e-01, -5.2714e-01, -3.1330e-01,\n",
       "                         2.1912e-02, -1.7013e-01, -2.2537e-01],\n",
       "                       [ 1.3670e-01,  5.9462e-02, -9.8365e-02, -2.0392e-01,  6.5024e-01,\n",
       "                        -2.6080e-01, -1.8472e-02,  1.2523e-01],\n",
       "                       [-2.3195e-02, -1.8864e-01, -2.4831e-01,  5.8854e-01, -7.9972e-02,\n",
       "                        -6.8683e-02,  3.4951e-01,  9.8612e-02]])),\n",
       "              ('mlp_extractor.value_net.0.bias',\n",
       "               tensor([ 0.2697, -0.1978,  0.3074, -0.0143,  0.0251,  0.0284, -0.1026, -0.2185,\n",
       "                        0.1992, -0.0070, -0.0011, -0.1025, -0.2241,  0.0295,  0.2696,  0.1108,\n",
       "                        0.2747, -0.1401,  0.1280,  0.2106,  0.2192, -0.1007,  0.2932, -0.2063,\n",
       "                        0.2512,  0.0591, -0.1700,  0.1355, -0.5205, -0.0054,  0.0246,  0.1504,\n",
       "                        0.3224, -0.1190, -0.0131,  0.0944,  0.2429, -0.0322, -0.1731,  0.2587,\n",
       "                       -0.0721,  0.0487,  0.1005,  0.0014, -0.1171, -0.0345, -0.1635,  0.2020,\n",
       "                       -0.5774, -0.3184,  0.3227,  0.1467, -0.2591, -0.0388,  0.0180,  0.0293,\n",
       "                        0.2358,  0.0780, -0.0669,  0.1988, -0.2213, -0.0094,  0.3630,  0.1379])),\n",
       "              ('mlp_extractor.value_net.2.weight',\n",
       "               tensor([[-0.0220, -0.1443,  0.3958,  ..., -0.1027,  0.4217,  0.6668],\n",
       "                       [ 0.0911, -0.0106,  0.3363,  ..., -0.0367,  0.2363,  0.3570],\n",
       "                       [ 0.1958, -0.2128, -0.2084,  ...,  0.2270, -0.6288, -0.5049],\n",
       "                       ...,\n",
       "                       [-0.1391,  0.3782, -0.2062,  ...,  0.5646, -0.0811, -0.1870],\n",
       "                       [ 0.5603, -0.0051,  0.2634,  ..., -0.2722, -0.2643, -0.0078],\n",
       "                       [ 0.2388, -0.0909,  0.3456,  ..., -0.3055,  0.3803,  0.5720]])),\n",
       "              ('mlp_extractor.value_net.2.bias',\n",
       "               tensor([ 2.2208e-01,  1.9923e-01, -2.4103e-01, -1.3497e-01, -2.7559e-01,\n",
       "                       -3.2651e-01,  1.1407e-01, -1.8698e-01, -4.0696e-02, -5.5229e-02,\n",
       "                       -1.8945e-01, -1.8137e-01,  2.2129e-01, -3.7832e-01, -2.6429e-01,\n",
       "                       -1.6948e-01, -1.0657e-01, -1.5488e-01, -1.4053e-01, -3.2871e-01,\n",
       "                        4.8574e-02,  1.0120e-01, -2.4823e-01, -1.1001e-01, -2.1953e-01,\n",
       "                        1.2637e-01,  1.9687e-01, -1.0416e-01,  2.7885e-01, -2.1336e-02,\n",
       "                        9.1960e-02, -3.9977e-02,  5.1774e-02,  1.2449e-01, -9.0174e-02,\n",
       "                       -3.1051e-01, -1.4045e-01, -2.1701e-01, -2.6014e-01,  2.0281e-01,\n",
       "                       -3.9066e-01, -2.8212e-01, -2.1627e-01,  1.9042e-01,  2.9443e-01,\n",
       "                        2.0775e-01, -1.3673e-01,  1.8014e-01, -2.2573e-01,  3.8645e-04,\n",
       "                        1.0791e-02,  9.1265e-02, -3.8870e-01,  3.1800e-01,  4.8434e-02,\n",
       "                       -1.5070e-02, -9.3297e-02, -2.4972e-01, -1.9333e-01, -6.7345e-02,\n",
       "                       -1.0839e-01,  8.7569e-02, -1.0707e-02,  2.7650e-01])),\n",
       "              ('action_net.weight',\n",
       "               tensor([[ 0.3296, -0.3282,  0.2765, -0.0260, -0.4004, -0.1927, -0.1585, -0.3174,\n",
       "                         0.1400,  0.2391,  0.2549, -0.3267,  0.1164, -0.3537, -0.2118,  0.2120,\n",
       "                        -0.0139,  0.2855, -0.1682, -0.0302,  0.1105,  0.1892, -0.1489, -0.0764,\n",
       "                         0.2788,  0.2511, -0.4099, -0.1076,  0.3850, -0.0403, -0.2010,  0.0404,\n",
       "                        -0.2087,  0.1686, -0.3811,  0.0673,  0.3276,  0.0221, -0.0757,  0.3012,\n",
       "                        -0.3485,  0.2852,  0.1392, -0.2266,  0.1315,  0.2331,  0.4227, -0.1959,\n",
       "                         0.1148, -0.2479, -0.1270,  0.2104, -0.4271, -0.3850, -0.1510, -0.1636,\n",
       "                        -0.3635,  0.2937,  0.1456, -0.0291, -0.2084,  0.1877, -0.1984,  0.2609],\n",
       "                       [ 0.0878, -0.4380,  0.3089,  0.2711, -0.3065, -0.4076,  0.1560, -0.5156,\n",
       "                         0.1455, -0.0645,  0.3812, -0.1966,  0.3525, -0.0259,  0.1012, -0.1106,\n",
       "                         0.2505,  0.0148,  0.0282,  0.3799,  0.0996, -0.2217,  0.0735,  0.1001,\n",
       "                         0.2996,  0.2574, -0.1933,  0.0568,  0.0725,  0.1965, -0.0151, -0.0624,\n",
       "                        -0.2218, -0.0993, -0.2446, -0.4932,  0.3770, -0.4397,  0.0155, -0.1345,\n",
       "                        -0.4791,  0.2082, -0.0538,  0.0907,  0.0316,  0.3906,  0.1738, -0.2720,\n",
       "                        -0.0294, -0.5320, -0.0060,  0.2928, -0.1474, -0.4108,  0.0686,  0.0875,\n",
       "                         0.0807,  0.0130, -0.0199, -0.1088,  0.0756,  0.3919,  0.1250,  0.3403],\n",
       "                       [-0.3511,  0.3782, -0.2380, -0.1371,  0.4329,  0.2445,  0.2896,  0.4211,\n",
       "                         0.0041, -0.4044, -0.3127,  0.2601, -0.2657,  0.3542,  0.1249, -0.2451,\n",
       "                        -0.0900, -0.4092,  0.3291, -0.2020, -0.1468, -0.1327,  0.0846,  0.2425,\n",
       "                        -0.3002, -0.2152,  0.4192,  0.3443, -0.3460,  0.0565,  0.3091, -0.2781,\n",
       "                         0.1531, -0.3391,  0.4418,  0.2217, -0.3567,  0.2182,  0.3669, -0.4219,\n",
       "                         0.4254, -0.2173, -0.2223,  0.4055,  0.1930, -0.2914, -0.3672,  0.1127,\n",
       "                        -0.3451,  0.3740,  0.2688, -0.3242,  0.3877,  0.4095,  0.3106,  0.2767,\n",
       "                         0.4864, -0.3402, -0.3249, -0.0039,  0.3406, -0.2617, -0.0420, -0.2901],\n",
       "                       [ 0.1592,  0.1602, -0.2092, -0.1740,  0.0261,  0.1853, -0.4737,  0.1598,\n",
       "                        -0.1721,  0.4377, -0.1715,  0.0848, -0.1444, -0.1249, -0.1477,  0.3931,\n",
       "                        -0.0767,  0.3826, -0.4154, -0.1153, -0.1104,  0.2610, -0.0152, -0.4134,\n",
       "                        -0.1439, -0.2385, -0.0025, -0.4720,  0.1166, -0.1921, -0.2229,  0.4470,\n",
       "                         0.1542,  0.4372,  0.0146,  0.1029, -0.1591,  0.1071, -0.5399,  0.4808,\n",
       "                         0.1232, -0.1634,  0.2927, -0.5115, -0.4015, -0.1516, -0.0208,  0.2195,\n",
       "                         0.4687,  0.1610, -0.2613, -0.0974, -0.0008,  0.1266, -0.4214, -0.3802,\n",
       "                        -0.4810,  0.0552,  0.3500,  0.1528, -0.4323, -0.1748,  0.0401, -0.1912]])),\n",
       "              ('action_net.bias',\n",
       "               tensor([ 0.1158, -0.0573, -0.0149,  0.0183])),\n",
       "              ('value_net.weight',\n",
       "               tensor([[-1.2071, -1.2218,  1.3238, -1.2145,  1.4719,  1.6707,  1.3202,  1.1580,\n",
       "                        -1.4452,  1.3994,  1.5298,  1.4972, -1.5765,  1.6108,  1.6349,  1.4747,\n",
       "                         1.3554,  1.3164,  1.4326,  1.5139, -1.3480, -1.7808,  1.5761,  1.3685,\n",
       "                         1.3139, -1.2005, -1.1540, -1.4273, -1.3348, -1.2774, -1.3667, -1.2746,\n",
       "                        -1.8325, -1.2027,  1.3638,  1.1856, -1.2599,  1.0915,  1.1789, -1.5981,\n",
       "                         1.3288,  1.5059,  1.1541, -1.7223, -1.8695, -1.2299,  1.2430, -1.1004,\n",
       "                         1.3052,  1.4277,  1.3435, -1.2949,  1.5531, -1.4043, -1.2089,  1.3045,\n",
       "                         1.0932,  1.6801, -1.3391,  1.8484,  1.9006, -1.3901,  1.3702, -1.6045]])),\n",
       "              ('value_net.bias', tensor([0.3510]))]),\n",
       " 'policy.optimizer': {'state': {},\n",
       "  'param_groups': [{'lr': 0.0007,\n",
       "    'momentum': 0,\n",
       "    'alpha': 0.99,\n",
       "    'eps': 1e-05,\n",
       "    'centered': False,\n",
       "    'weight_decay': 0,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save('a2c_lunar_multiproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Params as JSON\n",
    "## Function to Convert Params Dict to Flattened List\n",
    "def flatten_list(params):\n",
    "    \"\"\"\n",
    "    :param params: (dict)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    params_ = {}\n",
    "    for key in params.keys():\n",
    "        params_[key] = params[key].tolist()\n",
    "    return params_\n",
    "## Write Parameters to JSON File\n",
    "import json\n",
    "\n",
    "all_params = global_model.get_parameters()\n",
    "pol_params = flatten_list(all_params['policy'])\n",
    "\n",
    "all_params['policy'] = pol_params\n",
    "\n",
    "with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "    json.dump(all_params, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loaded = ALGO(\n",
    "#     \"MlpPolicy\",\n",
    "#     env\n",
    "# )\n",
    "\n",
    "# evaluate(model_loaded,env, verbose=1)\n",
    "\n",
    "# import json\n",
    "# with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "#     new_params = json.load(f)\n",
    "\n",
    "# loaded_pol_params = new_params['policy']\n",
    "# for key in loaded_pol_params.keys():\n",
    "#     loaded_pol_params[key] = th.tensor(loaded_pol_params[key])\n",
    "\n",
    "# new_params['policy'] = loaded_pol_params\n",
    "\n",
    "# model_loaded.set_parameters(new_params)\n",
    "\n",
    "model_loaded = ALGO.load('a2c_lunar_multiproc', env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 158.35763339629293 1 156.59294999945695 Type  Mean reward: 157.47529169787492\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "evaluate(model_loaded,env, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
