{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Train Gradient Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import threading\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import A2C as ALGO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init. ENV and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_1 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_2 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Evaluate Model and Train Model within Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, message = '', verbose = 0):\n",
    "    fitnesses = []\n",
    "    iterations = 10\n",
    "    for i in range(iterations):\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        if verbose == 1:\n",
    "            print(i, fitness, end=\" \")\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    mean_fitness = np.mean(sorted(fitnesses))\n",
    "    print(f'Type {message} Mean reward: {mean_fitness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, timesteps):\n",
    "    print('Starting Training')\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    print('Completed Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 9.27\n",
      "Type  Mean reward: 9.400000000000002\n",
      "Type  Mean reward: 9.370000000000001\n"
     ]
    }
   ],
   "source": [
    "model_trained_1.set_parameters(model.get_parameters())\n",
    "model_trained_2.set_parameters(model.get_parameters())\n",
    "\n",
    "evaluate(model, env)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 1K Steps and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "Type  Mean reward: 61.3\n",
      "Type  Mean reward: 46.589999999999996\n",
      "Type  Mean reward: 9.34\n"
     ]
    }
   ],
   "source": [
    "# Train MT Model 1\n",
    "t1 = threading.Thread(target=train, args=(model_trained_1, 10_00))\n",
    "\n",
    "# Train MT Model 2\n",
    "t2 = threading.Thread(target=train, args=(model_trained_2, 10_00))\n",
    "\n",
    "# starting thread\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until thread is completely executed\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "\n",
    "# model_trained.learn(total_timesteps=10_00)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Gradient and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 57.23\n"
     ]
    }
   ],
   "source": [
    "# For Trained Model 1\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "\n",
    "# For Trained Model 2\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model_trained_1.policy.load_state_dict(state_dict)\n",
    "model_trained_2.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iter:  0\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 63.9 1 58.5 2 62.6 3 53.5 4 72.9 5 54.9 6 62.8 7 72.9 8 63.4 9 63.0 Type Trained Model 1 Mean reward: 62.839999999999996\n",
      "0 61.2 1 53.5 2 72.5 3 54.7 4 67.7 5 50.8 6 62.7 7 55.9 8 80.7 9 67.9 Type Trained Model 2 Mean reward: 62.760000000000005\n",
      "0 57.3 1 64.6 2 60.9 3 58.4 4 58.8 5 54.1 6 75.5 7 55.6 8 54.6 9 61.7 Type Initial Model Mean reward: 60.15\n",
      "0 59.3 1 60.2 2 67.0 3 84.3 4 66.3 5 79.1 6 79.3 7 54.8 8 96.1 9 80.4 Type Updated Model Mean reward: 72.67999999999999\n",
      "Train Iter:  1\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 49.4 1 50.5 2 69.4 3 72.1 4 73.2 5 58.9 6 56.1 7 56.7 8 65.2 9 42.3 Type Trained Model 1 Mean reward: 59.38000000000001\n",
      "0 70.2 1 61.6 2 64.8 3 66.6 4 65.8 5 56.1 6 52.5 7 69.2 8 69.8 9 68.3 Type Trained Model 2 Mean reward: 64.49\n",
      "0 55.5 1 57.2 2 51.9 3 51.3 4 62.1 5 93.5 6 68.7 7 65.0 8 57.6 9 70.5 Type Initial Model Mean reward: 63.33\n",
      "0 67.8 1 67.5 2 70.5 3 52.0 4 101.8 5 75.5 6 57.6 7 75.8 8 57.2 9 61.1 Type Updated Model Mean reward: 68.67999999999999\n",
      "Train Iter:  2\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 54.8 1 64.0 2 48.7 3 70.9 4 58.9 5 75.4 6 54.5 7 58.2 8 47.1 9 53.8 Type Trained Model 1 Mean reward: 58.629999999999995\n",
      "0 62.7 1 58.0 2 57.0 3 67.5 4 71.6 5 59.1 6 59.0 7 67.7 8 78.1 9 50.5 Type Trained Model 2 Mean reward: 63.120000000000005\n",
      "0 55.8 1 61.9 2 104.3 3 58.5 4 56.0 5 104.8 6 69.7 7 64.7 8 84.3 9 66.4 Type Initial Model Mean reward: 72.63999999999999\n",
      "0 62.6 1 119.3 2 64.6 3 60.1 4 52.8 5 63.7 6 59.9 7 60.3 8 101.3 9 68.0 Type Updated Model Mean reward: 71.25999999999999\n",
      "Train Iter:  3\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 55.5 1 58.6 2 62.7 3 59.2 4 69.8 5 52.1 6 59.9 7 62.1 8 66.3 9 54.7 Type Trained Model 1 Mean reward: 60.089999999999996\n",
      "0 55.9 1 57.0 2 64.8 3 62.9 4 66.1 5 55.4 6 47.8 7 67.8 8 61.0 9 78.1 Type Trained Model 2 Mean reward: 61.67999999999999\n",
      "0 61.6 1 102.5 2 61.6 3 64.3 4 67.9 5 64.6 6 67.7 7 60.5 8 64.2 9 55.9 Type Initial Model Mean reward: 67.08000000000001\n",
      "0 58.9 1 65.2 2 77.1 3 71.4 4 70.1 5 60.9 6 59.1 7 70.4 8 103.0 9 59.8 Type Updated Model Mean reward: 69.59\n",
      "Train Iter:  4\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 52.7 1 65.3 2 47.5 3 65.4 4 65.9 5 51.2 6 58.2 7 60.5 8 51.1 9 55.1 Type Trained Model 1 Mean reward: 57.29\n",
      "0 79.6 1 76.2 2 82.2 3 86.4 4 61.2 5 86.7 6 90.1 7 77.2 8 85.9 9 69.6 Type Trained Model 2 Mean reward: 79.51000000000002\n",
      "0 56.2 1 65.1 2 72.2 3 82.2 4 62.4 5 61.5 6 56.6 7 63.4 8 64.9 9 69.0 Type Initial Model Mean reward: 65.35000000000001\n",
      "0 58.5 1 62.1 2 63.5 3 67.0 4 60.2 5 74.7 6 73.2 7 68.6 8 70.5 9 80.1 Type Updated Model Mean reward: 67.84\n",
      "Train Iter:  5\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 47.9 1 53.4 2 47.4 3 96.5 4 55.0 5 53.6 6 55.6 7 58.5 8 55.3 9 47.4 Type Trained Model 1 Mean reward: 57.06\n",
      "0 69.8 1 61.4 2 68.9 3 57.5 4 73.3 5 85.5 6 67.2 7 79.6 8 57.4 9 59.5 Type Trained Model 2 Mean reward: 68.01\n",
      "0 63.9 1 69.2 2 72.1 3 66.3 4 69.1 5 65.9 6 64.9 7 72.7 8 59.9 9 58.4 Type Initial Model Mean reward: 66.24000000000001\n",
      "0 62.2 1 73.9 2 80.4 3 67.1 4 62.8 5 60.5 6 68.5 7 57.5 8 63.1 9 67.6 Type Updated Model Mean reward: 66.35999999999999\n",
      "Train Iter:  6\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 66.8 1 89.1 2 68.3 3 80.0 4 87.3 5 65.7 6 65.6 7 65.9 8 72.4 9 71.0 Type Trained Model 1 Mean reward: 73.21000000000001\n",
      "0 78.6 1 78.9 2 73.3 3 76.3 4 89.1 5 111.9 6 74.6 7 63.1 8 72.2 9 96.7 Type Trained Model 2 Mean reward: 81.47\n",
      "0 62.4 1 58.8 2 75.4 3 60.7 4 61.6 5 66.8 6 61.9 7 66.4 8 64.9 9 59.7 Type Initial Model Mean reward: 63.86\n",
      "0 60.1 1 60.3 2 72.9 3 57.5 4 59.9 5 55.9 6 60.8 7 64.6 8 55.4 9 62.5 Type Updated Model Mean reward: 60.989999999999995\n",
      "Train Iter:  7\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 85.7 1 63.4 2 86.7 3 77.4 4 76.3 5 48.5 6 62.9 7 47.8 8 57.1 9 78.2 Type Trained Model 1 Mean reward: 68.4\n",
      "0 84.8 1 69.5 2 71.4 3 90.5 4 77.8 5 94.7 6 97.2 7 73.1 8 69.5 9 80.5 Type Trained Model 2 Mean reward: 80.9\n",
      "0 63.6 1 62.9 2 61.0 3 56.7 4 64.0 5 63.3 6 64.4 7 62.9 8 69.9 9 59.2 Type Initial Model Mean reward: 62.79\n",
      "0 59.7 1 72.2 2 69.0 3 55.0 4 67.9 5 78.0 6 62.2 7 53.1 8 61.5 9 53.8 Type Updated Model Mean reward: 63.24000000000001\n",
      "Train Iter:  8\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 58.9 1 68.6 2 55.3 3 68.7 4 86.6 5 64.5 6 59.8 7 67.0 8 59.3 9 58.2 Type Trained Model 1 Mean reward: 64.69\n",
      "0 73.9 1 75.8 2 80.5 3 78.5 4 84.7 5 80.0 6 90.7 7 65.1 8 67.3 9 62.7 Type Trained Model 2 Mean reward: 75.92\n",
      "0 80.9 1 75.9 2 63.5 3 64.2 4 56.5 5 58.1 6 76.1 7 71.2 8 63.1 9 64.6 Type Initial Model Mean reward: 67.41\n",
      "0 65.1 1 62.5 2 60.4 3 64.8 4 66.6 5 64.2 6 75.1 7 58.0 8 84.5 9 79.2 Type Updated Model Mean reward: 68.04\n",
      "Train Iter:  9\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 61.5 1 68.7 2 81.5 3 70.8 4 52.5 5 64.8 6 51.0 7 61.3 8 67.5 9 59.8 Type Trained Model 1 Mean reward: 63.94\n",
      "0 79.8 1 84.1 2 81.8 3 96.9 4 70.5 5 88.6 6 73.5 7 77.8 8 64.0 9 65.6 Type Trained Model 2 Mean reward: 78.26\n",
      "0 62.6 1 65.7 2 72.6 3 61.1 4 82.9 5 81.6 6 61.4 7 74.7 8 66.3 9 93.4 Type Initial Model Mean reward: 72.22999999999999\n",
      "0 55.6 1 70.5 2 64.9 3 70.8 4 57.9 5 69.2 6 60.5 7 58.5 8 67.9 9 79.8 Type Updated Model Mean reward: 65.55999999999999\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Train Iter: ', i)\n",
    "\n",
    "    # Train MT Model 1\n",
    "    t1 = threading.Thread(target=train, args=(model_trained_1, 10_0))\n",
    "\n",
    "    # Train MT Model 2\n",
    "    t2 = threading.Thread(target=train, args=(model_trained_2, 10_0))\n",
    "\n",
    "    # starting thread\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # wait until thread is completely executed\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    evaluate(model_trained_1, env, 'Trained Model 1', verbose=1)\n",
    "    evaluate(model_trained_2, env, 'Trained Model 2', verbose=1)\n",
    "    evaluate(model, env, 'Initial Model', verbose=1)\n",
    "\n",
    "    # For Trained Model 1\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "    # For Trained Model 2\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    evaluate(model, env, 'Updated Model', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': OrderedDict([('mlp_extractor.policy_net.0.weight',\n",
       "               tensor([[-13.6661,  -1.6162,   5.2286,  -3.7724],\n",
       "                       [ 15.5470,  -8.9814,  10.4736, -11.1178],\n",
       "                       [  2.0127,  -2.3448,  -1.4312,   1.2207],\n",
       "                       [ -0.9364,   6.6811,  -3.2110,   2.2838],\n",
       "                       [ -1.8378, -16.7624,  -4.6042,  16.0163],\n",
       "                       [-15.0264,   1.9382,   8.2095,  12.7957],\n",
       "                       [ 22.8689,   1.7694,  11.2260,   0.9689],\n",
       "                       [  7.0476,  16.5845,  -8.6652,  -7.1861],\n",
       "                       [-13.0648,  11.9390,   8.2493,   2.3556],\n",
       "                       [  8.0456,   5.6632,  -3.0767,  -3.3188],\n",
       "                       [  4.1010,  10.4970,   5.0210, -30.8385],\n",
       "                       [  5.4770,  -2.7635,  -9.2190,  -7.0158],\n",
       "                       [ -0.5031,  -8.9173,  14.9004,  15.6148],\n",
       "                       [-14.1457,  20.2595,  -4.5194,   0.0946],\n",
       "                       [  3.6887, -12.7706,  17.3138,  16.1286],\n",
       "                       [  2.3719,  17.5085, -13.7742,  -1.6605],\n",
       "                       [ -8.4626,  -0.4087,  -5.6266,  -7.8311],\n",
       "                       [-15.6777,   1.4992,  -9.4093,  -7.2515],\n",
       "                       [ -3.0958,  -0.6547,  -1.4371, -23.4965],\n",
       "                       [ -7.8979,  -7.7213,  24.8769,  -2.7374],\n",
       "                       [ -5.5695,   6.3513,   4.2377,  -0.9290],\n",
       "                       [-12.4420,  -1.8003,  31.8634,  -0.1940],\n",
       "                       [ 13.2155,  -5.6393, -18.9646,   4.9293],\n",
       "                       [-20.3831,   1.5283,   0.2987,   8.2860],\n",
       "                       [ 14.8344, -14.1406, -14.5791,  -5.6380],\n",
       "                       [ -6.7219,  20.6424, -13.2360, -17.0216],\n",
       "                       [-13.4475,  11.2118,  14.8149, -20.1671],\n",
       "                       [ -8.8895, -33.4726, -13.6230,  -2.5565],\n",
       "                       [-17.3406,   8.6935,   1.6528,   0.6160],\n",
       "                       [ -7.4251, -11.8235,   7.4946, -15.4225],\n",
       "                       [ 11.3035,   8.2000,   1.1655,  -8.0665],\n",
       "                       [ -1.7682,   8.5081,  -7.7078,   5.3212],\n",
       "                       [ 21.5306,  10.8143,  -4.5348, -15.2759],\n",
       "                       [  7.3143,  17.0951,  -2.3514,   6.7698],\n",
       "                       [  2.5397,  -2.8196,  10.4061,  12.5818],\n",
       "                       [ 12.1157, -10.9001,  -7.5198, -10.7210],\n",
       "                       [ -0.8199, -12.5379,  -1.7537,  -2.1963],\n",
       "                       [ -1.1502,   8.5941,   8.2295,  20.3790],\n",
       "                       [ -2.8686,  -3.2633,  20.6778,  -1.3528],\n",
       "                       [  7.6985, -17.2076,   9.6894,  -9.7590],\n",
       "                       [ 25.4522,   7.2033,   2.8437,  12.3364],\n",
       "                       [ -7.8218, -15.9350,  -8.2332,  32.3028],\n",
       "                       [  5.9089,   2.1767,  -7.8153,  10.1084],\n",
       "                       [  9.0312,  -4.7009,  -1.4022,  -3.1847],\n",
       "                       [ 16.5826,  -5.9746,  23.4523,   6.5658],\n",
       "                       [-22.6132,  -2.7622, -12.1900,  19.2364],\n",
       "                       [  8.8382,   5.5231,   7.6095,  13.9269],\n",
       "                       [ -9.0961, -16.3201,   2.1531, -12.3259],\n",
       "                       [ -0.4918,   5.0580,  -7.5869,   9.0554],\n",
       "                       [  3.8481,   7.3516,  -4.7288,   4.0294],\n",
       "                       [ -5.2132,  10.6125, -20.0655,   7.5416],\n",
       "                       [  7.1219,  -5.8595,  -8.3533,  -5.0044],\n",
       "                       [  1.7311,   2.5099,   5.1981,  -5.4316],\n",
       "                       [  3.2606,  21.3592,  -5.1315,   1.5209],\n",
       "                       [ -5.9657,   1.1647,  10.5715,  -5.1117],\n",
       "                       [-12.4991,   1.6690,  -6.8497,  -6.0630],\n",
       "                       [ 16.3493,   7.3484,  11.0385,   3.8477],\n",
       "                       [ 11.1210, -16.3082, -10.4322, -15.7525],\n",
       "                       [-10.5747,  -6.0812, -11.4448,   2.1325],\n",
       "                       [  4.8997,  -1.5106,   9.0343,  21.5415],\n",
       "                       [ 16.4037,  13.3246,   1.6475,   9.0478],\n",
       "                       [  3.1191, -11.0078, -10.2248,   9.7125],\n",
       "                       [  1.3080, -16.8177, -13.7615,   0.6663],\n",
       "                       [ -8.4858,  -9.9333,  10.1865,  -3.9144]])),\n",
       "              ('mlp_extractor.policy_net.0.bias',\n",
       "               tensor([-0.3921,  1.1594, -0.8338,  0.5410, -0.9620, -1.3597, -0.5796,  0.8513,\n",
       "                       -0.3664,  0.9050,  0.8973,  1.0868, -0.9987,  0.6120, -1.0855,  0.9205,\n",
       "                        1.1010,  0.8178,  1.0296, -0.1773,  0.8760, -0.9930,  0.7329, -1.1134,\n",
       "                        0.7190,  0.9738,  1.0451, -0.8441, -0.6058,  0.4968,  1.3054,  0.3420,\n",
       "                        0.9895, -0.3323, -1.2041,  1.1628,  0.0284, -1.1666, -1.0701, -0.4711,\n",
       "                       -0.8514, -0.9432, -0.9020, -0.3036, -0.8353, -1.0761, -1.0708,  0.2567,\n",
       "                       -1.0471,  0.0383,  0.9563,  1.1744,  0.7533,  0.7722,  0.6042, -0.3027,\n",
       "                        0.6306,  1.3305, -0.5583, -1.0298, -0.8558, -0.9497, -0.7484, -0.6048])),\n",
       "              ('mlp_extractor.policy_net.2.weight',\n",
       "               tensor([[ -4.2315,  -7.8145,   9.6502,  ...,  -0.4578,   4.3584, -22.8227],\n",
       "                       [ -6.1390,  21.1788,  10.2314,  ...,   4.3181, -11.8116,  10.1295],\n",
       "                       [ -5.9973, -18.2332,  10.4289,  ...,  -2.5922,  19.9613,   7.2010],\n",
       "                       ...,\n",
       "                       [  1.1866, -18.3137,  -5.1030,  ...,  -3.0939,  -6.0938, -10.3826],\n",
       "                       [ -0.8852,   1.6227,   2.1828,  ...,  17.5635,   9.6029,  -9.4761],\n",
       "                       [ -3.2284,   2.8219,  -5.2965,  ...,  -4.8927, -20.5034,  -7.7142]])),\n",
       "              ('mlp_extractor.policy_net.2.bias',\n",
       "               tensor([-0.8639,  0.9525,  0.9646, -0.8072,  0.9407, -1.0687,  0.6789,  0.7864,\n",
       "                        0.8653, -0.8086,  0.3420, -0.9971,  0.8300, -0.2328,  0.9428, -0.9832,\n",
       "                        0.7957, -0.7385,  0.8277,  0.9332,  0.0208,  0.8851,  0.8329,  1.0532,\n",
       "                       -0.9025,  0.7035, -0.6100, -0.8861,  0.1583,  0.9738, -0.7638,  0.9602,\n",
       "                        0.2046,  0.9206,  0.8361, -0.8640, -0.7633,  0.8454,  1.0788,  0.5494,\n",
       "                        0.0284,  0.7502,  0.8174,  0.8251,  0.6975,  0.4934, -0.0195, -0.7437,\n",
       "                       -0.8200,  0.8364,  0.7336, -0.6536, -0.7990, -0.5336,  0.7972,  0.7764,\n",
       "                        0.8927,  0.8361, -0.8326, -0.0369,  0.8868, -0.9116, -0.8673, -0.5505])),\n",
       "              ('mlp_extractor.value_net.0.weight',\n",
       "               tensor([[ 16.9953,   0.4149,   0.6602,   6.2321],\n",
       "                       [-18.0101, -10.7196,   5.3122,  -7.1999],\n",
       "                       [  3.7523,   8.7854,  -3.5326,  10.3034],\n",
       "                       [-19.3496,  -0.4919,   2.3757, -11.5268],\n",
       "                       [-14.3000,  13.6220,  -7.3600,  15.3264],\n",
       "                       [ 18.5114,  -6.4946,  14.7482, -24.1791],\n",
       "                       [ -8.1165, -17.6872,  10.1863,  11.3879],\n",
       "                       [  9.4040,   0.4604,   4.7989,  -5.3649],\n",
       "                       [ 14.1845,  -0.4413,  14.7607,   1.8849],\n",
       "                       [ -2.1640,  -7.3665,   2.9388,   3.3788],\n",
       "                       [ -7.7133, -12.7644, -12.2515,  -4.2892],\n",
       "                       [ -7.4825, -10.4049,  35.0763,   4.7808],\n",
       "                       [ -4.5995,  -2.8273,   5.9739,   0.8652],\n",
       "                       [ 19.2625,  11.2512,   8.0585,  -1.7447],\n",
       "                       [-10.4949,  -4.7533,   2.9909,   9.5738],\n",
       "                       [  5.0252, -14.8089,   4.7457,   0.9622],\n",
       "                       [  0.5802,  17.2530,  -8.9513,   0.1565],\n",
       "                       [-12.5539,   5.5359,   7.8771, -27.1586],\n",
       "                       [ -4.3683,   1.8277,  -7.8600,  -1.3459],\n",
       "                       [ -6.4417,  11.8600,  16.2066,  24.3429],\n",
       "                       [ 27.7697, -10.9951,   4.9513,  21.1304],\n",
       "                       [-15.5693,  11.9144,  -3.1430,   5.9455],\n",
       "                       [ -7.0011,  -3.5804, -21.0644,  -9.4153],\n",
       "                       [ -2.4280,  -3.3332,   3.3128,  22.4420],\n",
       "                       [ -1.5985, -25.1492,  -2.1057, -15.1096],\n",
       "                       [ 11.2027,   2.0022,  -7.3424,  -5.1128],\n",
       "                       [ -8.6558,   7.5773,  11.1831,   7.8679],\n",
       "                       [  8.6024,   6.3135,  -9.5451,  -4.9188],\n",
       "                       [-10.0923,   5.8940,  14.7858,  11.3787],\n",
       "                       [  0.2834,  15.4138,   9.8363,  -8.7509],\n",
       "                       [  9.0284,  -4.7366, -13.5027,   1.0799],\n",
       "                       [ -7.4228, -19.5050,   6.5158,  -1.9104],\n",
       "                       [-35.5974,   3.2427, -11.4592,   3.5464],\n",
       "                       [ 13.2904, -10.6167,   2.7202,   4.3412],\n",
       "                       [-18.9053,  -7.2770,   5.8754,  -1.2050],\n",
       "                       [  1.5021, -12.8397,   1.9170,   7.8455],\n",
       "                       [ -3.2423, -12.2085,   3.1904, -14.3700],\n",
       "                       [ -0.2155, -10.6914,   1.7208, -11.8528],\n",
       "                       [ -2.6885, -23.5609, -11.2746,   6.0083],\n",
       "                       [ -5.9457, -24.6165,   8.3850,   4.9385],\n",
       "                       [  8.9018,  -0.1014,  -1.0813, -16.2673],\n",
       "                       [-13.9210,  13.2484,   7.6898,   1.5107],\n",
       "                       [ -3.7009, -11.1403, -19.5708,  -0.4885],\n",
       "                       [ 14.8478,  -2.7518,   5.5065,  18.6020],\n",
       "                       [  3.4229,   6.9520,  12.9225,  -1.0304],\n",
       "                       [ -3.2769,  -2.2428,  -4.9837,  27.3469],\n",
       "                       [  3.9493,  -0.2903,  -6.1379,   8.0907],\n",
       "                       [  1.7943, -16.5161,  10.9860,   9.9783],\n",
       "                       [ -8.0223,  11.3149, -13.3854,  -2.3215],\n",
       "                       [ -7.0050,   1.0384,  11.0205, -10.3155],\n",
       "                       [ -2.2861,  -3.9789,  18.3379,  -6.5133],\n",
       "                       [ 11.8861,   4.5531,  -1.3356,  -0.9672],\n",
       "                       [ -8.5498,  11.2786,   9.7873,   1.0075],\n",
       "                       [ -5.0518,   1.2232,  23.2693,  -0.0772],\n",
       "                       [  2.9637, -12.6447,  -6.0200,  10.3297],\n",
       "                       [ -1.3229,   6.1677, -16.6397,  16.9139],\n",
       "                       [  1.7804,   1.7090,  -4.7199,  -5.8284],\n",
       "                       [-10.7090, -15.1673,  -4.9652,   9.5965],\n",
       "                       [ -3.4011, -18.7626, -13.0841,  -7.5089],\n",
       "                       [  1.2116,   0.8293,   7.6512,  -1.2099],\n",
       "                       [ -3.1022,  -2.9778,  -3.6132,  -2.8093],\n",
       "                       [  7.2058, -15.3792,  -7.4550,  -5.9938],\n",
       "                       [  6.5327,   7.4690,   4.6922,   4.6122],\n",
       "                       [ -8.4380,  -4.3208,  22.5283,  -7.4416]])),\n",
       "              ('mlp_extractor.value_net.0.bias',\n",
       "               tensor([ 7.3451,  7.5402, -8.0405,  7.7719, -1.6455, -7.2339,  6.1334,  7.5654,\n",
       "                       -7.1110,  7.4130, -8.2086, -7.2433,  7.3472, -8.1871, -8.0124, -8.0080,\n",
       "                       -8.2190, -5.5294, -8.1083,  6.9958,  6.6148,  8.1326,  6.9388,  7.1906,\n",
       "                       -7.7684, -7.2513,  6.7766,  7.3814, -7.8630, -6.3425, -7.9112,  7.0754,\n",
       "                       -8.0236, -6.2178,  7.8860,  5.9257,  7.9353, -8.4290,  6.5146,  5.3336,\n",
       "                       -7.7919,  7.3837, -7.5374,  6.6865, -7.9270, -6.2957,  7.1388,  6.2150,\n",
       "                       -7.5748, -6.8111, -7.9984, -8.4227,  7.5501, -7.1261, -7.5325, -7.0036,\n",
       "                        7.0617, -7.6388, -7.6208, -7.4451,  7.7840, -8.3262, -8.7937, -7.8582])),\n",
       "              ('mlp_extractor.value_net.2.weight',\n",
       "               tensor([[ -9.1614, -10.1372,  17.7161,  ...,  13.3464,   2.6135,  -0.2241],\n",
       "                       [ 21.5860,  -8.6763,   6.8127,  ..., -14.9677, -18.5169, -14.7242],\n",
       "                       [ 18.3936,  13.2813,   7.3070,  ..., -29.8288, -28.9436, -11.0661],\n",
       "                       ...,\n",
       "                       [ 27.3883,   0.8227, -26.1177,  ...,   3.8852, -13.3570, -11.8595],\n",
       "                       [ 17.7534,   4.7259,  13.0147,  ...,  22.5850,  10.9944,   3.9470],\n",
       "                       [ 20.4202,  -5.1161,  -6.6700,  ...,  -6.9341, -12.4173,   3.7222]])),\n",
       "              ('mlp_extractor.value_net.2.bias',\n",
       "               tensor([-6.4511,  6.0958,  6.0522, -5.1365, -4.8888,  6.3980,  5.5887, -5.6750,\n",
       "                        6.3094,  4.8341,  6.6650, -5.7553, -6.7929, -5.6931, -5.8143, -5.9382,\n",
       "                       -5.6345, -6.7242, -4.8557,  5.4450, -5.2394,  5.4262,  5.1755, -5.6547,\n",
       "                       -5.6482, -4.6793, -5.3082, -5.0997, -6.1470, -5.2018, -6.3682,  5.8184,\n",
       "                        5.1561, -5.4316,  5.4323, -7.1576, -6.1637,  5.3027, -5.6990,  7.6434,\n",
       "                       -6.7469, -7.2609, -4.9944, -4.4350,  5.2704,  4.7287,  6.4407,  5.4640,\n",
       "                       -5.3162,  5.5592, -5.8904,  6.9224,  6.6101, -5.5852, -5.5955, -5.1600,\n",
       "                        5.4718, -5.0738, -5.2353, -6.1890, -6.8608,  6.3547, -4.4132,  5.3395])),\n",
       "              ('action_net.weight',\n",
       "               tensor([[-0.8017,  1.0329,  0.7555, -0.9276,  0.9077, -1.0668,  0.9134,  0.9167,\n",
       "                         0.9273, -0.8446,  0.0996, -1.0192,  0.7795,  0.0971,  0.6879, -1.1356,\n",
       "                         0.6977, -0.7613,  1.0442,  0.9746,  0.7272,  0.9207,  0.8448,  0.4031,\n",
       "                        -1.2225,  0.4429, -0.4537, -0.8409,  0.3162,  1.0079, -0.9024,  1.1475,\n",
       "                         0.1692,  1.1233,  1.0507, -0.7504, -0.9055,  1.0356,  1.0390,  0.1925,\n",
       "                         0.0466,  0.8718,  0.9226,  0.9309,  0.8227,  0.2569,  0.4214, -0.9509,\n",
       "                        -0.8602,  0.9372,  0.8465, -0.3072, -0.7101, -0.2057,  0.9108,  0.6973,\n",
       "                         1.0800,  0.8610, -0.9459, -0.6454,  1.1468, -1.0083, -0.8430, -0.0863],\n",
       "                       [ 0.8788, -1.1155, -0.7551,  0.8905, -0.8467,  1.0297, -0.8499, -1.1067,\n",
       "                        -0.7130,  0.8628, -0.1807,  1.0027, -0.8164, -0.0855, -0.8758,  1.0764,\n",
       "                        -0.5505,  0.7690, -1.0078, -1.2120, -0.7095, -0.9428, -0.9101, -0.5255,\n",
       "                         1.0153, -0.3663,  0.6701,  0.9565, -0.2708, -0.8557,  0.8040, -0.9972,\n",
       "                        -0.1430, -1.1155, -0.9360,  0.8266,  0.9098, -0.9491, -0.9865, -0.1965,\n",
       "                        -0.2890, -0.8339, -0.7622, -0.8610, -0.7298, -0.1883, -0.1672,  0.8177,\n",
       "                         0.9474, -1.0612, -0.8661,  0.3288,  0.7628,  0.1991, -1.0408, -0.7957,\n",
       "                        -1.0420, -0.8719,  0.8153,  0.7205, -1.0012,  1.0443,  0.8361,  0.2682]])),\n",
       "              ('action_net.bias', tensor([ 0.4886, -0.4886])),\n",
       "              ('value_net.weight',\n",
       "               tensor([[-13.4980,  14.0400,  12.7828, -20.7324, -19.1870,  13.1869,  17.1510,\n",
       "                        -13.6490,   9.9259,  23.0341,  11.5467, -14.2924,  -8.7225, -11.7441,\n",
       "                        -17.1188, -11.5589, -16.8600,  -7.5028, -17.0011,  13.7073, -23.9934,\n",
       "                         15.3437,  18.6253, -15.7894, -16.0451, -24.8085, -17.8664, -19.5214,\n",
       "                        -13.7213, -16.7401, -12.1309,  15.6039,  20.6680, -15.2880,  14.7516,\n",
       "                         -8.6802,  -9.3916,  21.3649, -15.4297,  11.6561, -10.8596, -11.2781,\n",
       "                        -20.3028, -32.2508,  18.9853,  18.2501,  12.1933,  17.1821, -15.8731,\n",
       "                         17.7800, -14.0528,   9.1408,  10.9869, -13.4701, -12.3664, -17.6405,\n",
       "                         14.2851, -16.6145, -16.6235,  -8.5307, -11.7412,  10.2369, -29.7961,\n",
       "                         16.0870]])),\n",
       "              ('value_net.bias', tensor([7.1603]))]),\n",
       " 'policy.optimizer': {'state': {},\n",
       "  'param_groups': [{'lr': 0.0007,\n",
       "    'momentum': 0,\n",
       "    'alpha': 0.99,\n",
       "    'eps': 1e-05,\n",
       "    'centered': False,\n",
       "    'weight_decay': 0,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('a2c_lunar_multiproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Params as JSON\n",
    "## Function to Convert Params Dict to Flattened List\n",
    "def flatten_list(params):\n",
    "    \"\"\"\n",
    "    :param params: (dict)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    params_ = {}\n",
    "    for key in params.keys():\n",
    "        params_[key] = params[key].tolist()\n",
    "    return params_\n",
    "## Write Parameters to JSON File\n",
    "import json\n",
    "\n",
    "all_params = model.get_parameters()\n",
    "pol_params = flatten_list(all_params['policy'])\n",
    "\n",
    "all_params['policy'] = pol_params\n",
    "\n",
    "with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "    json.dump(all_params, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.5 1 9.3 2 8.9 3 9.7 4 9.3 5 8.9 6 9.1 7 8.9 8 9.8 9 9.3 Type  Mean reward: 9.27\n"
     ]
    }
   ],
   "source": [
    "model_loaded = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "evaluate(model_loaded,env, verbose=1)\n",
    "\n",
    "new_params = all_params\n",
    "loaded_pol_params = new_params['policy']\n",
    "for key in loaded_pol_params.keys():\n",
    "    loaded_pol_params[key] = th.tensor(loaded_pol_params[key])\n",
    "\n",
    "new_params['policy'] = loaded_pol_params\n",
    "\n",
    "model_loaded.set_parameters(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 79.3 1 89.0 2 75.4 3 61.1 4 78.8 5 66.9 6 63.8 7 58.4 8 66.0 9 78.9 Type  Mean reward: 71.75999999999999\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "evaluate(model_loaded,env, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
