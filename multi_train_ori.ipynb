{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Train Gradient Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import threading\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import A2C as ALGO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init. ENV and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_1 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_2 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Evaluate Model and Train Model within Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, message = '', verbose = 0):\n",
    "    fitnesses = []\n",
    "    iterations = 10\n",
    "    for i in range(iterations):\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        if verbose == 1:\n",
    "            print(i, fitness, end=\" \")\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    mean_fitness = np.mean(sorted(fitnesses))\n",
    "    print(f'Type {message} Mean reward: {mean_fitness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, timesteps):\n",
    "    # print('Starting Training')\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    # print('Completed Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 9.11\n",
      "Type  Mean reward: 8.989999999999998\n",
      "Type  Mean reward: 9.16\n"
     ]
    }
   ],
   "source": [
    "model_trained_1.set_parameters(model.get_parameters())\n",
    "model_trained_2.set_parameters(model.get_parameters())\n",
    "\n",
    "evaluate(model, env)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 1K Steps and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 9.349999999999998\n",
      "Type  Mean reward: 59.14999999999999\n",
      "Type  Mean reward: 9.030000000000001\n"
     ]
    }
   ],
   "source": [
    "# Train MT Model 1\n",
    "t1 = threading.Thread(target=train, args=(model_trained_1, 10_00))\n",
    "\n",
    "# Train MT Model 2\n",
    "t2 = threading.Thread(target=train, args=(model_trained_2, 10_00))\n",
    "\n",
    "# starting thread\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until thread is completely executed\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "\n",
    "# model_trained.learn(total_timesteps=10_00)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Gradient and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 68.25\n"
     ]
    }
   ],
   "source": [
    "# For Trained Model 1\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "\n",
    "# For Trained Model 2\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model_trained_1.policy.load_state_dict(state_dict)\n",
    "model_trained_2.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iter:  0| Type Updated Model Mean reward: 59.519999999999996\n",
      "Train Iter:  1| Type Updated Model Mean reward: 59.96\n",
      "Train Iter:  2| Type Updated Model Mean reward: 60.510000000000005\n",
      "Train Iter:  3| Type Updated Model Mean reward: 63.989999999999995\n",
      "Train Iter:  4| Type Updated Model Mean reward: 65.05999999999999\n",
      "Train Iter:  5| Type Updated Model Mean reward: 67.77000000000001\n",
      "Train Iter:  6| Type Updated Model Mean reward: 68.47\n",
      "Train Iter:  7| Type Updated Model Mean reward: 70.53\n",
      "Train Iter:  8| Type Updated Model Mean reward: 67.85\n",
      "Train Iter:  9| Type Updated Model Mean reward: 72.77\n",
      "Train Iter:  10| Type Updated Model Mean reward: 70.6\n",
      "Train Iter:  11| Type Updated Model Mean reward: 70.6\n",
      "Train Iter:  12| Type Updated Model Mean reward: 75.15\n",
      "Train Iter:  13| Type Updated Model Mean reward: 75.79999999999998\n",
      "Train Iter:  14| Type Updated Model Mean reward: 73.78\n",
      "Train Iter:  15| Type Updated Model Mean reward: 74.69999999999999\n",
      "Train Iter:  16| Type Updated Model Mean reward: 82.06\n",
      "Train Iter:  17| Type Updated Model Mean reward: 75.55\n",
      "Train Iter:  18| Type Updated Model Mean reward: 79.64\n",
      "Train Iter:  19| Type Updated Model Mean reward: 77.86999999999999\n",
      "Train Iter:  20| Type Updated Model Mean reward: 79.55\n",
      "Train Iter:  21| Type Updated Model Mean reward: 79.94\n",
      "Train Iter:  22| Type Updated Model Mean reward: 77.16\n",
      "Train Iter:  23| Type Updated Model Mean reward: 79.37\n",
      "Train Iter:  24| Type Updated Model Mean reward: 81.00000000000001\n",
      "Train Iter:  25| Type Updated Model Mean reward: 79.25\n",
      "Train Iter:  26| Type Updated Model Mean reward: 84.7\n",
      "Train Iter:  27| Type Updated Model Mean reward: 86.74000000000001\n",
      "Train Iter:  28| Type Updated Model Mean reward: 84.08000000000001\n",
      "Train Iter:  29| Type Updated Model Mean reward: 81.89000000000001\n",
      "Train Iter:  30| Type Updated Model Mean reward: 81.05\n",
      "Train Iter:  31| Type Updated Model Mean reward: 82.71000000000001\n",
      "Train Iter:  32| Type Updated Model Mean reward: 80.11\n",
      "Train Iter:  33| Type Updated Model Mean reward: 80.96000000000001\n",
      "Train Iter:  34| Type Updated Model Mean reward: 82.14\n",
      "Train Iter:  35| Type Updated Model Mean reward: 81.94999999999999\n",
      "Train Iter:  36| Type Updated Model Mean reward: 88.27\n",
      "Train Iter:  37| Type Updated Model Mean reward: 82.65\n",
      "Train Iter:  38| Type Updated Model Mean reward: 86.53999999999999\n",
      "Train Iter:  39| Type Updated Model Mean reward: 83.50999999999999\n",
      "Train Iter:  40| Type Updated Model Mean reward: 85.84\n",
      "Train Iter:  41| Type Updated Model Mean reward: 84.52000000000001\n",
      "Train Iter:  42| Type Updated Model Mean reward: 83.72999999999999\n",
      "Train Iter:  43| Type Updated Model Mean reward: 85.67999999999999\n",
      "Train Iter:  44| Type Updated Model Mean reward: 84.03\n",
      "Train Iter:  45| Type Updated Model Mean reward: 86.36999999999999\n",
      "Train Iter:  46| Type Updated Model Mean reward: 81.30999999999997\n",
      "Train Iter:  47| Type Updated Model Mean reward: 84.49000000000001\n",
      "Train Iter:  48| Type Updated Model Mean reward: 86.66\n",
      "Train Iter:  49| Type Updated Model Mean reward: 88.11\n",
      "Train Iter:  50| Type Updated Model Mean reward: 84.89\n",
      "Train Iter:  51| Type Updated Model Mean reward: 89.85\n",
      "Train Iter:  52| Type Updated Model Mean reward: 86.94999999999999\n",
      "Train Iter:  53| Type Updated Model Mean reward: 81.44\n",
      "Train Iter:  54| Type Updated Model Mean reward: 87.3\n",
      "Train Iter:  55| Type Updated Model Mean reward: 84.89999999999999\n",
      "Train Iter:  56| Type Updated Model Mean reward: 87.97\n",
      "Train Iter:  57| Type Updated Model Mean reward: 88.82000000000001\n",
      "Train Iter:  58| Type Updated Model Mean reward: 89.85\n",
      "Train Iter:  59| Type Updated Model Mean reward: 90.72999999999999\n",
      "Train Iter:  60| Type Updated Model Mean reward: 85.82000000000001\n",
      "Train Iter:  61| Type Updated Model Mean reward: 91.85999999999999\n",
      "Train Iter:  62| Type Updated Model Mean reward: 91.58999999999999\n",
      "Train Iter:  63| Type Updated Model Mean reward: 88.8\n",
      "Train Iter:  64| Type Updated Model Mean reward: 86.58000000000001\n",
      "Train Iter:  65| Type Updated Model Mean reward: 89.11\n",
      "Train Iter:  66| Type Updated Model Mean reward: 89.34\n",
      "Train Iter:  67| Type Updated Model Mean reward: 88.9\n",
      "Train Iter:  68| Type Updated Model Mean reward: 87.46000000000001\n",
      "Train Iter:  69| Type Updated Model Mean reward: 88.97\n",
      "Train Iter:  70| Type Updated Model Mean reward: 84.16999999999999\n",
      "Train Iter:  71| Type Updated Model Mean reward: 90.58000000000001\n",
      "Train Iter:  72| Type Updated Model Mean reward: 86.92999999999999\n",
      "Train Iter:  73| Type Updated Model Mean reward: 88.72\n",
      "Train Iter:  74| Type Updated Model Mean reward: 83.95\n",
      "Train Iter:  75| Type Updated Model Mean reward: 85.11999999999999\n",
      "Train Iter:  76| Type Updated Model Mean reward: 90.19000000000001\n",
      "Train Iter:  77| Type Updated Model Mean reward: 83.45\n",
      "Train Iter:  78| Type Updated Model Mean reward: 85.72999999999999\n",
      "Train Iter:  79| Type Updated Model Mean reward: 86.0\n",
      "Train Iter:  80| Type Updated Model Mean reward: 89.11\n",
      "Train Iter:  81| Type Updated Model Mean reward: 87.4\n",
      "Train Iter:  82| Type Updated Model Mean reward: 91.17999999999999\n",
      "Train Iter:  83| Type Updated Model Mean reward: 90.03999999999999\n",
      "Train Iter:  84| Type Updated Model Mean reward: 86.16999999999999\n",
      "Train Iter:  85| Type Updated Model Mean reward: 86.77\n",
      "Train Iter:  86| Type Updated Model Mean reward: 89.41999999999999\n",
      "Train Iter:  87| Type Updated Model Mean reward: 87.06\n",
      "Train Iter:  88| Type Updated Model Mean reward: 90.46000000000001\n",
      "Train Iter:  89| Type Updated Model Mean reward: 86.71000000000001\n",
      "Train Iter:  90| Type Updated Model Mean reward: 88.19\n",
      "Train Iter:  91| Type Updated Model Mean reward: 87.86\n",
      "Train Iter:  92| Type Updated Model Mean reward: 91.69\n",
      "Train Iter:  93| Type Updated Model Mean reward: 89.98\n",
      "Train Iter:  94| Type Updated Model Mean reward: 90.34\n",
      "Train Iter:  95| Type Updated Model Mean reward: 87.15\n",
      "Train Iter:  96| Type Updated Model Mean reward: 93.92\n",
      "Train Iter:  97| Type Updated Model Mean reward: 87.11\n",
      "Train Iter:  98| Type Updated Model Mean reward: 90.72\n",
      "Train Iter:  99| Type Updated Model Mean reward: 89.22999999999999\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('Train Iter: ', i, end=\"| \")\n",
    "\n",
    "    # Train MT Model 1\n",
    "    t1 = threading.Thread(target=train, args=(model_trained_1, 10_0))\n",
    "\n",
    "    # Train MT Model 2\n",
    "    t2 = threading.Thread(target=train, args=(model_trained_2, 10_0))\n",
    "\n",
    "    # starting thread\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # wait until thread is completely executed\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    # evaluate(model_trained_1, env, 'Trained Model 1', verbose=1)\n",
    "    # evaluate(model_trained_2, env, 'Trained Model 2', verbose=1)\n",
    "    # evaluate(model, env, 'Initial Model', verbose=1)\n",
    "\n",
    "    # For Trained Model 1\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "    # For Trained Model 2\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    evaluate(model, env, 'Updated Model', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': OrderedDict([('mlp_extractor.policy_net.0.weight',\n",
       "               tensor([[-123.0519,   51.6922, -280.5896,   69.7247],\n",
       "                       [-106.6337,   98.8092,  -18.5256,  -42.7743],\n",
       "                       [  28.9930,  -38.8804,  218.7339,  -11.2443],\n",
       "                       [ 241.2078,  195.0237,   41.0198,  -94.8296],\n",
       "                       [ -25.2027,   69.1166, -166.0602,   60.0438],\n",
       "                       [  -0.5561,    4.9710,  110.0662,  128.4443],\n",
       "                       [ -41.4945,   39.7966,  -10.4857,  -69.2420],\n",
       "                       [ 121.3245,  -64.1814,  -46.1715,  193.5544],\n",
       "                       [  -8.2849, -103.9983,   -0.6755,   17.9062],\n",
       "                       [ 165.1385,  159.8434,  -14.7355,   18.5616],\n",
       "                       [  43.3829, -306.0070,  112.3653, -203.6458],\n",
       "                       [-181.8154,   33.0797,   84.7722,  107.5359],\n",
       "                       [  59.8550,   18.5173,  -91.5292,  -47.9431],\n",
       "                       [-179.9169, -159.6340,  -11.4622,  120.1807],\n",
       "                       [   7.0706,   24.2795,  207.9187,   87.4070],\n",
       "                       [ -32.0084,   32.0673,   12.4870,  143.7430],\n",
       "                       [   5.0811,  251.1297,  155.2907, -188.0878],\n",
       "                       [   3.9310,    7.6352,  -91.9379,  -55.2027],\n",
       "                       [ -13.0377,    9.0460,  -10.3579, -192.9935],\n",
       "                       [  54.8948,   55.4763,   32.1039, -154.6589],\n",
       "                       [ -43.5669, -100.1607,    9.7575,   30.6873],\n",
       "                       [ 144.8412,   48.9266,  -27.5962,   -4.8199],\n",
       "                       [ -82.4468,   42.1368,  -74.9696,   94.0247],\n",
       "                       [-101.0813,   -5.0549,   61.8873,   86.5886],\n",
       "                       [ -41.6587,   95.6074,  -31.8718, -102.7691],\n",
       "                       [  64.6066,   -7.1845,    4.2582,  193.5707],\n",
       "                       [ -45.2495, -119.2540,  -13.7624,  -12.6083],\n",
       "                       [ 142.1772,  -75.1073,  -73.6041,  -34.9299],\n",
       "                       [  23.1918,   84.8485,   90.2655,   23.2902],\n",
       "                       [ -38.2790,  -73.7068,   27.3106,   46.5752],\n",
       "                       [  35.0896,  -82.3454, -119.1145,  -78.2663],\n",
       "                       [ -93.6149,   18.6700,  -50.6526,   31.9059],\n",
       "                       [ -95.2759,  -53.9494,   91.8348, -208.2774],\n",
       "                       [-200.9933,   57.7306,  103.6567,   72.3327],\n",
       "                       [ 156.1931,   34.8599,   11.5784,  -22.9488],\n",
       "                       [ -50.0077,  -23.0768,   96.0616,  159.1574],\n",
       "                       [  91.0868,  -43.7994,    4.6659,  152.8394],\n",
       "                       [-189.3767,  -23.9943,   60.6436,  -72.7071],\n",
       "                       [ -99.3710, -217.8668,  -49.3617,   72.4949],\n",
       "                       [  26.6059,  -81.9159, -212.0633, -111.4174],\n",
       "                       [  71.9412,  229.2788,  112.9186,  209.9732],\n",
       "                       [  52.6386,   35.6220,  105.6114,  -87.9643],\n",
       "                       [-114.8839,  -11.6644,  131.3923, -208.3841],\n",
       "                       [ -78.4385,  102.4538,   90.2523,   98.5915],\n",
       "                       [  44.1607,  135.8494,   78.3133,  127.9935],\n",
       "                       [ -10.2375,  148.1736,   31.1029,  -80.3211],\n",
       "                       [  -1.2014,  -89.2682,  -76.3561,  117.8976],\n",
       "                       [ -72.4313,  141.8294,  -22.0852, -146.0096],\n",
       "                       [  -6.6965,  -71.8686,   35.0348,   47.2482],\n",
       "                       [ 174.3486,   -4.3566, -212.6217,  124.1378],\n",
       "                       [ -55.1650,   14.3078,  -81.8970,  120.9902],\n",
       "                       [ -53.5469, -158.0742,  119.9673,   95.7981],\n",
       "                       [  -6.0434,  -69.8916,    5.7386,  168.7492],\n",
       "                       [ -84.7599,   79.5137,   68.5762, -177.6896],\n",
       "                       [-200.2040,   90.9462,  -33.1026,   69.9269],\n",
       "                       [-173.4892,   85.4700,  -51.2660,  -87.5801],\n",
       "                       [ -11.1929,  -72.4888,  101.4086, -135.3570],\n",
       "                       [  46.8433,  -57.3903,  131.2835,   54.9510],\n",
       "                       [-191.6588,  162.5182,   58.4354, -103.6481],\n",
       "                       [ 260.6371,  -70.0176,   70.2444,  -24.3361],\n",
       "                       [ -75.2135,  106.6001, -275.7711, -182.8657],\n",
       "                       [ -96.6332,  215.5153,  -17.9142,   79.3637],\n",
       "                       [ -40.3693,   25.0186, -217.0638,  -56.4123],\n",
       "                       [   1.0233, -226.5359,   32.3767,   49.2159]])),\n",
       "              ('mlp_extractor.policy_net.0.bias',\n",
       "               tensor([  3.9367,  -5.3584,  -7.3757,  -6.0218,  -0.7434,  -8.8916,  -1.6539,\n",
       "                         5.4104,   4.6560,  -8.2484,   8.3702,  -0.0474,  -2.0052,   6.3275,\n",
       "                        -4.5754, -10.9702,  -6.2219,  13.3149,  -2.9907,  -4.2798,   5.5449,\n",
       "                         0.2409,   0.4970,   3.9209,  -5.1536,   3.6820,   8.4752,   5.7553,\n",
       "                        -6.0811,  -3.4066,   6.4281,   1.7585,  -3.0646,  -9.4314,  -0.5333,\n",
       "                         3.2281,  -0.9611,  13.1060,   7.5396,   6.2181,  -7.0227,   5.7387,\n",
       "                        -4.6854, -12.2757, -14.7060,  -4.9860,   5.5290,  -6.5928,   5.5124,\n",
       "                         5.7741,   4.7707,   6.5237,   4.2319,  -6.7521,   0.3140,  -4.4792,\n",
       "                         0.1451,  -1.7771,  -6.1294,   7.2870,   0.3602,  -3.1170,   7.4173,\n",
       "                         5.6149])),\n",
       "              ('mlp_extractor.policy_net.2.weight',\n",
       "               tensor([[-101.5497,   -2.2275,  145.4497,  ...,   36.7203,   48.0993,\n",
       "                          66.7702],\n",
       "                       [ 110.7184, -179.3535,  -59.9722,  ...,   33.6696,  -67.2993,\n",
       "                         -87.8957],\n",
       "                       [-271.5632,   49.9801, -137.4560,  ...,   65.5979,  185.9530,\n",
       "                          21.8331],\n",
       "                       ...,\n",
       "                       [ -23.3236,    1.2452,  -98.7989,  ...,   33.6720,  -11.8921,\n",
       "                        -116.1356],\n",
       "                       [  18.7982,    1.0439,   66.6309,  ...,   20.8508,   67.3056,\n",
       "                        -200.0852],\n",
       "                       [ 132.5619,  -27.2489,  129.3882,  ...,   80.1191,   27.4965,\n",
       "                         -89.3704]])),\n",
       "              ('mlp_extractor.policy_net.2.bias',\n",
       "               tensor([-6.3032e+00, -7.1422e+00,  2.0373e+00,  4.3082e+00,  7.2069e+00,\n",
       "                        5.8377e+00, -5.8830e+00,  6.6100e+00,  8.0475e-01, -5.4600e-01,\n",
       "                       -1.7670e-01,  5.2366e+00,  6.8999e+00, -2.9662e+00,  1.1720e+00,\n",
       "                       -3.2800e+00, -1.5274e+00, -6.7380e+00, -9.0318e+00,  7.2316e+00,\n",
       "                       -7.0511e+00,  2.3062e+00,  8.5642e-01, -2.9701e+00, -6.8673e+00,\n",
       "                        1.2504e+00,  5.2084e+00, -6.9740e+00, -1.6516e+00, -7.6842e+00,\n",
       "                        6.4378e+00, -6.4686e+00,  1.4631e+00, -6.2744e+00, -7.5145e+00,\n",
       "                        9.2549e-03,  5.0564e+00,  6.0587e+00, -6.7180e+00,  6.3696e+00,\n",
       "                        6.3482e+00, -6.4650e-01,  2.5458e+00,  6.0783e+00,  3.8280e+00,\n",
       "                       -1.8691e+00, -6.9080e+00, -6.6358e+00, -2.0453e+00,  9.9994e+00,\n",
       "                       -7.4375e+00,  3.2769e+00,  6.2896e+00,  3.5560e+00, -6.4575e+00,\n",
       "                       -3.9851e+00,  7.0051e+00, -7.0810e+00, -6.8169e+00, -3.8059e+00,\n",
       "                       -4.5850e+00, -1.9854e-01, -6.5844e+00,  6.6164e+00])),\n",
       "              ('mlp_extractor.value_net.0.weight',\n",
       "               tensor([[-251.5731,   33.5282, -165.8038,   12.3262],\n",
       "                       [  24.8957, -150.7438,  -20.7466,   63.4085],\n",
       "                       [-120.6255,  220.8712,  137.5145,  -26.9130],\n",
       "                       [ 112.8239,  -30.0406,  -57.3663,   37.1130],\n",
       "                       [-164.2903,  -17.8538,  125.5098,   41.3592],\n",
       "                       [ -42.6907,  -34.4538, -151.3307,  -32.5776],\n",
       "                       [   1.9881,  -23.9854, -121.5825,   24.4874],\n",
       "                       [ 103.8136,  -71.3304,   74.9171,   61.1626],\n",
       "                       [ 255.1146, -125.0914, -109.9740,  -51.9634],\n",
       "                       [ -19.9054,   64.6103, -164.6008,  -43.0587],\n",
       "                       [  88.4119,   68.8511,   80.4320, -107.2190],\n",
       "                       [-115.6186, -113.0619,  -36.1254,  -45.3726],\n",
       "                       [ -85.1988, -219.2768,  -80.9137, -197.6048],\n",
       "                       [  66.2311,  133.9363,   92.6622,   23.6014],\n",
       "                       [  62.5535,   29.9531,  211.0515,  -75.9604],\n",
       "                       [-161.9692,  -36.6242,  105.9403,   46.9062],\n",
       "                       [ -42.2060, -140.4893,  193.1190,  244.9801],\n",
       "                       [ -19.2832,  -12.4958,  168.9475,  -83.3195],\n",
       "                       [ -79.1821,  -31.9098,  -34.7996,   -2.1087],\n",
       "                       [ -64.6769,   46.4760,  -47.4959, -144.3653],\n",
       "                       [-166.8462,   59.7403, -137.3472,  -10.4821],\n",
       "                       [  79.5996,  226.8179,   59.5243,  177.4403],\n",
       "                       [-176.2136,  183.1067, -211.6701,   38.1884],\n",
       "                       [ -92.2390, -219.5267,  -92.5936, -153.6473],\n",
       "                       [ 185.6918,  -81.8253,   37.0671,  -65.3797],\n",
       "                       [ -33.5286,  -18.1977,   54.4972, -101.8110],\n",
       "                       [ -94.8584,   32.4697,  -21.6145,   95.7268],\n",
       "                       [ -20.9850,  -86.2535,  -58.7508,   44.8775],\n",
       "                       [  31.9665,  -99.5579,  -27.7937,   34.1608],\n",
       "                       [ -28.8247,  -65.4101,   12.9929,   92.8166],\n",
       "                       [ -97.1208, -132.3527,    1.9363,  106.0345],\n",
       "                       [ -72.5482,   77.8579,  159.2738,  287.2432],\n",
       "                       [-135.2485,  -57.0041, -156.6338, -106.9350],\n",
       "                       [  40.3861, -109.3514,  -23.3094,   -9.9706],\n",
       "                       [  -4.7417,  -20.2785,   65.9654, -216.9518],\n",
       "                       [ 144.1210,  -76.1384,   58.6897,  -40.2329],\n",
       "                       [  12.7830,  158.4292,  -27.5870,  -43.3898],\n",
       "                       [ -79.9013,  -59.8573,  111.6781,   81.8946],\n",
       "                       [   8.1947,   89.8832,  145.5339,  -85.6064],\n",
       "                       [-135.4258,   43.7325,  212.4668,  -50.4779],\n",
       "                       [  22.7866,  135.7164,   68.4942,  -36.9771],\n",
       "                       [-119.2915, -109.0906, -125.7941,   17.7852],\n",
       "                       [ -58.5740,   91.2497,  -34.9723,  -93.6665],\n",
       "                       [  62.8709,   11.6106,   95.2508,  -23.2501],\n",
       "                       [ 152.6697,   67.3657,   13.6494,  124.7819],\n",
       "                       [-131.1106,  134.8551, -113.8342,  132.6164],\n",
       "                       [ -27.2788,  -52.6735,  -44.7676, -140.0674],\n",
       "                       [  35.1522,   48.6885,  165.4253, -123.2141],\n",
       "                       [ -52.3469,   11.2122,   19.7939, -130.9080],\n",
       "                       [  52.4151,  -14.5018,  -81.1490,  197.5641],\n",
       "                       [ -15.1836,  -50.8789, -126.3786,  100.9902],\n",
       "                       [ -36.9846,  -98.8822,   42.6385,   75.5554],\n",
       "                       [ -79.2695,   -5.5478,  -89.3468,   60.0762],\n",
       "                       [ 273.6583,  123.0359, -118.0162,   19.5914],\n",
       "                       [ 137.7571,  102.0395,  -77.0870,   98.3774],\n",
       "                       [ -35.6178,  -17.5365,  -73.2497,  -36.0787],\n",
       "                       [ -18.6380, -232.7910,   49.8973,   19.3139],\n",
       "                       [ -49.6824,   66.5240,  190.6370, -111.8253],\n",
       "                       [ 133.5336,   27.3336, -104.8455,   93.3935],\n",
       "                       [  34.3971,  124.7706,  -26.4915,  124.1820],\n",
       "                       [-182.9897,    9.7679,  104.6554,   33.7375],\n",
       "                       [ 104.7030, -160.3152,  -12.6509,  -93.2434],\n",
       "                       [ -47.5992,  121.7652,  -85.7050,  -45.0285],\n",
       "                       [  80.2853,   36.3931,  119.8628,   63.8495]])),\n",
       "              ('mlp_extractor.value_net.0.bias',\n",
       "               tensor([-63.7509,  92.5169, -70.0136, -64.4892, -64.8133, -60.8341, -65.2546,\n",
       "                        52.9802,  87.4713, -70.2013, -70.1718, -64.5960,  63.4609,  63.7947,\n",
       "                        65.3677, -61.4076, -66.4840, -71.4688,  67.1770, -55.7664, -55.3333,\n",
       "                        71.4469, -59.9627, -68.4544, -65.7271, -72.5211,  78.5723, -65.6503,\n",
       "                        90.2349,  86.9021, -68.2812,  54.2859,  59.7425, -63.4706,  62.2562,\n",
       "                        64.3205, -86.0765,  71.0300,  63.6606, -59.6635,  48.6235,   8.7897,\n",
       "                       -67.9071,  68.8392, -66.6954,  64.4245, -55.0176,  70.7017, -67.5045,\n",
       "                       -74.9566, -63.8749, -59.4487, -79.9261, -61.7428,  58.5804,  91.9009,\n",
       "                        23.9529, -61.1633,  70.5939, -13.3938,  62.8114, -62.9496, -59.0369,\n",
       "                        72.4531])),\n",
       "              ('mlp_extractor.value_net.2.weight',\n",
       "               tensor([[ 107.6161,  -43.8516,  103.1244,  ...,   24.3260,   35.2910,\n",
       "                           9.3097],\n",
       "                       [-157.7796,  106.8257, -154.8616,  ...,  -51.1063, -111.8457,\n",
       "                         131.3732],\n",
       "                       [-118.5479,   -9.2485,   27.8192,  ..., -173.9028,  -70.7295,\n",
       "                         -63.1399],\n",
       "                       ...,\n",
       "                       [-171.2770,   34.8305, -282.3960,  ...,  -60.5256, -139.9823,\n",
       "                         -94.4522],\n",
       "                       [ 157.0777,  131.4417,   36.7213,  ...,  -52.0417,  123.5445,\n",
       "                        -212.7628],\n",
       "                       [ 167.1980,  139.7085, -240.0209,  ...,  -82.5453,   97.8085,\n",
       "                          66.9538]])),\n",
       "              ('mlp_extractor.value_net.2.bias',\n",
       "               tensor([-48.0170,  55.7844, -64.5703,  59.3083, -57.0158,  46.7693,  51.7955,\n",
       "                        58.7648, -55.2566, -47.2189,  52.0253,  51.0860,  48.7929,  64.6557,\n",
       "                       -47.9895,  59.8474,  57.9330,  66.4397,  45.4594, -54.3608, -50.3991,\n",
       "                       -57.9904,  69.6050, -43.0622,  51.9869, -27.4781,  39.5038,  70.7343,\n",
       "                       -50.1648, -50.4984, -71.9009,  52.0069, -48.6788, -60.7825, -47.5848,\n",
       "                        46.1378,  49.6367, -46.2810, -51.9911, -51.5727, -58.0570, -43.5260,\n",
       "                        49.5527,  44.4044, -58.7016, -74.6865,  57.1025, -59.3028,  35.1927,\n",
       "                       -47.2447, -46.0159,  66.1706, -51.1118,  45.0285,  50.3957,  51.4899,\n",
       "                       -57.2023, -52.5173,  54.7163,  48.4417,  64.3288,  45.9190, -62.3988,\n",
       "                       -38.0068])),\n",
       "              ('action_net.weight',\n",
       "               tensor([[  3.8281,  10.2150,  -9.2575,  14.7051,  -6.9042,  -5.1544,   8.0371,\n",
       "                         -9.7954,  -0.5861,   9.3402,  -5.6657,  -8.9039,  -6.4098,   8.3432,\n",
       "                         -7.6304,  10.1305, -15.3176,   8.1310,  -1.3571, -10.5885,   0.4456,\n",
       "                         10.1834,   9.8676,  -0.4783,   4.0976,  16.9852, -11.5599,   9.5664,\n",
       "                         -9.3743,  10.4750,  -7.4960,   3.6119,  -8.7722,  11.5862,  -4.0370,\n",
       "                          4.7244,  16.3909, -10.4407,   2.7076, -12.6421,  -2.7234, -10.5494,\n",
       "                          9.4179, -13.0947,   5.4975,  10.4012,   8.5148,   6.9072, -10.5563,\n",
       "                          2.0287,   9.8416,  14.1137,  -9.5101,   8.0523,   9.2723,  -0.9923,\n",
       "                         -9.8121,  12.2765,  11.4815, -11.3034,  -5.4673,  10.7928,   5.0985,\n",
       "                         -2.4109],\n",
       "                       [ -3.2263, -10.0674,   8.8359, -15.2172,   6.7283,   5.0495,  -7.7339,\n",
       "                          8.6801,  -1.1664, -11.1653,   6.0090,   8.7456,   7.2571,  -8.2395,\n",
       "                          9.9590,  -9.5257,  17.4311,  -7.9265,   0.5153,   8.6808,  -1.1678,\n",
       "                         -8.9030, -10.2433,   1.1062,  -4.8252, -17.5026,  13.1876,  -9.9414,\n",
       "                          9.5462,  -9.1008,   6.5669,  -3.7976,   9.6354, -10.8679,   4.5832,\n",
       "                         -4.1154, -15.4964,   8.9496,  -3.4415,  12.7387,   3.7339,  10.6912,\n",
       "                         -7.6999,  11.5779,  -5.0293, -10.3557,  -7.1272,  -6.2293,   8.9863,\n",
       "                         -0.0691, -10.7798, -14.9184,  11.2681,  -7.6774,  -9.7345,   1.7065,\n",
       "                          9.8236, -12.2804, -11.0112,   9.6372,   4.9606, -10.1530,  -3.3376,\n",
       "                          4.4507]])),\n",
       "              ('action_net.bias', tensor([ 4.4503, -4.4503])),\n",
       "              ('value_net.weight',\n",
       "               tensor([[-268.1917,  256.6754, -229.6358,  202.9589, -266.3803,  340.6231,\n",
       "                         212.7921,  210.1842, -203.4510, -276.1397,  227.7620,  253.5623,\n",
       "                         282.8860,  243.6034, -237.0872,  195.5458,  186.9166,  173.1756,\n",
       "                         359.0718, -211.0175, -263.2141, -252.4435,  178.3753, -255.7933,\n",
       "                         242.0344, -110.2245,  290.1061,  179.5032, -247.3389, -218.1778,\n",
       "                        -181.5064,  239.1511, -345.6025, -219.7309, -293.2859,  294.5186,\n",
       "                         235.6660, -261.5305, -205.9688, -264.9681, -174.5990, -299.9849,\n",
       "                         251.7430,  305.9095, -236.6998, -186.8106,  227.9444, -225.5113,\n",
       "                         118.2155, -253.3177, -289.5544,  171.1772, -254.5506,  294.6583,\n",
       "                         362.4717,  216.4993, -205.8914, -226.6321,  237.9789,  262.4095,\n",
       "                         220.6802,  293.5706, -190.3189, -241.3398]])),\n",
       "              ('value_net.bias', tensor([162.5434]))]),\n",
       " 'policy.optimizer': {'state': {},\n",
       "  'param_groups': [{'lr': 0.0007,\n",
       "    'momentum': 0,\n",
       "    'alpha': 0.99,\n",
       "    'eps': 1e-05,\n",
       "    'centered': False,\n",
       "    'weight_decay': 0,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('a2c_lunar_multiproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Params as JSON\n",
    "## Function to Convert Params Dict to Flattened List\n",
    "def flatten_list(params):\n",
    "    \"\"\"\n",
    "    :param params: (dict)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    params_ = {}\n",
    "    for key in params.keys():\n",
    "        params_[key] = params[key].tolist()\n",
    "    return params_\n",
    "## Write Parameters to JSON File\n",
    "import json\n",
    "\n",
    "all_params = model.get_parameters()\n",
    "pol_params = flatten_list(all_params['policy'])\n",
    "\n",
    "all_params['policy'] = pol_params\n",
    "\n",
    "with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "    json.dump(all_params, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.7 1 9.4 2 9.5 3 9.5 4 9.7 5 9.8 6 9.7 7 9.8 8 9.5 9 9.9 Type  Mean reward: 9.65\n"
     ]
    }
   ],
   "source": [
    "model_loaded = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "evaluate(model_loaded,env, verbose=1)\n",
    "\n",
    "new_params = all_params\n",
    "loaded_pol_params = new_params['policy']\n",
    "for key in loaded_pol_params.keys():\n",
    "    loaded_pol_params[key] = th.tensor(loaded_pol_params[key])\n",
    "\n",
    "new_params['policy'] = loaded_pol_params\n",
    "\n",
    "model_loaded.set_parameters(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 92.7 1 91.7 2 88.1 3 90.5 4 86.6 5 95.3 6 92.6 7 101.4 8 86.3 9 88.4 Type  Mean reward: 91.35999999999999\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "evaluate(model_loaded,env, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
