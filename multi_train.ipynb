{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Train Gradient Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import threading\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import A2C as ALGO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init. ENV and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_1 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_2 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Evaluate Model and Train Model within Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, message = '', verbose = 0):\n",
    "    fitnesses = []\n",
    "    iterations = 10\n",
    "    for i in range(iterations):\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        if verbose == 1:\n",
    "            print(i, fitness, end=\" \")\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    mean_fitness = np.mean(sorted(fitnesses))\n",
    "    print(f'Type {message} Mean reward: {mean_fitness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, timesteps):\n",
    "    print('Starting Training')\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    print('Completed Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 63.64\n",
      "Type  Mean reward: 9.45\n",
      "Type  Mean reward: 8.92\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, env)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 1K Steps and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "Type  Mean reward: 73.7\n",
      "Type  Mean reward: 9.319999999999999\n",
      "Type  Mean reward: 64.47\n"
     ]
    }
   ],
   "source": [
    "# Train MT Model 1\n",
    "t1 = threading.Thread(target=train, args=(model_trained_1, 10_00))\n",
    "\n",
    "# Train MT Model 2\n",
    "t2 = threading.Thread(target=train, args=(model_trained_2, 10_00))\n",
    "\n",
    "# starting thread\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until thread is completely executed\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "\n",
    "# model_trained.learn(total_timesteps=10_00)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Gradient and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 88.98\n"
     ]
    }
   ],
   "source": [
    "# For Trained Model 1\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "\n",
    "# For Trained Model 2\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iter:  0\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 263.3 1 222.7 2 308.9 3 166.0 4 251.6 5 337.3 6 205.5 7 203.8 8 183.6 9 177.2 Type Trained Model 1 Mean reward: 231.99\n",
      "0 116.9 1 111.6 2 129.6 3 110.9 4 103.6 5 121.7 6 134.1 7 109.5 8 152.0 9 108.4 Type Trained Model 2 Mean reward: 119.83\n",
      "0 105.7 1 120.0 2 99.4 3 96.9 4 154.0 5 108.6 6 76.0 7 78.4 8 91.5 9 123.5 Type Initial Model Mean reward: 105.4\n",
      "0 142.7 1 117.9 2 153.0 3 142.9 4 165.4 5 127.2 6 126.5 7 164.6 8 115.3 9 139.9 Type Updated Model Mean reward: 139.54000000000002\n",
      "Train Iter:  1\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 259.8 1 187.3 2 274.6 3 171.1 4 231.1 5 305.7 6 365.7 7 120.8 8 204.0 9 157.6 Type Trained Model 1 Mean reward: 227.77000000000004\n",
      "0 195.0 1 168.0 2 175.5 3 206.5 4 203.2 5 171.2 6 168.4 7 160.5 8 179.7 9 197.5 Type Trained Model 2 Mean reward: 182.55\n",
      "0 112.9 1 126.5 2 174.3 3 95.1 4 153.2 5 109.3 6 153.8 7 98.4 8 106.9 9 209.6 Type Initial Model Mean reward: 133.99999999999997\n",
      "0 208.5 1 259.9 2 272.7 3 151.4 4 301.6 5 240.8 6 221.1 7 174.3 8 195.7 9 206.3 Type Updated Model Mean reward: 223.23000000000002\n",
      "Train Iter:  2\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 113.1 1 200.1 2 151.4 3 169.6 4 109.9 5 118.2 6 111.5 7 167.9 8 184.1 9 151.5 Type Trained Model 1 Mean reward: 147.72999999999996\n",
      "0 139.4 1 132.2 2 136.3 3 130.4 4 134.0 5 132.7 6 136.9 7 139.8 8 135.3 9 138.3 Type Trained Model 2 Mean reward: 135.53\n",
      "0 209.5 1 238.3 2 243.6 3 171.7 4 281.0 5 243.8 6 229.4 7 209.9 8 319.1 9 160.5 Type Initial Model Mean reward: 230.67999999999998\n",
      "0 258.2 1 259.7 2 253.7 3 294.1 4 302.8 5 248.6 6 355.4 7 258.5 8 254.9 9 327.2 Type Updated Model Mean reward: 281.31\n",
      "Train Iter:  3\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 91.0 1 92.7 2 86.3 3 99.0 4 83.0 5 89.4 6 88.2 7 90.6 8 88.0 9 72.2 Type Trained Model 1 Mean reward: 88.04\n",
      "0 227.4 1 227.9 2 231.4 3 232.3 4 228.5 5 229.7 6 227.2 7 233.7 8 236.1 9 239.8 Type Trained Model 2 Mean reward: 231.4\n",
      "0 275.9 1 272.9 2 290.2 3 289.1 4 328.6 5 374.2 6 324.1 7 243.7 8 164.4 9 287.1 Type Initial Model Mean reward: 285.02\n",
      "0 262.1 1 414.0 2 302.5 3 315.0 4 392.7 5 333.6 6 299.4 7 374.7 8 423.5 9 461.6 Type Updated Model Mean reward: 357.90999999999997\n",
      "Train Iter:  4\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 74.4 1 75.3 2 78.8 3 75.6 4 75.3 5 82.8 6 67.6 7 75.2 8 79.4 9 70.6 Type Trained Model 1 Mean reward: 75.49999999999999\n",
      "0 500.0 1 500.0 2 500.0 3 500.0 4 500.0 5 500.0 6 500.0 7 500.0 8 500.0 9 500.0 Type Trained Model 2 Mean reward: 500.0\n",
      "0 425.1 1 442.3 2 421.6 3 431.0 4 388.0 5 398.6 6 307.9 7 300.1 8 305.2 9 475.2 Type Initial Model Mean reward: 389.5\n",
      "0 370.5 1 308.2 2 299.6 3 351.5 4 316.8 5 326.2 6 322.8 7 326.3 8 318.4 9 334.2 Type Updated Model Mean reward: 327.45\n",
      "Train Iter:  5\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 40.6 1 43.6 2 45.5 3 46.0 4 40.7 5 46.8 6 43.0 7 43.6 8 45.6 9 47.4 Type Trained Model 1 Mean reward: 44.28\n",
      "0 11.1 1 12.2 2 11.0 3 14.3 4 14.9 5 11.8 6 12.9 7 13.7 8 11.2 9 14.5 Type Trained Model 2 Mean reward: 12.760000000000002\n",
      "0 327.3 1 378.8 2 416.4 3 368.6 4 377.5 5 357.6 6 419.9 7 337.1 8 380.6 9 294.7 Type Initial Model Mean reward: 365.85\n",
      "0 260.7 1 247.8 2 203.8 3 273.1 4 182.7 5 142.4 6 275.4 7 313.9 8 215.9 9 279.9 Type Updated Model Mean reward: 239.56000000000003\n",
      "Train Iter:  6\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 48.9 1 53.5 2 49.7 3 53.0 4 45.1 5 51.7 6 47.5 7 49.8 8 42.4 9 47.7 Type Trained Model 1 Mean reward: 48.92999999999999\n",
      "0 256.6 1 250.4 2 256.8 3 255.2 4 256.7 5 258.1 6 252.9 7 255.0 8 257.1 9 257.4 Type Trained Model 2 Mean reward: 255.61999999999998\n",
      "0 214.1 1 248.9 2 223.6 3 208.5 4 277.9 5 263.7 6 189.9 7 219.0 8 241.2 9 210.0 Type Initial Model Mean reward: 229.68\n",
      "0 217.3 1 136.6 2 193.3 3 184.3 4 116.8 5 157.0 6 232.4 7 158.3 8 239.3 9 155.1 Type Updated Model Mean reward: 179.04000000000002\n",
      "Train Iter:  7\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 58.5 1 52.0 2 58.2 3 54.7 4 52.8 5 63.2 6 53.3 7 58.7 8 54.0 9 55.7 Type Trained Model 1 Mean reward: 56.11\n",
      "0 150.5 1 145.9 2 150.7 3 147.4 4 155.6 5 146.9 6 152.1 7 147.5 8 153.5 9 147.6 Type Trained Model 2 Mean reward: 149.76999999999998\n",
      "0 179.6 1 191.1 2 161.4 3 155.5 4 130.8 5 190.2 6 163.5 7 156.2 8 139.5 9 126.1 Type Initial Model Mean reward: 159.39\n",
      "0 136.0 1 133.9 2 143.7 3 156.7 4 195.7 5 183.8 6 177.5 7 150.0 8 161.5 9 130.2 Type Updated Model Mean reward: 156.9\n",
      "Train Iter:  8\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 50.8 1 51.8 2 48.6 3 49.3 4 50.4 5 55.7 6 47.1 7 49.7 8 53.1 9 54.9 Type Trained Model 1 Mean reward: 51.13999999999999\n",
      "0 230.0 1 231.4 2 228.4 3 238.4 4 233.4 5 232.1 6 232.9 7 233.2 8 236.0 9 230.1 Type Trained Model 2 Mean reward: 232.59\n",
      "0 116.3 1 204.3 2 139.9 3 155.5 4 126.8 5 121.4 6 162.9 7 124.5 8 170.9 9 156.3 Type Initial Model Mean reward: 147.88\n",
      "0 151.1 1 157.3 2 201.7 3 158.2 4 180.4 5 163.7 6 149.2 7 149.2 8 164.0 9 149.9 Type Updated Model Mean reward: 162.47\n",
      "Train Iter:  9\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 51.1 1 48.8 2 50.0 3 47.6 4 51.9 5 47.6 6 52.7 7 48.2 8 51.3 9 49.4 Type Trained Model 1 Mean reward: 49.86\n",
      "0 127.9 1 126.7 2 131.0 3 130.5 4 127.8 5 122.4 6 125.4 7 123.7 8 126.0 9 124.5 Type Trained Model 2 Mean reward: 126.59\n",
      "0 148.1 1 134.5 2 153.9 3 137.6 4 146.1 5 210.0 6 143.0 7 149.0 8 164.6 9 138.8 Type Initial Model Mean reward: 152.56\n",
      "0 135.5 1 168.2 2 153.7 3 138.4 4 157.0 5 165.8 6 161.7 7 126.2 8 120.7 9 158.5 Type Updated Model Mean reward: 148.57\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Train Iter: ', i)\n",
    "\n",
    "    # Train MT Model 1\n",
    "    t1 = threading.Thread(target=train, args=(model_trained_1, 10_00))\n",
    "\n",
    "    # Train MT Model 2\n",
    "    t2 = threading.Thread(target=train, args=(model_trained_2, 10_00))\n",
    "\n",
    "    # starting thread\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # wait until thread is completely executed\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    evaluate(model_trained_1, env, 'Trained Model 1', verbose=1)\n",
    "    evaluate(model_trained_2, env, 'Trained Model 2', verbose=1)\n",
    "    evaluate(model, env, 'Initial Model', verbose=1)\n",
    "\n",
    "    # For Trained Model 1\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "    # For Trained Model 2\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    evaluate(model, env, 'Updated Model', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': OrderedDict([('mlp_extractor.policy_net.0.weight',\n",
       "               tensor([[-4.7994,  2.7285,  1.8746,  4.0797],\n",
       "                       [ 0.7623, -7.0949,  2.1134, -1.0655],\n",
       "                       [ 1.9194,  3.3921, -0.3741,  3.4626],\n",
       "                       [-3.6278, -1.0268, -2.4102,  3.2928],\n",
       "                       [ 1.1961, -0.9835, -2.6474,  4.2740],\n",
       "                       [-3.1046,  3.6106, -5.5946,  0.2108],\n",
       "                       [ 0.1898, -4.1656, -1.2745,  1.0092],\n",
       "                       [-3.5225,  0.8290,  4.2771,  2.9213],\n",
       "                       [-0.1257, -0.5160, -1.3684, -3.4648],\n",
       "                       [-1.7615,  0.8610,  1.5810,  3.8147],\n",
       "                       [-3.9144, -0.5135,  0.4511,  1.2394],\n",
       "                       [ 4.6794,  1.8337, -0.4528, -0.6097],\n",
       "                       [ 3.7276,  0.2791,  3.8379,  4.3595],\n",
       "                       [ 1.5737, -0.0986, -0.5604, -1.5846],\n",
       "                       [ 0.0509,  2.3838, -1.1136,  2.0262],\n",
       "                       [ 0.6830,  0.0303,  0.8477, -5.2035],\n",
       "                       [ 1.8422, -4.4948, -5.3122, -4.0556],\n",
       "                       [-0.6589,  0.6233,  1.8882, -1.9282],\n",
       "                       [ 4.3976,  4.0874,  3.0394, -0.3283],\n",
       "                       [-0.4132, -3.4626,  0.9184, -2.5168],\n",
       "                       [-0.3807,  1.5897,  7.9293,  1.5521],\n",
       "                       [ 2.0764, -1.2970,  2.1397,  5.4734],\n",
       "                       [-5.2327, -1.4966,  1.3400, -0.7254],\n",
       "                       [-2.2385,  1.3108, -3.0078, -1.1979],\n",
       "                       [-0.4858, -3.4949, -0.5876,  4.4911],\n",
       "                       [-1.2264, -1.3892, -0.7526,  2.6735],\n",
       "                       [-3.2186,  1.6114,  0.8041,  2.0411],\n",
       "                       [-0.0587,  0.5145,  0.7180, -0.8354],\n",
       "                       [ 1.2058,  0.4822, -2.3137,  1.2869],\n",
       "                       [ 1.5005, -0.3798, -4.0698, -2.3888],\n",
       "                       [ 2.5727,  6.8803,  5.1863,  0.3076],\n",
       "                       [-4.4956,  2.4163, -2.7145,  6.2767],\n",
       "                       [-1.2540, -2.5383,  1.3568,  4.7728],\n",
       "                       [-0.4514, -2.5015, -4.8832, -1.0919],\n",
       "                       [ 3.5496, -0.3053,  1.0841, -2.3703],\n",
       "                       [ 1.6874, -0.1408, -3.1314,  1.5083],\n",
       "                       [-0.0785, -1.1784,  2.5228,  2.8121],\n",
       "                       [ 1.3972, -0.2470,  6.5250,  2.8049],\n",
       "                       [ 3.7450, -1.3293,  7.6355,  9.0531],\n",
       "                       [-1.1809,  0.7658,  0.0506, -1.3690],\n",
       "                       [-2.0713,  2.3091, -0.8312,  1.9652],\n",
       "                       [-0.1595,  1.6684,  8.9179,  2.8971],\n",
       "                       [-0.5942,  1.8092, -4.4066, -2.3998],\n",
       "                       [ 2.6139,  1.2618,  0.1374, -5.3794],\n",
       "                       [-3.5145,  0.2205, -0.9907, -0.5378],\n",
       "                       [-2.2250, -1.7033, -0.2772, -1.1429],\n",
       "                       [-6.7402,  1.1284, -0.8019,  3.9400],\n",
       "                       [-4.4336,  1.8352,  4.7224,  3.3367],\n",
       "                       [ 1.9292,  1.5645, -0.7023, -2.5437],\n",
       "                       [ 1.7587,  0.2588,  0.2439,  0.9901],\n",
       "                       [-1.8851,  1.0442, -3.4276,  1.5909],\n",
       "                       [ 3.1655,  3.2967, -1.1541, -4.3400],\n",
       "                       [ 0.2946, -2.7394,  7.3321,  2.9176],\n",
       "                       [ 2.3619, -1.9860,  2.6349, -1.9566],\n",
       "                       [-1.5469, -3.5017, -0.3032,  1.8298],\n",
       "                       [ 0.6211,  4.2922,  0.1202,  2.4150],\n",
       "                       [-0.5468,  4.0329,  0.4661,  1.5599],\n",
       "                       [-3.5270, -5.4135, -2.9339, -0.5096],\n",
       "                       [ 0.8614, -2.9678, -0.8254,  0.1202],\n",
       "                       [-2.7220,  5.0215, -2.3877, -0.6494],\n",
       "                       [ 1.0509,  2.1652, -3.0272,  0.8847],\n",
       "                       [ 4.8713, -3.8089, -3.2406, -3.3750],\n",
       "                       [ 0.3989, -0.2534, -1.0118,  1.5615],\n",
       "                       [-0.1301, -1.0020,  1.2667,  2.1611]])),\n",
       "              ('mlp_extractor.policy_net.0.bias',\n",
       "               tensor([ 6.5449e-03, -5.7727e-01, -4.8135e-02,  6.3050e-01,  9.1870e-01,\n",
       "                        4.2468e-01,  1.6058e-04,  1.8470e-01,  2.7916e-01, -2.2331e-02,\n",
       "                       -5.3709e-01,  5.1639e-01,  2.5899e-02,  2.2300e-01,  1.7975e-01,\n",
       "                        4.7131e-01,  4.6057e-01,  3.5805e-01,  4.4185e-01, -6.4345e-01,\n",
       "                       -1.3148e-01,  1.7711e-01, -4.5574e-01, -2.2030e-01, -5.7274e-02,\n",
       "                       -4.0580e-01, -7.8073e-01,  2.8607e-01,  1.0068e+00,  2.3353e-01,\n",
       "                       -1.7450e-01,  3.3908e-02, -2.6204e-01,  5.0406e-01,  3.6703e-01,\n",
       "                        7.7920e-01, -3.8612e-01, -1.0209e-01,  2.4491e-02, -8.1241e-01,\n",
       "                        8.9817e-01, -2.1895e-01, -1.4242e-01, -2.8030e-01, -3.4582e-01,\n",
       "                        9.4929e-01,  9.9471e-02,  4.6736e-02,  4.2476e-01,  7.0884e-01,\n",
       "                        6.9942e-01, -1.7667e-01,  4.1501e-01,  4.0242e-01, -5.9381e-01,\n",
       "                        4.4450e-01, -8.0226e-01, -6.6694e-02, -4.2956e-01,  4.1187e-01,\n",
       "                       -4.8749e-02,  1.6381e-01, -6.5190e-01,  5.4392e-01])),\n",
       "              ('mlp_extractor.policy_net.2.weight',\n",
       "               tensor([[ 2.4340, -0.8301, -4.1288,  ..., -1.8800, -2.1661,  0.0451],\n",
       "                       [-1.0790, -3.7795, -0.6124,  ...,  3.2330, -4.6878,  6.1949],\n",
       "                       [ 0.1757,  0.5501,  5.2680,  ..., -2.2803, -0.2071, -2.5262],\n",
       "                       ...,\n",
       "                       [ 1.4924,  2.1917,  0.4397,  ..., -2.4246,  4.4207,  0.0549],\n",
       "                       [-3.6745,  2.9444, -3.3632,  ...,  0.9183,  0.0955,  1.1136],\n",
       "                       [-3.2052,  1.1830, -6.0765,  ..., -1.3604, -3.2421, -0.9082]])),\n",
       "              ('mlp_extractor.policy_net.2.bias',\n",
       "               tensor([ 0.2669,  0.2354,  0.1842, -0.2820,  0.4454,  0.6026, -0.0233,  0.6927,\n",
       "                       -0.2611,  0.1537,  0.2075, -0.3193,  0.2949,  0.1324,  0.7099, -0.4064,\n",
       "                        0.4994,  0.0950, -0.1679,  0.1263, -0.5091,  0.2110, -0.4761, -0.5750,\n",
       "                       -0.3987, -0.5292, -0.1465,  0.1401, -0.0218,  0.1364,  0.2144, -0.1745,\n",
       "                        0.5569, -0.0845, -0.1149, -0.6421,  0.5003, -0.1540,  0.4387, -0.5127,\n",
       "                        0.4873,  0.4304, -0.1083,  0.2544,  0.5551,  0.0201, -0.1084, -0.5102,\n",
       "                       -0.0743,  0.3786, -0.5411,  0.4540,  0.0107, -0.1847, -0.2991, -0.4065,\n",
       "                       -0.2506,  0.1029, -0.3309,  0.4879, -0.6493, -0.6771,  0.4888, -0.0227])),\n",
       "              ('mlp_extractor.value_net.0.weight',\n",
       "               tensor([[-5.2254e+00, -4.2040e+00, -3.6259e-01, -1.0274e+00],\n",
       "                       [ 4.1751e+00, -2.4434e-01,  5.2376e+00,  1.8009e-01],\n",
       "                       [ 1.4270e+00, -1.0680e+00,  1.2894e+00, -2.1603e+00],\n",
       "                       [ 5.1143e+00,  9.8280e-01, -1.6624e+00,  2.0361e+00],\n",
       "                       [ 2.6092e+00,  1.0822e+00, -2.1110e+00, -2.4155e+00],\n",
       "                       [ 2.1954e+00,  1.7525e+00,  9.9558e-01,  1.6372e+00],\n",
       "                       [ 9.0637e-01,  2.6014e+00, -3.5625e+00,  2.7289e+00],\n",
       "                       [ 5.6986e-01, -2.0559e+00, -2.3784e+00,  3.7730e+00],\n",
       "                       [-5.1375e-02,  3.1690e-01,  2.0454e+00,  3.5556e+00],\n",
       "                       [-3.8810e-01,  4.7981e-01, -1.4502e+00, -1.8393e+00],\n",
       "                       [-1.1878e+00,  1.6984e+00, -9.8528e-01, -1.8763e+00],\n",
       "                       [-2.1434e+00,  9.6462e-01, -1.2917e+00,  1.2481e+00],\n",
       "                       [ 2.0735e+00,  1.3688e-01, -6.4411e-01, -1.3728e+00],\n",
       "                       [ 2.7628e+00, -2.4789e+00, -2.2364e+00, -3.1731e+00],\n",
       "                       [-6.2352e-01, -4.6462e+00,  1.1275e+00,  3.3647e+00],\n",
       "                       [-3.2430e+00, -1.5398e+00, -7.0138e-01,  2.0712e+00],\n",
       "                       [ 1.8665e+00, -5.9010e-01, -1.1606e+00, -4.7606e-01],\n",
       "                       [-3.3511e-01,  2.3429e+00,  2.1284e-01,  7.1653e+00],\n",
       "                       [ 3.8770e+00, -4.3565e+00,  3.9797e+00,  2.5585e+00],\n",
       "                       [ 1.7390e-01,  9.4626e-01, -5.2073e+00,  4.1921e-01],\n",
       "                       [-9.9259e-01, -3.0123e+00,  2.8412e+00,  5.7181e+00],\n",
       "                       [ 1.1564e+00,  3.9278e+00,  1.5518e+00,  2.3629e+00],\n",
       "                       [ 1.8177e+00,  1.7545e+00,  1.2955e-01,  2.0086e+00],\n",
       "                       [ 2.9286e+00,  1.2179e+00,  2.3124e+00, -2.0012e+00],\n",
       "                       [ 2.2663e+00, -6.8511e+00, -2.5852e-01, -2.0011e+00],\n",
       "                       [ 2.1984e+00,  1.7001e+00, -2.7891e+00, -1.4819e+00],\n",
       "                       [ 6.8954e-01,  8.4345e-01,  1.5152e+00,  5.9931e-01],\n",
       "                       [ 2.2180e-01,  5.5225e-01, -7.5740e-01,  1.9650e+00],\n",
       "                       [-2.5688e+00, -4.7797e+00,  4.9136e-01, -4.8485e+00],\n",
       "                       [-1.6593e-01,  2.5097e+00, -3.0361e+00, -7.1702e-01],\n",
       "                       [ 2.3809e+00, -1.9671e-01, -1.8293e+00,  1.8404e+00],\n",
       "                       [-4.4450e+00, -4.4977e+00,  1.4193e+00,  3.0733e+00],\n",
       "                       [ 2.4279e+00,  9.3113e-01,  2.3614e+00, -1.4958e+00],\n",
       "                       [-1.9946e-01, -1.1510e+00,  6.3227e-01, -2.5714e+00],\n",
       "                       [ 4.0403e-01, -1.4216e+00, -1.4689e+00,  9.2480e-01],\n",
       "                       [ 2.2226e+00, -3.3730e-01, -3.4159e-01, -3.4883e+00],\n",
       "                       [ 3.2056e+00, -3.0046e+00,  2.1844e+00, -2.8802e+00],\n",
       "                       [-2.8202e+00, -3.7448e-01, -1.7907e+00,  4.9126e+00],\n",
       "                       [ 3.9021e+00, -1.6769e-01, -4.2322e+00,  2.2044e+00],\n",
       "                       [-4.1318e+00,  2.4885e+00,  2.2243e+00, -1.4943e-01],\n",
       "                       [ 2.3066e+00, -3.2383e+00,  5.8375e-01, -3.1370e+00],\n",
       "                       [-4.9876e+00, -6.3811e-01,  4.5229e+00, -2.3536e+00],\n",
       "                       [ 2.6786e-01,  8.6819e-01, -1.9507e+00,  8.2356e-01],\n",
       "                       [-2.0082e+00,  1.2087e+00,  2.3098e+00, -1.4261e+00],\n",
       "                       [-5.1821e+00, -1.8003e+00,  2.5836e+00, -1.7138e+00],\n",
       "                       [-2.9150e+00,  3.8845e-01, -1.4584e-01,  2.4689e+00],\n",
       "                       [-6.8227e-03, -8.9688e-01, -1.6833e-01,  9.0653e-01],\n",
       "                       [ 2.6425e+00,  1.1530e+00,  2.6887e+00, -2.0282e+00],\n",
       "                       [ 5.7557e-02, -1.8653e+00, -4.1710e+00, -6.1764e+00],\n",
       "                       [-2.4052e+00,  2.4883e+00, -5.7017e+00,  4.4315e+00],\n",
       "                       [ 2.6327e-01, -1.0347e+00,  1.9069e+00, -3.3961e+00],\n",
       "                       [-5.5383e+00,  3.6492e+00, -9.6316e-01,  1.7033e+00],\n",
       "                       [-1.5596e+00,  1.4218e+00,  1.6947e+00, -8.1562e-01],\n",
       "                       [ 3.0178e+00, -2.3588e+00,  1.3877e+00,  1.5406e-01],\n",
       "                       [-5.8409e+00,  8.9803e-01, -3.7823e+00, -1.9203e+00],\n",
       "                       [ 4.7430e-01, -7.4729e-01,  1.5331e+00, -5.8310e-01],\n",
       "                       [ 8.6475e-01, -2.1233e+00,  1.0684e+00, -1.9467e+00],\n",
       "                       [ 5.2926e+00,  8.4075e-01, -2.4931e+00, -5.3826e+00],\n",
       "                       [-6.3705e-01,  2.2584e+00, -4.3122e+00,  8.0805e-01],\n",
       "                       [ 1.1634e+00, -3.8109e+00, -5.7306e+00, -4.6506e-01],\n",
       "                       [ 7.6098e-01, -3.7965e+00,  4.1011e+00,  6.5271e-01],\n",
       "                       [ 3.4787e+00, -2.2820e+00,  2.3494e+00,  9.8773e-02],\n",
       "                       [ 6.4164e+00, -1.9164e+00, -8.2487e-01,  8.0826e-01],\n",
       "                       [ 3.4076e+00,  2.2689e+00, -8.9543e-01, -1.0732e+00]])),\n",
       "              ('mlp_extractor.value_net.0.bias',\n",
       "               tensor([ 7.0117e+00,  8.0838e+00,  8.0896e+00,  8.1102e+00, -7.7904e+00,\n",
       "                       -3.7196e-01, -8.1913e+00, -1.1722e-01, -2.8651e-01,  7.3125e+00,\n",
       "                        4.4692e-01,  7.4560e+00,  1.1790e-02, -7.3029e+00, -5.4682e-01,\n",
       "                        6.5524e-01, -7.5238e-03,  5.3154e-02, -7.3215e+00,  1.2273e-01,\n",
       "                        6.3682e+00, -1.2620e-01, -7.5638e+00,  5.5192e-01,  2.1113e-01,\n",
       "                       -7.5807e+00,  7.5453e+00, -7.6584e+00,  5.1594e-02,  7.7912e+00,\n",
       "                        7.5379e+00,  6.6292e+00, -6.6742e-01, -2.6950e-01, -7.0168e+00,\n",
       "                       -7.5851e+00, -3.6314e-02, -1.9773e-01,  5.5159e-01, -7.4111e+00,\n",
       "                       -7.6523e+00,  3.8229e-01,  7.8316e+00, -8.1280e+00,  3.5549e-01,\n",
       "                       -2.3695e-01, -2.7601e-01, -6.9485e+00,  5.7200e-01,  7.8174e+00,\n",
       "                        1.7133e-01,  1.6605e-01,  7.2384e+00,  7.1911e+00, -7.7759e+00,\n",
       "                       -2.9810e-01, -5.4823e-01,  7.3124e+00,  7.6053e+00, -7.5924e+00,\n",
       "                        7.6084e+00,  4.9166e-01, -7.3317e+00,  6.9735e-01])),\n",
       "              ('mlp_extractor.value_net.2.weight',\n",
       "               tensor([[-6.9435, -5.1429, -6.8366,  ...,  4.3387,  7.9542,  3.0726],\n",
       "                       [ 3.8306, -2.7932,  2.1862,  ..., -4.4659, -0.0172,  3.3463],\n",
       "                       [ 0.8041, -7.0344, -7.1954,  ...,  2.3568,  9.2806, -0.8751],\n",
       "                       ...,\n",
       "                       [ 7.8454,  4.8587,  6.4940,  ..., -1.6138, -2.0126,  2.0453],\n",
       "                       [-2.1677, -3.4132, -3.8366,  ..., -5.8677, -1.5665,  5.0338],\n",
       "                       [-0.9885, -7.5786, -4.4150,  ...,  4.7702,  2.2572, -3.9356]])),\n",
       "              ('mlp_extractor.value_net.2.bias',\n",
       "               tensor([-4.1339, -0.6408, -5.4144, -5.1216, -4.1836, -1.0117,  5.5654,  5.3042,\n",
       "                       -4.9736, -5.5290, -4.2702,  0.6885,  0.9838,  0.3337, -0.2724, -0.0473,\n",
       "                       -0.1896, -5.6975,  0.5949,  0.2350,  0.1872,  0.4197, -4.3421,  0.3906,\n",
       "                        0.1152, -0.5764,  0.6294,  5.4817, -4.3165,  5.1635, -0.6193,  0.4230,\n",
       "                       -5.3538,  4.7246, -0.4046,  4.4335, -0.6267,  0.8496, -0.1144, -0.3231,\n",
       "                        4.9771,  4.6406, -0.9211,  4.2362, -5.4922,  4.2782, -4.1607, -0.6267,\n",
       "                        4.5564,  0.2291, -0.5422, -4.9601, -0.6901, -0.3600,  0.3795,  4.9720,\n",
       "                       -4.7527, -4.0998,  0.6039,  4.9249,  5.6868,  4.8902, -0.8246, -4.6392])),\n",
       "              ('action_net.weight',\n",
       "               tensor([[-1.2804,  0.7854, -1.3792, -0.8581,  0.4557,  0.0856, -1.2867,  0.8062,\n",
       "                        -1.3004, -0.7222, -0.9844, -1.5443,  1.2828, -0.8789,  0.1963, -0.3858,\n",
       "                         0.5514,  1.1199, -1.1973, -0.6253, -0.4840, -0.3655, -0.5432, -0.5889,\n",
       "                        -0.3546, -0.6262,  0.8130, -1.2969, -1.5577,  1.1396,  1.0783,  0.6398,\n",
       "                         0.1679,  1.1625, -0.9039, -0.4195,  0.4603,  1.1553,  0.2749,  0.3164,\n",
       "                        -0.1457,  0.6438,  1.5482,  1.3678,  0.2494, -1.5964, -0.9466, -0.2840,\n",
       "                         1.0762,  1.3933,  0.2360,  1.1065, -1.0029,  0.9839,  1.0526, -0.6025,\n",
       "                        -1.1454,  1.2336, -1.3995,  0.1768, -0.2600,  0.0358,  0.6307,  1.0058],\n",
       "                       [ 1.2191, -0.7657,  1.3881,  0.8936, -0.4947, -0.0934,  1.2642, -0.8121,\n",
       "                         1.2984,  0.6718,  0.9732,  1.4970, -1.2878,  0.8626, -0.1867,  0.3809,\n",
       "                        -0.5942, -1.0906,  1.1636,  0.6244,  0.4470,  0.3343,  0.5249,  0.5693,\n",
       "                         0.3699,  0.6470, -0.8474,  1.2375,  1.5936, -1.1496, -1.0792, -0.6017,\n",
       "                        -0.1638, -1.1750,  0.8845,  0.4186, -0.4304, -1.1632, -0.2822, -0.3266,\n",
       "                         0.1767, -0.6281, -1.5145, -1.3320, -0.2675,  1.5942,  0.9315,  0.2776,\n",
       "                        -1.0798, -1.3925, -0.2462, -1.1077,  0.9747, -0.9636, -1.0712,  0.6190,\n",
       "                         1.1171, -1.2084,  1.4022, -0.1442,  0.2824, -0.0288, -0.5877, -0.9558]])),\n",
       "              ('action_net.bias', tensor([ 0.2741, -0.2741])),\n",
       "              ('value_net.weight',\n",
       "               tensor([[-2.2850e+01,  2.1427e+00, -1.8021e+01, -1.8650e+01, -2.1024e+01,\n",
       "                         3.2120e+00,  1.8808e+01,  1.9066e+01, -1.9533e+01, -1.8623e+01,\n",
       "                        -2.0849e+01, -1.4083e+00, -2.4015e+00, -2.1900e+00,  7.5624e-03,\n",
       "                         1.6170e+00,  3.2220e-01, -1.8523e+01, -1.1437e+00, -7.0271e-02,\n",
       "                         3.6216e-01, -1.4285e+00, -2.1081e+01, -1.4764e+00, -9.1492e-02,\n",
       "                        -3.1726e-01, -1.1804e+00,  1.8269e+01, -2.0443e+01,  2.0202e+01,\n",
       "                         2.5198e+00, -5.9808e-01, -1.8356e+01,  2.0160e+01,  7.3908e-01,\n",
       "                         2.0613e+01,  3.0386e+00, -1.8408e+00,  2.4357e-01,  1.2145e+00,\n",
       "                         2.0859e+01,  2.1482e+01,  1.9912e+00,  2.1031e+01, -1.8195e+01,\n",
       "                         2.2111e+01, -2.1062e+01,  1.9477e+00,  1.9307e+01,  9.4374e-02,\n",
       "                         6.4176e-01, -1.8731e+01,  9.8058e-01,  8.0161e-01,  5.2367e-01,\n",
       "                         1.9323e+01, -2.0896e+01, -2.1848e+01, -1.9434e+00,  1.9969e+01,\n",
       "                         1.7792e+01,  1.8718e+01,  2.2468e+00, -2.1873e+01]])),\n",
       "              ('value_net.bias', tensor([16.4884]))]),\n",
       " 'policy.optimizer': {'state': {},\n",
       "  'param_groups': [{'lr': 0.0007,\n",
       "    'momentum': 0,\n",
       "    'alpha': 0.99,\n",
       "    'eps': 1e-05,\n",
       "    'centered': False,\n",
       "    'weight_decay': 0,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('a2c_lunar_multiproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Params as JSON\n",
    "## Function to Convert Params Dict to Flattened List\n",
    "def flatten_list(params):\n",
    "    \"\"\"\n",
    "    :param params: (dict)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    params_ = {}\n",
    "    for key in params.keys():\n",
    "        params_[key] = params[key].tolist()\n",
    "    return params_\n",
    "## Write Parameters to JSON File\n",
    "import json\n",
    "\n",
    "all_params = model.get_parameters()\n",
    "pol_params = flatten_list(all_params['policy'])\n",
    "\n",
    "all_params['policy'] = pol_params\n",
    "\n",
    "with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "    json.dump(all_params, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 285.4 1 150.9 2 149.6 3 153.2 4 146.2 5 235.6 6 233.5 7 151.5 8 243.4 9 287.2 Type  Mean reward: 203.65000000000003\n"
     ]
    }
   ],
   "source": [
    "model_loaded = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "evaluate(model_loaded,env, verbose=1)\n",
    "\n",
    "new_params = all_params\n",
    "loaded_pol_params = new_params['policy']\n",
    "for key in loaded_pol_params.keys():\n",
    "    loaded_pol_params[key] = th.tensor(loaded_pol_params[key])\n",
    "\n",
    "new_params['policy'] = loaded_pol_params\n",
    "\n",
    "model_loaded.set_parameters(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 153.9 1 128.9 2 182.4 3 145.9 4 161.4 5 159.1 6 152.4 7 185.1 8 177.8 9 145.6 Type  Mean reward: 159.25\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "evaluate(model_loaded,env, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
