{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Train Gradient Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import threading\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import A2C as ALGO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init. ENV and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_1 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "model_trained_2 = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Evaluate Model and Train Model within Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, message = '', verbose = 0):\n",
    "    fitnesses = []\n",
    "    iterations = 10\n",
    "    for i in range(iterations):\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        if verbose == 1:\n",
    "            print(i, fitness, end=\" \")\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    mean_fitness = np.mean(sorted(fitnesses))\n",
    "    print(f'Type {message} Mean reward: {mean_fitness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, timesteps):\n",
    "    print('Starting Training')\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    print('Completed Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 9.110000000000001\n",
      "Type  Mean reward: 9.73\n",
      "Type  Mean reward: 86.55\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, env)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 1K Steps and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "Type  Mean reward: 50.940000000000005\n",
      "Type  Mean reward: 9.309999999999999\n",
      "Type  Mean reward: 9.24\n"
     ]
    }
   ],
   "source": [
    "# Train MT Model 1\n",
    "t1 = threading.Thread(target=train, args=(model_trained_1, 10_00))\n",
    "\n",
    "# Train MT Model 2\n",
    "t2 = threading.Thread(target=train, args=(model_trained_2, 10_00))\n",
    "\n",
    "# starting thread\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until thread is completely executed\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "\n",
    "# model_trained.learn(total_timesteps=10_00)\n",
    "evaluate(model_trained_1, env)\n",
    "evaluate(model_trained_2, env)\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Gradient and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  if __name__ == '__main__':\n",
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 101.23\n"
     ]
    }
   ],
   "source": [
    "# For Trained Model 1\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "\n",
    "# For Trained Model 2\n",
    "state_dict = model.policy.state_dict()\n",
    "optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "optim_index = 0\n",
    "for key, value in state_dict.items():\n",
    "    # print(key)\n",
    "    state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "    optim_index += 1\n",
    "\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model_trained_1.policy.load_state_dict(state_dict)\n",
    "model_trained_2.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iter:  0\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 126.3 1 164.4 2 151.6 3 186.8 4 101.8 5 137.1 6 102.7 7 160.3 8 162.3 9 136.6 Type Trained Model 1 Mean reward: 142.98999999999998\n",
      "0 51.6 1 65.1 2 69.1 3 75.1 4 67.5 5 65.1 6 69.7 7 56.0 8 59.0 9 64.8 Type Trained Model 2 Mean reward: 64.3\n",
      "0 68.3 1 105.4 2 93.3 3 63.1 4 60.5 5 108.2 6 61.7 7 111.2 8 73.3 9 107.5 Type Initial Model Mean reward: 85.25000000000001\n",
      "0 101.7 1 84.8 2 107.0 3 151.5 4 131.2 5 145.4 6 118.4 7 109.2 8 184.5 9 117.0 Type Updated Model Mean reward: 125.07000000000001\n",
      "Train Iter:  1\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 181.3 1 186.0 2 224.1 3 147.6 4 237.3 5 145.3 6 194.1 7 200.3 8 158.6 9 225.8 Type Trained Model 1 Mean reward: 190.04\n",
      "0 155.0 1 149.2 2 165.2 3 156.5 4 166.4 5 161.9 6 176.8 7 155.2 8 147.2 9 159.7 Type Trained Model 2 Mean reward: 159.31\n",
      "0 128.1 1 179.7 2 88.6 3 96.8 4 134.1 5 79.2 6 147.6 7 143.8 8 78.1 9 117.2 Type Initial Model Mean reward: 119.32000000000001\n",
      "0 164.8 1 116.1 2 152.1 3 227.6 4 144.9 5 169.0 6 161.3 7 162.2 8 135.4 9 140.7 Type Updated Model Mean reward: 157.41\n",
      "Train Iter:  2\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 203.8 1 191.3 2 211.1 3 227.2 4 193.1 5 115.9 6 155.9 7 207.3 8 157.8 9 211.5 Type Trained Model 1 Mean reward: 187.49\n",
      "0 265.4 1 354.1 2 364.3 3 325.5 4 297.2 5 379.3 6 274.7 7 302.5 8 413.4 9 289.2 Type Trained Model 2 Mean reward: 326.56000000000006\n",
      "0 202.7 1 189.4 2 146.3 3 211.9 4 145.6 5 165.0 6 211.5 7 175.8 8 98.7 9 156.3 Type Initial Model Mean reward: 170.32000000000002\n",
      "0 151.2 1 111.0 2 150.4 3 228.6 4 234.6 5 216.8 6 201.5 7 201.4 8 116.1 9 193.6 Type Updated Model Mean reward: 180.51999999999998\n",
      "Train Iter:  3\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 143.7 1 102.4 2 136.4 3 142.8 4 110.2 5 149.6 6 199.6 7 98.1 8 137.9 9 141.5 Type Trained Model 1 Mean reward: 136.21999999999997\n",
      "0 315.1 1 348.7 2 255.6 3 371.3 4 351.0 5 395.2 6 309.7 7 328.2 8 235.9 9 272.8 Type Trained Model 2 Mean reward: 318.35\n",
      "0 144.0 1 188.5 2 113.8 3 224.2 4 235.8 5 231.7 6 158.1 7 150.4 8 226.5 9 135.4 Type Initial Model Mean reward: 180.83999999999997\n",
      "0 180.9 1 158.2 2 201.8 3 189.7 4 219.8 5 235.5 6 208.8 7 194.1 8 165.4 9 272.0 Type Updated Model Mean reward: 202.62\n",
      "Train Iter:  4\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 78.9 1 94.6 2 84.5 3 78.5 4 92.1 5 79.2 6 81.8 7 68.0 8 74.7 9 67.6 Type Trained Model 1 Mean reward: 79.99000000000001\n",
      "0 223.4 1 239.7 2 195.6 3 210.2 4 202.1 5 214.6 6 205.1 7 221.9 8 196.2 9 212.6 Type Trained Model 2 Mean reward: 212.14000000000001\n",
      "0 231.7 1 303.8 2 254.0 3 166.6 4 184.0 5 220.4 6 288.3 7 238.2 8 252.2 9 246.0 Type Initial Model Mean reward: 238.52000000000004\n",
      "0 189.1 1 265.3 2 231.3 3 308.1 4 285.3 5 178.3 6 246.9 7 229.4 8 230.0 9 190.1 Type Updated Model Mean reward: 235.38000000000002\n",
      "Train Iter:  5\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 84.4 1 129.5 2 74.3 3 73.2 4 99.7 5 67.8 6 87.7 7 65.8 8 99.4 9 65.9 Type Trained Model 1 Mean reward: 84.77000000000001\n",
      "0 173.4 1 167.8 2 164.4 3 164.5 4 145.8 5 165.0 6 194.9 7 150.7 8 171.5 9 176.0 Type Trained Model 2 Mean reward: 167.4\n",
      "0 208.3 1 217.5 2 182.1 3 158.2 4 210.0 5 195.7 6 200.7 7 277.9 8 217.3 9 267.8 Type Initial Model Mean reward: 213.55\n",
      "0 310.4 1 257.8 2 265.8 3 204.2 4 237.1 5 184.4 6 190.7 7 275.9 8 209.2 9 272.3 Type Updated Model Mean reward: 240.78000000000003\n",
      "Train Iter:  6\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 111.9 1 87.5 2 115.1 3 93.5 4 99.0 5 123.7 6 106.6 7 104.9 8 99.6 9 94.1 Type Trained Model 1 Mean reward: 103.59\n",
      "0 133.9 1 125.0 2 135.7 3 138.8 4 172.9 5 158.0 6 123.5 7 146.7 8 158.6 9 138.6 Type Trained Model 2 Mean reward: 143.17\n",
      "0 261.8 1 293.0 2 255.4 3 258.7 4 252.4 5 242.9 6 198.5 7 337.9 8 207.5 9 318.8 Type Initial Model Mean reward: 262.69\n",
      "0 282.9 1 259.8 2 243.1 3 289.8 4 262.5 5 237.2 6 133.4 7 167.4 8 268.2 9 259.5 Type Updated Model Mean reward: 240.38000000000002\n",
      "Train Iter:  7\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 156.6 1 154.6 2 140.6 3 153.2 4 154.2 5 158.2 6 147.0 7 153.4 8 160.1 9 147.5 Type Trained Model 1 Mean reward: 152.54\n",
      "0 127.4 1 123.3 2 118.6 3 187.4 4 153.9 5 232.4 6 159.5 7 163.1 8 140.3 9 124.8 Type Trained Model 2 Mean reward: 153.07000000000002\n",
      "0 248.0 1 299.1 2 211.5 3 211.9 4 216.7 5 175.6 6 195.3 7 235.7 8 272.0 9 263.1 Type Initial Model Mean reward: 232.89000000000001\n",
      "0 278.1 1 244.0 2 167.3 3 216.4 4 180.4 5 166.7 6 226.0 7 160.7 8 253.1 9 156.0 Type Updated Model Mean reward: 204.86999999999998\n",
      "Train Iter:  8\n",
      "Starting TrainingStarting Training\n",
      "\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 174.9 1 185.3 2 202.2 3 178.3 4 181.9 5 179.5 6 194.6 7 193.7 8 193.9 9 173.5 Type Trained Model 1 Mean reward: 185.78\n",
      "0 170.9 1 178.1 2 172.1 3 169.2 4 176.6 5 194.9 6 260.6 7 200.6 8 119.5 9 250.6 Type Trained Model 2 Mean reward: 189.31\n",
      "0 155.1 1 238.0 2 230.9 3 178.0 4 259.5 5 214.5 6 228.1 7 177.0 8 159.9 9 275.5 Type Initial Model Mean reward: 211.65\n",
      "0 258.3 1 290.4 2 218.2 3 263.1 4 252.3 5 221.7 6 260.7 7 225.5 8 218.7 9 272.8 Type Updated Model Mean reward: 248.17000000000002\n",
      "Train Iter:  9\n",
      "Starting Training\n",
      "Starting Training\n",
      "Completed Training\n",
      "Completed Training\n",
      "0 200.5 1 212.9 2 198.1 3 230.6 4 185.0 5 202.2 6 208.7 7 195.2 8 178.0 9 223.8 Type Trained Model 1 Mean reward: 203.49999999999997\n",
      "0 210.1 1 268.1 2 219.4 3 158.5 4 216.6 5 183.6 6 186.5 7 216.2 8 215.3 9 222.6 Type Trained Model 2 Mean reward: 209.69\n",
      "0 243.9 1 256.4 2 224.7 3 275.2 4 154.5 5 136.4 6 231.1 7 221.3 8 184.5 9 264.7 Type Initial Model Mean reward: 219.26999999999998\n",
      "0 198.3 1 286.1 2 217.7 3 244.3 4 189.5 5 212.7 6 253.2 7 278.4 8 243.6 9 208.7 Type Updated Model Mean reward: 233.25\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Train Iter: ', i)\n",
    "\n",
    "    # Train MT Model 1\n",
    "    t1 = threading.Thread(target=train, args=(model_trained_1, 10_00))\n",
    "\n",
    "    # Train MT Model 2\n",
    "    t2 = threading.Thread(target=train, args=(model_trained_2, 10_00))\n",
    "\n",
    "    # starting thread\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # wait until thread is completely executed\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    evaluate(model_trained_1, env, 'Trained Model 1', verbose=1)\n",
    "    evaluate(model_trained_2, env, 'Trained Model 2', verbose=1)\n",
    "    evaluate(model, env, 'Initial Model', verbose=1)\n",
    "\n",
    "    # For Trained Model 1\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_1.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "    # For Trained Model 2\n",
    "    state_dict = model.policy.state_dict()\n",
    "    optim_dict = model_trained_2.policy.optimizer.param_groups[0]['params']\n",
    "    optim_alpha = model.policy.optimizer.param_groups[0]['alpha']\n",
    "\n",
    "    optim_index = 0\n",
    "    for key, value in state_dict.items():\n",
    "        # print(key)\n",
    "        state_dict[key].add_(optim_alpha, optim_dict[optim_index])\n",
    "        optim_index += 1\n",
    "\n",
    "    model.policy.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    evaluate(model, env, 'Updated Model', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': OrderedDict([('mlp_extractor.policy_net.0.weight',\n",
       "               tensor([[-4.7994,  2.7285,  1.8746,  4.0797],\n",
       "                       [ 0.7623, -7.0949,  2.1134, -1.0655],\n",
       "                       [ 1.9194,  3.3921, -0.3741,  3.4626],\n",
       "                       [-3.6278, -1.0268, -2.4102,  3.2928],\n",
       "                       [ 1.1961, -0.9835, -2.6474,  4.2740],\n",
       "                       [-3.1046,  3.6106, -5.5946,  0.2108],\n",
       "                       [ 0.1898, -4.1656, -1.2745,  1.0092],\n",
       "                       [-3.5225,  0.8290,  4.2771,  2.9213],\n",
       "                       [-0.1257, -0.5160, -1.3684, -3.4648],\n",
       "                       [-1.7615,  0.8610,  1.5810,  3.8147],\n",
       "                       [-3.9144, -0.5135,  0.4511,  1.2394],\n",
       "                       [ 4.6794,  1.8337, -0.4528, -0.6097],\n",
       "                       [ 3.7276,  0.2791,  3.8379,  4.3595],\n",
       "                       [ 1.5737, -0.0986, -0.5604, -1.5846],\n",
       "                       [ 0.0509,  2.3838, -1.1136,  2.0262],\n",
       "                       [ 0.6830,  0.0303,  0.8477, -5.2035],\n",
       "                       [ 1.8422, -4.4948, -5.3122, -4.0556],\n",
       "                       [-0.6589,  0.6233,  1.8882, -1.9282],\n",
       "                       [ 4.3976,  4.0874,  3.0394, -0.3283],\n",
       "                       [-0.4132, -3.4626,  0.9184, -2.5168],\n",
       "                       [-0.3807,  1.5897,  7.9293,  1.5521],\n",
       "                       [ 2.0764, -1.2970,  2.1397,  5.4734],\n",
       "                       [-5.2327, -1.4966,  1.3400, -0.7254],\n",
       "                       [-2.2385,  1.3108, -3.0078, -1.1979],\n",
       "                       [-0.4858, -3.4949, -0.5876,  4.4911],\n",
       "                       [-1.2264, -1.3892, -0.7526,  2.6735],\n",
       "                       [-3.2186,  1.6114,  0.8041,  2.0411],\n",
       "                       [-0.0587,  0.5145,  0.7180, -0.8354],\n",
       "                       [ 1.2058,  0.4822, -2.3137,  1.2869],\n",
       "                       [ 1.5005, -0.3798, -4.0698, -2.3888],\n",
       "                       [ 2.5727,  6.8803,  5.1863,  0.3076],\n",
       "                       [-4.4956,  2.4163, -2.7145,  6.2767],\n",
       "                       [-1.2540, -2.5383,  1.3568,  4.7728],\n",
       "                       [-0.4514, -2.5015, -4.8832, -1.0919],\n",
       "                       [ 3.5496, -0.3053,  1.0841, -2.3703],\n",
       "                       [ 1.6874, -0.1408, -3.1314,  1.5083],\n",
       "                       [-0.0785, -1.1784,  2.5228,  2.8121],\n",
       "                       [ 1.3972, -0.2470,  6.5250,  2.8049],\n",
       "                       [ 3.7450, -1.3293,  7.6355,  9.0531],\n",
       "                       [-1.1809,  0.7658,  0.0506, -1.3690],\n",
       "                       [-2.0713,  2.3091, -0.8312,  1.9652],\n",
       "                       [-0.1595,  1.6684,  8.9179,  2.8971],\n",
       "                       [-0.5942,  1.8092, -4.4066, -2.3998],\n",
       "                       [ 2.6139,  1.2618,  0.1374, -5.3794],\n",
       "                       [-3.5145,  0.2205, -0.9907, -0.5378],\n",
       "                       [-2.2250, -1.7033, -0.2772, -1.1429],\n",
       "                       [-6.7402,  1.1284, -0.8019,  3.9400],\n",
       "                       [-4.4336,  1.8352,  4.7224,  3.3367],\n",
       "                       [ 1.9292,  1.5645, -0.7023, -2.5437],\n",
       "                       [ 1.7587,  0.2588,  0.2439,  0.9901],\n",
       "                       [-1.8851,  1.0442, -3.4276,  1.5909],\n",
       "                       [ 3.1655,  3.2967, -1.1541, -4.3400],\n",
       "                       [ 0.2946, -2.7394,  7.3321,  2.9176],\n",
       "                       [ 2.3619, -1.9860,  2.6349, -1.9566],\n",
       "                       [-1.5469, -3.5017, -0.3032,  1.8298],\n",
       "                       [ 0.6211,  4.2922,  0.1202,  2.4150],\n",
       "                       [-0.5468,  4.0329,  0.4661,  1.5599],\n",
       "                       [-3.5270, -5.4135, -2.9339, -0.5096],\n",
       "                       [ 0.8614, -2.9678, -0.8254,  0.1202],\n",
       "                       [-2.7220,  5.0215, -2.3877, -0.6494],\n",
       "                       [ 1.0509,  2.1652, -3.0272,  0.8847],\n",
       "                       [ 4.8713, -3.8089, -3.2406, -3.3750],\n",
       "                       [ 0.3989, -0.2534, -1.0118,  1.5615],\n",
       "                       [-0.1301, -1.0020,  1.2667,  2.1611]])),\n",
       "              ('mlp_extractor.policy_net.0.bias',\n",
       "               tensor([ 6.5449e-03, -5.7727e-01, -4.8135e-02,  6.3050e-01,  9.1870e-01,\n",
       "                        4.2468e-01,  1.6058e-04,  1.8470e-01,  2.7916e-01, -2.2331e-02,\n",
       "                       -5.3709e-01,  5.1639e-01,  2.5899e-02,  2.2300e-01,  1.7975e-01,\n",
       "                        4.7131e-01,  4.6057e-01,  3.5805e-01,  4.4185e-01, -6.4345e-01,\n",
       "                       -1.3148e-01,  1.7711e-01, -4.5574e-01, -2.2030e-01, -5.7274e-02,\n",
       "                       -4.0580e-01, -7.8073e-01,  2.8607e-01,  1.0068e+00,  2.3353e-01,\n",
       "                       -1.7450e-01,  3.3908e-02, -2.6204e-01,  5.0406e-01,  3.6703e-01,\n",
       "                        7.7920e-01, -3.8612e-01, -1.0209e-01,  2.4491e-02, -8.1241e-01,\n",
       "                        8.9817e-01, -2.1895e-01, -1.4242e-01, -2.8030e-01, -3.4582e-01,\n",
       "                        9.4929e-01,  9.9471e-02,  4.6736e-02,  4.2476e-01,  7.0884e-01,\n",
       "                        6.9942e-01, -1.7667e-01,  4.1501e-01,  4.0242e-01, -5.9381e-01,\n",
       "                        4.4450e-01, -8.0226e-01, -6.6694e-02, -4.2956e-01,  4.1187e-01,\n",
       "                       -4.8749e-02,  1.6381e-01, -6.5190e-01,  5.4392e-01])),\n",
       "              ('mlp_extractor.policy_net.2.weight',\n",
       "               tensor([[ 2.4340, -0.8301, -4.1288,  ..., -1.8800, -2.1661,  0.0451],\n",
       "                       [-1.0790, -3.7795, -0.6124,  ...,  3.2330, -4.6878,  6.1949],\n",
       "                       [ 0.1757,  0.5501,  5.2680,  ..., -2.2803, -0.2071, -2.5262],\n",
       "                       ...,\n",
       "                       [ 1.4924,  2.1917,  0.4397,  ..., -2.4246,  4.4207,  0.0549],\n",
       "                       [-3.6745,  2.9444, -3.3632,  ...,  0.9183,  0.0955,  1.1136],\n",
       "                       [-3.2052,  1.1830, -6.0765,  ..., -1.3604, -3.2421, -0.9082]])),\n",
       "              ('mlp_extractor.policy_net.2.bias',\n",
       "               tensor([ 0.2669,  0.2354,  0.1842, -0.2820,  0.4454,  0.6026, -0.0233,  0.6927,\n",
       "                       -0.2611,  0.1537,  0.2075, -0.3193,  0.2949,  0.1324,  0.7099, -0.4064,\n",
       "                        0.4994,  0.0950, -0.1679,  0.1263, -0.5091,  0.2110, -0.4761, -0.5750,\n",
       "                       -0.3987, -0.5292, -0.1465,  0.1401, -0.0218,  0.1364,  0.2144, -0.1745,\n",
       "                        0.5569, -0.0845, -0.1149, -0.6421,  0.5003, -0.1540,  0.4387, -0.5127,\n",
       "                        0.4873,  0.4304, -0.1083,  0.2544,  0.5551,  0.0201, -0.1084, -0.5102,\n",
       "                       -0.0743,  0.3786, -0.5411,  0.4540,  0.0107, -0.1847, -0.2991, -0.4065,\n",
       "                       -0.2506,  0.1029, -0.3309,  0.4879, -0.6493, -0.6771,  0.4888, -0.0227])),\n",
       "              ('mlp_extractor.value_net.0.weight',\n",
       "               tensor([[-5.2254e+00, -4.2040e+00, -3.6259e-01, -1.0274e+00],\n",
       "                       [ 4.1751e+00, -2.4434e-01,  5.2376e+00,  1.8009e-01],\n",
       "                       [ 1.4270e+00, -1.0680e+00,  1.2894e+00, -2.1603e+00],\n",
       "                       [ 5.1143e+00,  9.8280e-01, -1.6624e+00,  2.0361e+00],\n",
       "                       [ 2.6092e+00,  1.0822e+00, -2.1110e+00, -2.4155e+00],\n",
       "                       [ 2.1954e+00,  1.7525e+00,  9.9558e-01,  1.6372e+00],\n",
       "                       [ 9.0637e-01,  2.6014e+00, -3.5625e+00,  2.7289e+00],\n",
       "                       [ 5.6986e-01, -2.0559e+00, -2.3784e+00,  3.7730e+00],\n",
       "                       [-5.1375e-02,  3.1690e-01,  2.0454e+00,  3.5556e+00],\n",
       "                       [-3.8810e-01,  4.7981e-01, -1.4502e+00, -1.8393e+00],\n",
       "                       [-1.1878e+00,  1.6984e+00, -9.8528e-01, -1.8763e+00],\n",
       "                       [-2.1434e+00,  9.6462e-01, -1.2917e+00,  1.2481e+00],\n",
       "                       [ 2.0735e+00,  1.3688e-01, -6.4411e-01, -1.3728e+00],\n",
       "                       [ 2.7628e+00, -2.4789e+00, -2.2364e+00, -3.1731e+00],\n",
       "                       [-6.2352e-01, -4.6462e+00,  1.1275e+00,  3.3647e+00],\n",
       "                       [-3.2430e+00, -1.5398e+00, -7.0138e-01,  2.0712e+00],\n",
       "                       [ 1.8665e+00, -5.9010e-01, -1.1606e+00, -4.7606e-01],\n",
       "                       [-3.3511e-01,  2.3429e+00,  2.1284e-01,  7.1653e+00],\n",
       "                       [ 3.8770e+00, -4.3565e+00,  3.9797e+00,  2.5585e+00],\n",
       "                       [ 1.7390e-01,  9.4626e-01, -5.2073e+00,  4.1921e-01],\n",
       "                       [-9.9259e-01, -3.0123e+00,  2.8412e+00,  5.7181e+00],\n",
       "                       [ 1.1564e+00,  3.9278e+00,  1.5518e+00,  2.3629e+00],\n",
       "                       [ 1.8177e+00,  1.7545e+00,  1.2955e-01,  2.0086e+00],\n",
       "                       [ 2.9286e+00,  1.2179e+00,  2.3124e+00, -2.0012e+00],\n",
       "                       [ 2.2663e+00, -6.8511e+00, -2.5852e-01, -2.0011e+00],\n",
       "                       [ 2.1984e+00,  1.7001e+00, -2.7891e+00, -1.4819e+00],\n",
       "                       [ 6.8954e-01,  8.4345e-01,  1.5152e+00,  5.9931e-01],\n",
       "                       [ 2.2180e-01,  5.5225e-01, -7.5740e-01,  1.9650e+00],\n",
       "                       [-2.5688e+00, -4.7797e+00,  4.9136e-01, -4.8485e+00],\n",
       "                       [-1.6593e-01,  2.5097e+00, -3.0361e+00, -7.1702e-01],\n",
       "                       [ 2.3809e+00, -1.9671e-01, -1.8293e+00,  1.8404e+00],\n",
       "                       [-4.4450e+00, -4.4977e+00,  1.4193e+00,  3.0733e+00],\n",
       "                       [ 2.4279e+00,  9.3113e-01,  2.3614e+00, -1.4958e+00],\n",
       "                       [-1.9946e-01, -1.1510e+00,  6.3227e-01, -2.5714e+00],\n",
       "                       [ 4.0403e-01, -1.4216e+00, -1.4689e+00,  9.2480e-01],\n",
       "                       [ 2.2226e+00, -3.3730e-01, -3.4159e-01, -3.4883e+00],\n",
       "                       [ 3.2056e+00, -3.0046e+00,  2.1844e+00, -2.8802e+00],\n",
       "                       [-2.8202e+00, -3.7448e-01, -1.7907e+00,  4.9126e+00],\n",
       "                       [ 3.9021e+00, -1.6769e-01, -4.2322e+00,  2.2044e+00],\n",
       "                       [-4.1318e+00,  2.4885e+00,  2.2243e+00, -1.4943e-01],\n",
       "                       [ 2.3066e+00, -3.2383e+00,  5.8375e-01, -3.1370e+00],\n",
       "                       [-4.9876e+00, -6.3811e-01,  4.5229e+00, -2.3536e+00],\n",
       "                       [ 2.6786e-01,  8.6819e-01, -1.9507e+00,  8.2356e-01],\n",
       "                       [-2.0082e+00,  1.2087e+00,  2.3098e+00, -1.4261e+00],\n",
       "                       [-5.1821e+00, -1.8003e+00,  2.5836e+00, -1.7138e+00],\n",
       "                       [-2.9150e+00,  3.8845e-01, -1.4584e-01,  2.4689e+00],\n",
       "                       [-6.8227e-03, -8.9688e-01, -1.6833e-01,  9.0653e-01],\n",
       "                       [ 2.6425e+00,  1.1530e+00,  2.6887e+00, -2.0282e+00],\n",
       "                       [ 5.7557e-02, -1.8653e+00, -4.1710e+00, -6.1764e+00],\n",
       "                       [-2.4052e+00,  2.4883e+00, -5.7017e+00,  4.4315e+00],\n",
       "                       [ 2.6327e-01, -1.0347e+00,  1.9069e+00, -3.3961e+00],\n",
       "                       [-5.5383e+00,  3.6492e+00, -9.6316e-01,  1.7033e+00],\n",
       "                       [-1.5596e+00,  1.4218e+00,  1.6947e+00, -8.1562e-01],\n",
       "                       [ 3.0178e+00, -2.3588e+00,  1.3877e+00,  1.5406e-01],\n",
       "                       [-5.8409e+00,  8.9803e-01, -3.7823e+00, -1.9203e+00],\n",
       "                       [ 4.7430e-01, -7.4729e-01,  1.5331e+00, -5.8310e-01],\n",
       "                       [ 8.6475e-01, -2.1233e+00,  1.0684e+00, -1.9467e+00],\n",
       "                       [ 5.2926e+00,  8.4075e-01, -2.4931e+00, -5.3826e+00],\n",
       "                       [-6.3705e-01,  2.2584e+00, -4.3122e+00,  8.0805e-01],\n",
       "                       [ 1.1634e+00, -3.8109e+00, -5.7306e+00, -4.6506e-01],\n",
       "                       [ 7.6098e-01, -3.7965e+00,  4.1011e+00,  6.5271e-01],\n",
       "                       [ 3.4787e+00, -2.2820e+00,  2.3494e+00,  9.8773e-02],\n",
       "                       [ 6.4164e+00, -1.9164e+00, -8.2487e-01,  8.0826e-01],\n",
       "                       [ 3.4076e+00,  2.2689e+00, -8.9543e-01, -1.0732e+00]])),\n",
       "              ('mlp_extractor.value_net.0.bias',\n",
       "               tensor([ 7.0117e+00,  8.0838e+00,  8.0896e+00,  8.1102e+00, -7.7904e+00,\n",
       "                       -3.7196e-01, -8.1913e+00, -1.1722e-01, -2.8651e-01,  7.3125e+00,\n",
       "                        4.4692e-01,  7.4560e+00,  1.1790e-02, -7.3029e+00, -5.4682e-01,\n",
       "                        6.5524e-01, -7.5238e-03,  5.3154e-02, -7.3215e+00,  1.2273e-01,\n",
       "                        6.3682e+00, -1.2620e-01, -7.5638e+00,  5.5192e-01,  2.1113e-01,\n",
       "                       -7.5807e+00,  7.5453e+00, -7.6584e+00,  5.1594e-02,  7.7912e+00,\n",
       "                        7.5379e+00,  6.6292e+00, -6.6742e-01, -2.6950e-01, -7.0168e+00,\n",
       "                       -7.5851e+00, -3.6314e-02, -1.9773e-01,  5.5159e-01, -7.4111e+00,\n",
       "                       -7.6523e+00,  3.8229e-01,  7.8316e+00, -8.1280e+00,  3.5549e-01,\n",
       "                       -2.3695e-01, -2.7601e-01, -6.9485e+00,  5.7200e-01,  7.8174e+00,\n",
       "                        1.7133e-01,  1.6605e-01,  7.2384e+00,  7.1911e+00, -7.7759e+00,\n",
       "                       -2.9810e-01, -5.4823e-01,  7.3124e+00,  7.6053e+00, -7.5924e+00,\n",
       "                        7.6084e+00,  4.9166e-01, -7.3317e+00,  6.9735e-01])),\n",
       "              ('mlp_extractor.value_net.2.weight',\n",
       "               tensor([[-6.9435, -5.1429, -6.8366,  ...,  4.3387,  7.9542,  3.0726],\n",
       "                       [ 3.8306, -2.7932,  2.1862,  ..., -4.4659, -0.0172,  3.3463],\n",
       "                       [ 0.8041, -7.0344, -7.1954,  ...,  2.3568,  9.2806, -0.8751],\n",
       "                       ...,\n",
       "                       [ 7.8454,  4.8587,  6.4940,  ..., -1.6138, -2.0126,  2.0453],\n",
       "                       [-2.1677, -3.4132, -3.8366,  ..., -5.8677, -1.5665,  5.0338],\n",
       "                       [-0.9885, -7.5786, -4.4150,  ...,  4.7702,  2.2572, -3.9356]])),\n",
       "              ('mlp_extractor.value_net.2.bias',\n",
       "               tensor([-4.1339, -0.6408, -5.4144, -5.1216, -4.1836, -1.0117,  5.5654,  5.3042,\n",
       "                       -4.9736, -5.5290, -4.2702,  0.6885,  0.9838,  0.3337, -0.2724, -0.0473,\n",
       "                       -0.1896, -5.6975,  0.5949,  0.2350,  0.1872,  0.4197, -4.3421,  0.3906,\n",
       "                        0.1152, -0.5764,  0.6294,  5.4817, -4.3165,  5.1635, -0.6193,  0.4230,\n",
       "                       -5.3538,  4.7246, -0.4046,  4.4335, -0.6267,  0.8496, -0.1144, -0.3231,\n",
       "                        4.9771,  4.6406, -0.9211,  4.2362, -5.4922,  4.2782, -4.1607, -0.6267,\n",
       "                        4.5564,  0.2291, -0.5422, -4.9601, -0.6901, -0.3600,  0.3795,  4.9720,\n",
       "                       -4.7527, -4.0998,  0.6039,  4.9249,  5.6868,  4.8902, -0.8246, -4.6392])),\n",
       "              ('action_net.weight',\n",
       "               tensor([[-1.2804,  0.7854, -1.3792, -0.8581,  0.4557,  0.0856, -1.2867,  0.8062,\n",
       "                        -1.3004, -0.7222, -0.9844, -1.5443,  1.2828, -0.8789,  0.1963, -0.3858,\n",
       "                         0.5514,  1.1199, -1.1973, -0.6253, -0.4840, -0.3655, -0.5432, -0.5889,\n",
       "                        -0.3546, -0.6262,  0.8130, -1.2969, -1.5577,  1.1396,  1.0783,  0.6398,\n",
       "                         0.1679,  1.1625, -0.9039, -0.4195,  0.4603,  1.1553,  0.2749,  0.3164,\n",
       "                        -0.1457,  0.6438,  1.5482,  1.3678,  0.2494, -1.5964, -0.9466, -0.2840,\n",
       "                         1.0762,  1.3933,  0.2360,  1.1065, -1.0029,  0.9839,  1.0526, -0.6025,\n",
       "                        -1.1454,  1.2336, -1.3995,  0.1768, -0.2600,  0.0358,  0.6307,  1.0058],\n",
       "                       [ 1.2191, -0.7657,  1.3881,  0.8936, -0.4947, -0.0934,  1.2642, -0.8121,\n",
       "                         1.2984,  0.6718,  0.9732,  1.4970, -1.2878,  0.8626, -0.1867,  0.3809,\n",
       "                        -0.5942, -1.0906,  1.1636,  0.6244,  0.4470,  0.3343,  0.5249,  0.5693,\n",
       "                         0.3699,  0.6470, -0.8474,  1.2375,  1.5936, -1.1496, -1.0792, -0.6017,\n",
       "                        -0.1638, -1.1750,  0.8845,  0.4186, -0.4304, -1.1632, -0.2822, -0.3266,\n",
       "                         0.1767, -0.6281, -1.5145, -1.3320, -0.2675,  1.5942,  0.9315,  0.2776,\n",
       "                        -1.0798, -1.3925, -0.2462, -1.1077,  0.9747, -0.9636, -1.0712,  0.6190,\n",
       "                         1.1171, -1.2084,  1.4022, -0.1442,  0.2824, -0.0288, -0.5877, -0.9558]])),\n",
       "              ('action_net.bias', tensor([ 0.2741, -0.2741])),\n",
       "              ('value_net.weight',\n",
       "               tensor([[-2.2850e+01,  2.1427e+00, -1.8021e+01, -1.8650e+01, -2.1024e+01,\n",
       "                         3.2120e+00,  1.8808e+01,  1.9066e+01, -1.9533e+01, -1.8623e+01,\n",
       "                        -2.0849e+01, -1.4083e+00, -2.4015e+00, -2.1900e+00,  7.5624e-03,\n",
       "                         1.6170e+00,  3.2220e-01, -1.8523e+01, -1.1437e+00, -7.0271e-02,\n",
       "                         3.6216e-01, -1.4285e+00, -2.1081e+01, -1.4764e+00, -9.1492e-02,\n",
       "                        -3.1726e-01, -1.1804e+00,  1.8269e+01, -2.0443e+01,  2.0202e+01,\n",
       "                         2.5198e+00, -5.9808e-01, -1.8356e+01,  2.0160e+01,  7.3908e-01,\n",
       "                         2.0613e+01,  3.0386e+00, -1.8408e+00,  2.4357e-01,  1.2145e+00,\n",
       "                         2.0859e+01,  2.1482e+01,  1.9912e+00,  2.1031e+01, -1.8195e+01,\n",
       "                         2.2111e+01, -2.1062e+01,  1.9477e+00,  1.9307e+01,  9.4374e-02,\n",
       "                         6.4176e-01, -1.8731e+01,  9.8058e-01,  8.0161e-01,  5.2367e-01,\n",
       "                         1.9323e+01, -2.0896e+01, -2.1848e+01, -1.9434e+00,  1.9969e+01,\n",
       "                         1.7792e+01,  1.8718e+01,  2.2468e+00, -2.1873e+01]])),\n",
       "              ('value_net.bias', tensor([16.4884]))]),\n",
       " 'policy.optimizer': {'state': {},\n",
       "  'param_groups': [{'lr': 0.0007,\n",
       "    'momentum': 0,\n",
       "    'alpha': 0.99,\n",
       "    'eps': 1e-05,\n",
       "    'centered': False,\n",
       "    'weight_decay': 0,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('a2c_lunar_multiproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Params as JSON\n",
    "## Function to Convert Params Dict to Flattened List\n",
    "def flatten_list(params):\n",
    "    \"\"\"\n",
    "    :param params: (dict)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    params_ = {}\n",
    "    for key in params.keys():\n",
    "        params_[key] = params[key].tolist()\n",
    "    return params_\n",
    "## Write Parameters to JSON File\n",
    "import json\n",
    "\n",
    "all_params = model.get_parameters()\n",
    "pol_params = flatten_list(all_params['policy'])\n",
    "\n",
    "all_params['policy'] = pol_params\n",
    "\n",
    "with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "    json.dump(all_params, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 285.4 1 150.9 2 149.6 3 153.2 4 146.2 5 235.6 6 233.5 7 151.5 8 243.4 9 287.2 Type  Mean reward: 203.65000000000003\n"
     ]
    }
   ],
   "source": [
    "model_loaded = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "evaluate(model_loaded,env, verbose=1)\n",
    "\n",
    "new_params = all_params\n",
    "loaded_pol_params = new_params['policy']\n",
    "for key in loaded_pol_params.keys():\n",
    "    loaded_pol_params[key] = th.tensor(loaded_pol_params[key])\n",
    "\n",
    "new_params['policy'] = loaded_pol_params\n",
    "\n",
    "model_loaded.set_parameters(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 153.9 1 128.9 2 182.4 3 145.9 4 161.4 5 159.1 6 152.4 7 185.1 8 177.8 9 145.6 Type  Mean reward: 159.25\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "evaluate(model_loaded,env, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
