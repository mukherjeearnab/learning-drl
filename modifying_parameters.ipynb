{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying and Loading Parameters of Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutate Function to modify Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(params: Dict[str, th.Tensor]) -> Dict[str, th.Tensor]:\n",
    "    \"\"\"Mutate parameters by adding normal noise to them\"\"\"\n",
    "    return dict((name, param + th.randn_like(param)) for name, param in params.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Policy with a small Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    \"CartPole-v1\",\n",
    "    ent_coef=0.0,\n",
    "    policy_kwargs={\"net_arch\": [32]},\n",
    "    seed=0,\n",
    "    learning_rate=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7fc9926a5be0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use traditional actor-critic policy gradient updates to\n",
    "# find good initial parameters\n",
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Policy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlp_extractor.shared_net.0.weight': tensor([[ 1.2296e+00, -1.4302e+00, -1.7079e+00, -8.3761e-01],\n",
       "         [-1.0285e-01,  1.9513e+00,  4.9012e+00,  1.8903e+00],\n",
       "         [-1.3991e-01, -6.8936e-01, -3.2703e+00, -8.2682e-01],\n",
       "         [-2.4834e-01,  3.5618e-01, -1.8914e+00,  3.3280e-01],\n",
       "         [-1.1993e+00, -6.7687e-01,  6.6309e-01, -6.3695e-01],\n",
       "         [ 4.1149e-02, -1.8906e+00, -7.4173e+00, -1.8766e+00],\n",
       "         [-9.8579e-01,  1.0869e+00,  1.6673e+00,  5.7690e-01],\n",
       "         [ 9.5711e-01,  1.6833e-01,  5.6043e-01,  3.7882e-01],\n",
       "         [-1.5834e+00, -8.9660e-01,  7.4962e+00,  2.6119e+00],\n",
       "         [ 1.0654e-02,  1.1045e+00,  3.2932e+00,  1.0957e+00],\n",
       "         [-8.7981e-01, -1.5747e+00,  6.1326e+00,  3.8232e+00],\n",
       "         [ 6.5508e-01, -1.7383e+00, -7.5128e+00, -1.8467e+00],\n",
       "         [-1.4266e+00,  2.6727e-01,  1.1462e+01,  2.7812e+00],\n",
       "         [-2.0198e-01, -1.9209e+00, -7.2255e+00, -1.8872e+00],\n",
       "         [-3.2611e-02, -2.8132e+00, -6.2025e+00, -2.0255e+00],\n",
       "         [ 5.7684e-02, -1.8475e+00, -7.3231e+00, -1.6391e+00],\n",
       "         [-1.0491e-01,  2.3847e+00,  8.2366e+00,  1.7275e+00],\n",
       "         [ 2.7443e-01,  1.0660e+00,  3.0955e+00,  9.7804e-01],\n",
       "         [ 3.5806e-01, -1.5627e+00, -7.2461e+00, -1.5755e+00],\n",
       "         [-1.3487e+00, -2.1421e-01, -4.3625e-01, -3.3265e-01],\n",
       "         [ 1.7334e+00, -5.9706e-02,  8.2388e-02,  3.6715e-01],\n",
       "         [ 8.2479e-01, -1.2540e+00,  1.5659e-01, -6.3465e-01],\n",
       "         [-1.0864e+00, -8.5123e-01, -3.4547e+00, -6.9856e-01],\n",
       "         [ 1.0264e+00,  2.7626e+00,  5.8212e+00,  7.4101e-01],\n",
       "         [-5.2313e-01,  9.1578e-01,  7.3815e+00,  2.6508e+00],\n",
       "         [-2.8595e-02,  2.4909e+00,  7.8389e+00,  1.4267e+00],\n",
       "         [-6.0584e-01, -2.6065e+00, -7.2348e+00, -1.6153e+00],\n",
       "         [-1.0760e+00,  1.7662e+00,  1.5987e+00,  1.3408e+00],\n",
       "         [ 1.0217e+00, -8.1517e-01, -6.1894e-01, -4.7495e-01],\n",
       "         [-8.9669e-01,  9.3600e-01,  2.4260e+00,  5.1863e-01],\n",
       "         [-1.1161e+00,  3.4336e-01, -4.7467e-01, -1.1677e-01],\n",
       "         [ 1.0126e-01,  1.0376e-02,  2.1141e+00,  2.6268e-01]]),\n",
       " 'mlp_extractor.shared_net.0.bias': tensor([-4.5893,  2.0968,  4.7204,  4.9689,  4.5581,  2.5609,  4.4523, -5.2170,\n",
       "          0.3272, -4.5535,  0.7295,  2.5524,  0.4202,  2.5740, -0.5442,  2.6157,\n",
       "         -2.1062, -4.1884,  2.7854,  4.8183, -4.7428, -4.4489,  4.5142, -1.5798,\n",
       "          0.7628, -2.1778,  1.6258,  4.4135, -5.2737,  4.4511,  5.1679, -5.1619]),\n",
       " 'action_net.weight': tensor([[ 1.5881, -1.6859,  0.3036, -0.4882, -1.9233,  1.2867, -1.8561,  1.5905,\n",
       "          -5.7599, -0.4132, -5.7335,  1.5162, -9.6718,  1.9795,  2.6554,  1.0137,\n",
       "          -2.6042,  0.2802,  1.0110, -1.0336,  0.9435,  0.0110,  0.2500, -2.6360,\n",
       "          -4.2083, -2.1074,  2.4692, -2.0390,  1.2021, -2.0540, -1.0552, -0.4659],\n",
       "         [-1.5830,  1.6861, -0.3065,  0.4902,  1.9231, -1.2832,  1.8545, -1.5895,\n",
       "           5.7613,  0.4146,  5.7331, -1.5124,  9.6677, -1.9821, -2.6558, -1.0111,\n",
       "           2.6018, -0.2753, -1.0108,  1.0351, -0.9454, -0.0123, -0.2518,  2.6368,\n",
       "           4.2085,  2.1068, -2.4699,  2.0456, -1.1999,  2.0549,  1.0568,  0.4645]]),\n",
       " 'action_net.bias': tensor([ 0.0730, -0.0730])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include only variables with \"policy\", \"action\" (policy) or \"shared_net\" (shared layers)\n",
    "# in their name: only these ones affect the action.\n",
    "# NOTE: you can retrieve those parameters using model.get_parameters() too\n",
    "mean_params = dict(\n",
    "    (key, value)\n",
    "    for key, value in model.policy.state_dict().items()\n",
    "    if (\"policy\" in key or \"shared_net\" in key or \"action\" in key)\n",
    ")\n",
    "\n",
    "mean_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Population Size and Retrieve Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population size of 50 invdiduals\n",
    "pop_size = 50\n",
    "# Keep top 10%\n",
    "n_elite = pop_size // 10\n",
    "# Retrieve the environment\n",
    "env = model.get_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through Mutated Policy Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1   Mean top fitness: 221.44\n",
      "Best fitness: 305.20\n",
      "Iteration 2   Mean top fitness: 494.48\n",
      "Best fitness: 500.00\n",
      "Iteration 3   Mean top fitness: 500.00\n",
      "Best fitness: 500.00\n",
      "Iteration 4   Mean top fitness: 500.00\n",
      "Best fitness: 500.00\n",
      "Iteration 5   Mean top fitness: 500.00\n",
      "Best fitness: 500.00\n",
      "Iteration 6   Mean top fitness: 500.00\n",
      "Best fitness: 500.00\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10):\n",
    "    # Create population of candidates and evaluate them\n",
    "    population = []\n",
    "    for population_i in range(pop_size):\n",
    "        candidate = mutate(mean_params)\n",
    "        # Load new policy parameters to agent.\n",
    "        # Tell function that it should only update parameters\n",
    "        # we give it (policy parameters)\n",
    "        model.policy.load_state_dict(candidate, strict=False)\n",
    "        # Evaluate the candidate\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        population.append((candidate, fitness))\n",
    "    # Take top 10% and use average over their parameters as next mean parameter\n",
    "    top_candidates = sorted(population, key=lambda x: x[1], reverse=True)[:n_elite]\n",
    "    mean_params = dict(\n",
    "        (\n",
    "            name,\n",
    "            th.stack([candidate[0][name] for candidate in top_candidates]).mean(dim=0),\n",
    "        )\n",
    "        for name in mean_params.keys()\n",
    "    )\n",
    "    mean_fitness = sum(top_candidate[1] for top_candidate in top_candidates) / n_elite\n",
    "    print(f\"Iteration {iteration + 1:<3} Mean top fitness: {mean_fitness:.2f}\")\n",
    "    print(f\"Best fitness: {top_candidates[0][1]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
