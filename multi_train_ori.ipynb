{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Train Gradient Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import threading\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from stable_baselines3 import PPO as ALGO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "NUM_CLIENT_MODELS = 4\n",
    "NUM_TRAINING_STEPS = 1000\n",
    "NUM_ITERATIONS = 10\n",
    "ENV_NAME = 'CartPole-v1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init. ENV and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "global_model = ALGO(\n",
    "    \"MlpPolicy\",\n",
    "    env\n",
    ")\n",
    "\n",
    "client_models = [ALGO(\"MlpPolicy\", gym.make(ENV_NAME)) for i in range(NUM_CLIENT_MODELS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Evaluate Model and Train Model within Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, message = '', verbose = 0):\n",
    "    # print('Starting Eval')\n",
    "    fitnesses = []\n",
    "    iterations = 10\n",
    "    for i in range(iterations):\n",
    "        fitness, _ = evaluate_policy(model, env)\n",
    "        if verbose == 1:\n",
    "            print(i, fitness, end=\" \")\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    mean_fitness = np.mean(sorted(fitnesses))\n",
    "    print(f'Type {message} Mean reward: {mean_fitness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, index, timesteps):\n",
    "    # print('Starting Training')\n",
    "    models[index] = models[index].learn(reset_num_timesteps=False, total_timesteps=timesteps)\n",
    "    # print('Completed Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_eval(client_models):\n",
    "    # Create Threads\n",
    "    client_threads = [] \n",
    "    for ci in range(NUM_CLIENT_MODELS):\n",
    "        thread = threading.Thread(target=evaluate, args=(client_models[ci], gym.make(ENV_NAME), f'Trained Model {ci}'))\n",
    "        client_threads.append(thread)\n",
    "\n",
    "    # Start Threads\n",
    "    for thread in client_threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Join Threads (wait until thread is completely executed)\n",
    "    for thread in client_threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/.miniconda3/envs/pydrl/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  Mean reward: 9.27\n",
      "Type Trained Model 1 Mean reward: 9.28\n",
      "Type Trained Model 3 Mean reward: 9.459999999999999\n",
      "Type Trained Model 0 Mean reward: 9.3\n",
      "Type Trained Model 2 Mean reward: 9.4\n"
     ]
    }
   ],
   "source": [
    "for model in client_models:\n",
    "    model.set_parameters(global_model.get_parameters())\n",
    "\n",
    "global_model.save('initial')\n",
    "\n",
    "evaluate(global_model, env)\n",
    "\n",
    "multithread_eval(client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Global Initial Model Mean reward: 9.360000000000001\n",
      "Train Iter:  0\n",
      "Optim Steps:  320\n",
      "Type Trained Model 0 Mean reward: 86.98\n",
      "Type Trained Model 3 Mean reward: 100.82\n",
      "Type Trained Model 2 Mean reward: 192.35999999999999\n",
      "Type Trained Model 1 Mean reward: 332.11\n",
      "Type Global Updated Model Mean reward: 148.93\n",
      "Train Iter:  1\n",
      "Optim Steps:  640\n",
      "Type Trained Model 1 Mean reward: 147.87\n",
      "Type Trained Model 2 Mean reward: 173.61\n",
      "Type Trained Model 3 Mean reward: 175.47\n",
      "Type Trained Model 0 Mean reward: 277.65\n",
      "Type Global Updated Model Mean reward: 174.96\n",
      "Train Iter:  2\n",
      "Optim Steps:  960\n",
      "Type Trained Model 2 Mean reward: 305.57\n",
      "Type Trained Model 1 Mean reward: 326.74\n",
      "Type Trained Model 3 Mean reward: 329.68\n",
      "Type Trained Model 0 Mean reward: 333.05\n",
      "Type Global Updated Model Mean reward: 350.71\n",
      "Train Iter:  3\n",
      "Optim Steps:  1280\n",
      "Type Trained Model 3 Mean reward: 375.85\n",
      "Type Trained Model 0 Mean reward: 390.54\n",
      "Type Trained Model 2 Mean reward: 395.01000000000005\n",
      "Type Trained Model 1 Mean reward: 408.05\n",
      "Type Global Updated Model Mean reward: 398.0799999999999\n",
      "Train Iter:  4\n",
      "Optim Steps:  1600\n",
      "Type Trained Model 0 Mean reward: 357.76\n",
      "Type Trained Model 3 Mean reward: 407.21000000000004\n",
      "Type Trained Model 1 Mean reward: 412.28000000000003\n",
      "Type Trained Model 2 Mean reward: 420.74000000000007\n",
      "Type Global Updated Model Mean reward: 406.01\n",
      "Train Iter:  5\n",
      "Optim Steps:  1920\n",
      "Type Trained Model 3 Mean reward: 395.44\n",
      "Type Trained Model 2 Mean reward: 412.77\n",
      "Type Trained Model 1 Mean reward: 445.16\n",
      "Type Trained Model 0 Mean reward: 454.1\n",
      "Type Global Updated Model Mean reward: 442.64000000000004\n",
      "Train Iter:  6\n",
      "Optim Steps:  2240\n",
      "Type Trained Model 2 Mean reward: 463.77\n",
      "Type Trained Model 1 Mean reward: 474.41999999999996\n",
      "Type Trained Model 0 Mean reward: 479.99000000000007\n",
      "Type Trained Model 3 Mean reward: 484.6\n",
      "Type Global Updated Model Mean reward: 473.82000000000005\n",
      "Train Iter:  7\n",
      "Optim Steps:  2560\n",
      "Type Trained Model 3 Mean reward: 467.38\n",
      "Type Trained Model 1 Mean reward: 495.25\n",
      "Type Trained Model 2 Mean reward: 499.31000000000006\n",
      "Type Trained Model 0 Mean reward: 499.91999999999996\n",
      "Type Global Updated Model Mean reward: 495.59\n",
      "Train Iter:  8\n",
      "Optim Steps:  2880\n",
      "Type Trained Model 1 Mean reward: 476.14\n",
      "Type Trained Model 2 Mean reward: 490.03000000000003\n",
      "Type Trained Model 0 Mean reward: 500.0\n",
      "Type Trained Model 3 Mean reward: 500.0\n",
      "Type Global Updated Model Mean reward: 500.0\n",
      "Train Iter:  9\n",
      "Optim Steps:  3200\n",
      "Type Trained Model 0 Mean reward: 499.21999999999997\n",
      "Type Trained Model 1 Mean reward: 500.0\n",
      "Type Trained Model 3 Mean reward: 500.0\n",
      "Type Trained Model 2 Mean reward: 500.0\n",
      "Type Global Updated Model Mean reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Before Iterated Training\n",
    "evaluate(global_model, env, \"Global Initial Model\")\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    print('Train Iter: ', i)\n",
    "\n",
    "    # Create Threads\n",
    "    client_threads = [] \n",
    "    for ci in range(NUM_CLIENT_MODELS):\n",
    "        thread = threading.Thread(target=train, args=(client_models, ci, NUM_TRAINING_STEPS))\n",
    "        client_threads.append(thread)\n",
    "\n",
    "\n",
    "    # Start Threads\n",
    "    for thread in client_threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Join Threads (wait until thread is completely executed)\n",
    "    for thread in client_threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Optimization Steps Check\n",
    "    print('Optim Steps: ', client_models[0].get_parameters()['policy.optimizer']['state'][0]['step'])\n",
    "\n",
    "    # Evaluation after Training\n",
    "    multithread_eval(client_models)\n",
    "\n",
    "    # Accumulate Client Parameters / Weights\n",
    "    global_dict = global_model.policy.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].policy.state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "\n",
    "    # Load New Parameters to Global Model\n",
    "    global_model.policy.load_state_dict(global_dict)\n",
    "\n",
    "    # Load New Parameters to clients\n",
    "    for model in client_models:\n",
    "        model.policy.load_state_dict(global_model.policy.state_dict())\n",
    "\n",
    "    # Evaluate Updated Global Model\n",
    "    evaluate(model, env, 'Global Updated Model', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': OrderedDict([('mlp_extractor.policy_net.0.weight',\n",
       "               tensor([[-0.1020, -0.0423, -0.1422,  0.2610],\n",
       "                       [-0.0262,  0.1690, -0.1342, -0.2009],\n",
       "                       [ 0.0452,  0.2361, -0.1432,  0.1706],\n",
       "                       [ 0.1497, -0.2435, -0.3376, -0.0246],\n",
       "                       [-0.0697,  0.0147,  0.4047,  0.2382],\n",
       "                       [-0.0320,  0.1160, -0.3323, -0.4182],\n",
       "                       [-0.3215,  0.2539,  0.1269,  0.1439],\n",
       "                       [-0.1777,  0.2547,  0.2744, -0.0048],\n",
       "                       [-0.2374,  0.0122, -0.1653,  0.2059],\n",
       "                       [-0.0441,  0.1529, -0.2613, -0.3425],\n",
       "                       [-0.1478, -0.0044, -0.4353, -0.2480],\n",
       "                       [ 0.0151, -0.2379, -0.4968, -0.1327],\n",
       "                       [-0.0747,  0.1341, -0.1716, -0.0738],\n",
       "                       [-0.4569, -0.0599, -0.4047, -0.4025],\n",
       "                       [ 0.1470, -0.2175, -0.3141, -0.3726],\n",
       "                       [ 0.2908,  0.1527,  0.2571,  0.2889],\n",
       "                       [-0.1157, -0.0516, -0.1145,  0.0432],\n",
       "                       [-0.1393, -0.1367,  0.3366,  0.2457],\n",
       "                       [-0.3749,  0.3589,  0.1762, -0.1553],\n",
       "                       [ 0.0742,  0.0933, -0.2697,  0.2769],\n",
       "                       [-0.4023,  0.2369,  0.1647,  0.1953],\n",
       "                       [-0.1148,  0.1260, -0.1311,  0.1143],\n",
       "                       [ 0.0163,  0.0287, -0.0385, -0.4597],\n",
       "                       [ 0.0579,  0.1357,  0.4167,  0.3966],\n",
       "                       [-0.2483, -0.3336, -0.2480, -0.3670],\n",
       "                       [-0.2537, -0.2286,  0.0616,  0.3784],\n",
       "                       [ 0.1164, -0.2176, -0.4254, -0.2886],\n",
       "                       [ 0.1244, -0.2846, -0.2277, -0.3467],\n",
       "                       [ 0.1630, -0.1762, -0.2112, -0.2521],\n",
       "                       [-0.0073, -0.2249,  0.1819,  0.2791],\n",
       "                       [ 0.2285,  0.2063, -0.2969, -0.0955],\n",
       "                       [-0.0329, -0.1313, -0.2058,  0.0383],\n",
       "                       [ 0.2841,  0.1265, -0.3747, -0.5741],\n",
       "                       [-0.2821, -0.0398, -0.0892, -0.1300],\n",
       "                       [ 0.3034,  0.0208, -0.4658, -0.2229],\n",
       "                       [ 0.1040,  0.4813,  0.2206, -0.0039],\n",
       "                       [ 0.0131, -0.1925, -0.1222, -0.1393],\n",
       "                       [-0.1740,  0.0728,  0.5123, -0.1389],\n",
       "                       [-0.0435,  0.5398, -0.4409, -0.3004],\n",
       "                       [-0.2567, -0.3303,  0.1437,  0.0159],\n",
       "                       [-0.5430, -0.1199, -0.3880, -0.2677],\n",
       "                       [-0.0054, -0.1619,  0.0693,  0.0274],\n",
       "                       [ 0.0700, -0.0604, -0.4724,  0.1799],\n",
       "                       [ 0.3000, -0.0300, -0.2507, -0.0263],\n",
       "                       [ 0.0964,  0.0207, -0.4961, -0.2117],\n",
       "                       [-0.2100, -0.0365, -0.4283, -0.0015],\n",
       "                       [-0.2111,  0.0603,  0.3719, -0.0762],\n",
       "                       [ 0.1010, -0.0915,  0.2683,  0.2584],\n",
       "                       [-0.2586, -0.1955,  0.1097, -0.0232],\n",
       "                       [-0.0932,  0.2743,  0.0073, -0.0291],\n",
       "                       [ 0.0347,  0.2317,  0.1868, -0.1831],\n",
       "                       [ 0.0798,  0.0240,  0.3255,  0.0026],\n",
       "                       [ 0.0791,  0.3350,  0.2147,  0.0225],\n",
       "                       [ 0.2445,  0.0505,  0.5738,  0.5088],\n",
       "                       [-0.0497, -0.0069, -0.3299, -0.3989],\n",
       "                       [-0.0371, -0.0812, -0.2385, -0.2706],\n",
       "                       [ 0.2409,  0.2759,  0.3348,  0.3838],\n",
       "                       [ 0.0244,  0.0454,  0.3045, -0.1469],\n",
       "                       [-0.2387, -0.1049,  0.2041,  0.0375],\n",
       "                       [ 0.0398, -0.0328,  0.1775,  0.3200],\n",
       "                       [-0.0995, -0.0839,  0.3721,  0.4807],\n",
       "                       [ 0.0563, -0.2540, -0.3471, -0.5976],\n",
       "                       [ 0.0336, -0.2547,  0.0371, -0.3918],\n",
       "                       [ 0.1299,  0.0522,  0.0201, -0.3033]])),\n",
       "              ('mlp_extractor.policy_net.0.bias',\n",
       "               tensor([-8.0689e-03, -2.3515e-02, -1.2556e-02,  8.2421e-03,  7.7167e-03,\n",
       "                        7.7116e-03, -6.9641e-03, -6.4257e-03,  2.6112e-03,  3.4963e-03,\n",
       "                       -1.1289e-03,  9.9820e-03, -2.1631e-02, -8.6807e-03, -1.3466e-02,\n",
       "                        2.0346e-02,  4.0577e-03,  2.1332e-02, -1.1606e-02,  1.9334e-02,\n",
       "                        1.5055e-02,  1.4235e-02, -2.2984e-03, -1.6382e-03, -1.2484e-02,\n",
       "                       -3.5902e-03, -3.0828e-03,  2.1827e-03, -1.2112e-02,  8.7851e-03,\n",
       "                        2.0676e-03,  2.8574e-03,  5.0662e-03, -2.1777e-02, -4.2429e-03,\n",
       "                       -6.3231e-03, -2.9656e-03, -6.1697e-03,  1.0067e-02,  1.0066e-02,\n",
       "                       -1.8085e-02,  1.0741e-02,  7.1252e-03, -1.5972e-05,  4.4039e-03,\n",
       "                       -2.3113e-02, -6.3964e-03, -1.2796e-03,  4.0560e-03,  1.3757e-03,\n",
       "                       -1.5882e-02, -7.5019e-03, -1.8774e-03,  2.9314e-04,  4.6733e-03,\n",
       "                        3.4544e-03,  6.3345e-03, -2.1086e-02, -6.3884e-04,  1.6227e-02,\n",
       "                       -6.5833e-03, -3.0973e-03, -8.9400e-03,  1.4130e-03])),\n",
       "              ('mlp_extractor.policy_net.2.weight',\n",
       "               tensor([[-0.3249,  0.0142, -0.1431,  ..., -0.1753, -0.0034, -0.0650],\n",
       "                       [-0.0946,  0.1644, -0.2042,  ...,  0.2890, -0.2758, -0.2882],\n",
       "                       [ 0.3850,  0.1023, -0.1639,  ...,  0.3413,  0.1838,  0.3167],\n",
       "                       ...,\n",
       "                       [-0.2298, -0.0223,  0.0378,  ..., -0.0574,  0.1380,  0.1265],\n",
       "                       [-0.1817, -0.0348, -0.2182,  ...,  0.1452, -0.0587, -0.2764],\n",
       "                       [ 0.0371, -0.0110,  0.0487,  ..., -0.1594, -0.1572, -0.1474]])),\n",
       "              ('mlp_extractor.policy_net.2.bias',\n",
       "               tensor([ 0.0128,  0.0093,  0.0018,  0.0011,  0.0004,  0.0022, -0.0035, -0.0002,\n",
       "                       -0.0148,  0.0009,  0.0130,  0.0056,  0.0044, -0.0072, -0.0044, -0.0072,\n",
       "                       -0.0031, -0.0063,  0.0133,  0.0120,  0.0140,  0.0023, -0.0051,  0.0117,\n",
       "                       -0.0007, -0.0135, -0.0045, -0.0025, -0.0013, -0.0139,  0.0004,  0.0109,\n",
       "                       -0.0031,  0.0132, -0.0037,  0.0146, -0.0098,  0.0036,  0.0051,  0.0019,\n",
       "                        0.0065, -0.0060, -0.0079,  0.0100,  0.0126, -0.0123, -0.0017,  0.0062,\n",
       "                       -0.0127,  0.0132, -0.0132, -0.0006,  0.0037,  0.0003, -0.0033, -0.0101,\n",
       "                        0.0127,  0.0011,  0.0075, -0.0055,  0.0081,  0.0168, -0.0091, -0.0094])),\n",
       "              ('mlp_extractor.value_net.0.weight',\n",
       "               tensor([[-0.2651, -0.0562,  0.1463,  0.0597],\n",
       "                       [-0.2966,  0.2185, -0.2458, -0.0899],\n",
       "                       [-0.1814, -0.1066,  0.4134,  0.2012],\n",
       "                       [ 0.2307,  0.3527,  0.1577,  0.1050],\n",
       "                       [ 0.0888, -0.1598,  0.2832,  0.2226],\n",
       "                       [-0.2475, -0.0112, -0.1839, -0.1297],\n",
       "                       [ 0.2603, -0.2421,  0.8254, -0.0272],\n",
       "                       [ 0.3689,  0.1957,  0.6151,  0.1678],\n",
       "                       [ 0.1388, -0.1004, -0.4630, -0.4756],\n",
       "                       [-0.2422,  0.0268, -0.5165, -0.1085],\n",
       "                       [ 0.0538,  0.1756,  0.2720,  0.1555],\n",
       "                       [ 0.3038,  0.2426,  0.1502, -0.0315],\n",
       "                       [-0.0380,  0.0985, -0.1654, -0.0177],\n",
       "                       [ 0.0600,  0.0674,  0.2041, -0.0948],\n",
       "                       [-0.3232,  0.1015,  0.4891,  0.3266],\n",
       "                       [ 0.1110,  0.0983, -0.2410, -0.2525],\n",
       "                       [ 0.1859, -0.1499,  0.6281,  0.4234],\n",
       "                       [ 0.2539,  0.2285,  0.5238,  0.0043],\n",
       "                       [ 0.2440,  0.1259,  0.4748, -0.0169],\n",
       "                       [ 0.5159,  0.0733, -0.2244, -0.1074],\n",
       "                       [ 0.0466,  0.3147,  0.5410,  0.3480],\n",
       "                       [ 0.3910,  0.1040,  0.1239,  0.0822],\n",
       "                       [ 0.4009,  0.1278,  0.3629, -0.1106],\n",
       "                       [-0.0651, -0.2851, -0.6224, -0.2759],\n",
       "                       [-0.1728,  0.0396,  0.1955,  0.5751],\n",
       "                       [ 0.5023, -0.0530,  0.4312,  0.1054],\n",
       "                       [ 0.3269,  0.1327,  0.4581,  0.5341],\n",
       "                       [-0.1938, -0.2970, -0.6057, -0.1384],\n",
       "                       [ 0.1352,  0.4191,  0.4441,  0.2284],\n",
       "                       [ 0.0139, -0.1696, -0.6976, -0.3065],\n",
       "                       [-0.3917,  0.0663,  0.1841, -0.0783],\n",
       "                       [-0.0681, -0.0674,  0.4932,  0.4364],\n",
       "                       [-0.1123,  0.3021,  0.2970,  0.1822],\n",
       "                       [-0.1492,  0.3286,  0.0016,  0.3064],\n",
       "                       [-0.1159, -0.1065, -0.5462, -0.0039],\n",
       "                       [ 0.2966,  0.1474, -0.1792, -0.1159],\n",
       "                       [-0.1789, -0.0657, -0.0108,  0.1473],\n",
       "                       [-0.1518, -0.1381, -0.5718, -0.1314],\n",
       "                       [ 0.0188, -0.0109,  0.2311,  0.2843],\n",
       "                       [-0.2897, -0.1925,  0.2462,  0.2187],\n",
       "                       [ 0.1148,  0.4402,  0.5181, -0.0100],\n",
       "                       [ 0.1066,  0.0172,  0.7697,  0.1207],\n",
       "                       [ 0.1393,  0.3655, -0.3613, -0.0874],\n",
       "                       [ 0.4657,  0.1019,  0.1088,  0.0848],\n",
       "                       [ 0.0696,  0.0391,  0.4040, -0.1044],\n",
       "                       [ 0.1742, -0.0297,  0.8664, -0.0722],\n",
       "                       [ 0.0343, -0.1383,  0.1588,  0.1998],\n",
       "                       [-0.2273, -0.4163, -0.5845, -0.2954],\n",
       "                       [ 0.1921, -0.3783,  0.4567,  0.2166],\n",
       "                       [-0.2880, -0.1132, -0.5002, -0.4011],\n",
       "                       [-0.1076,  0.1193, -0.1093, -0.0462],\n",
       "                       [-0.0530,  0.1420, -0.4420, -0.3115],\n",
       "                       [ 0.2484,  0.1537,  0.5216,  0.3327],\n",
       "                       [ 0.3416,  0.2379,  0.7479,  0.2918],\n",
       "                       [-0.3601, -0.0855, -0.1872,  0.0366],\n",
       "                       [-0.3667,  0.1248,  0.5795,  0.4254],\n",
       "                       [-0.4925, -0.1253, -0.1678, -0.3004],\n",
       "                       [ 0.1338,  0.3987,  0.6048, -0.0259],\n",
       "                       [ 0.0447, -0.2421, -0.1687, -0.0407],\n",
       "                       [-0.0938,  0.0017,  0.1572,  0.1932],\n",
       "                       [ 0.0667,  0.0736,  0.2730, -0.1328],\n",
       "                       [-0.1793, -0.3530, -0.4266,  0.1455],\n",
       "                       [ 0.2207,  0.0166, -0.7687, -0.2418],\n",
       "                       [-0.2126, -0.2409, -0.2881, -0.2191]])),\n",
       "              ('mlp_extractor.value_net.0.bias',\n",
       "               tensor([-0.2879, -0.2588,  0.2209, -0.3062, -0.2812,  0.2576,  0.2344, -0.2519,\n",
       "                        0.3142,  0.2547,  0.2726,  0.3018,  0.2549,  0.2404, -0.0408, -0.2264,\n",
       "                       -0.2514,  0.1532, -0.2833,  0.0518, -0.3037,  0.2617,  0.3081,  0.2202,\n",
       "                       -0.2992,  0.3166,  0.1948, -0.1243,  0.1310, -0.2490,  0.2491, -0.2877,\n",
       "                        0.2495,  0.2308, -0.2547,  0.3117, -0.2142,  0.2250,  0.2837,  0.2716,\n",
       "                        0.1429,  0.2426, -0.2660, -0.3287,  0.2618,  0.1545,  0.2671,  0.1054,\n",
       "                        0.2576, -0.2339, -0.2734,  0.2787, -0.3059, -0.0827,  0.2824,  0.2770,\n",
       "                       -0.2443,  0.1460, -0.2173,  0.2792, -0.2587, -0.1249, -0.2392,  0.2837])),\n",
       "              ('mlp_extractor.value_net.2.weight',\n",
       "               tensor([[-0.0104, -0.3701,  0.4642,  ...,  0.1800, -0.4790,  0.2001],\n",
       "                       [ 0.1639,  0.4087, -0.2689,  ...,  0.3923,  0.1893,  0.1103],\n",
       "                       [ 0.4423,  0.3610, -0.5837,  ...,  0.0626,  0.0619, -0.4101],\n",
       "                       ...,\n",
       "                       [-0.0350, -0.1645, -0.3909,  ...,  0.1935,  0.8518, -0.3737],\n",
       "                       [-0.1383, -0.0470, -0.0306,  ...,  0.0648,  0.0632,  0.8114],\n",
       "                       [ 0.6062,  0.3247, -0.2733,  ...,  0.0890,  0.4786, -0.1033]])),\n",
       "              ('mlp_extractor.value_net.2.bias',\n",
       "               tensor([ 0.1109, -0.1674, -0.1904, -0.1323, -0.2010, -0.1693,  0.1685, -0.1819,\n",
       "                        0.1107, -0.1009,  0.1284, -0.1484,  0.1363, -0.1107, -0.1795,  0.1469,\n",
       "                        0.1525, -0.1545,  0.1536,  0.1227,  0.1593,  0.1619,  0.1690,  0.1184,\n",
       "                       -0.2275,  0.1073, -0.1506, -0.1295, -0.1698,  0.0945,  0.1364, -0.1950,\n",
       "                       -0.1698, -0.1502,  0.0931,  0.1467,  0.0850,  0.1264, -0.1298,  0.1401,\n",
       "                        0.1419, -0.2308, -0.1416, -0.1864, -0.1457, -0.1388, -0.1900, -0.1498,\n",
       "                        0.1570,  0.1251, -0.1833, -0.1894,  0.2917, -0.1446, -0.1884, -0.1836,\n",
       "                        0.1277,  0.1320,  0.1655,  0.1586,  0.2260, -0.1075,  0.1560, -0.1327])),\n",
       "              ('action_net.weight',\n",
       "               tensor([[ 0.0954,  0.0171,  0.1108, -0.0083, -0.1235, -0.0854, -0.0267, -0.1155,\n",
       "                        -0.1326, -0.0504,  0.0423,  0.0223,  0.0362, -0.0184, -0.0239, -0.0754,\n",
       "                        -0.0234, -0.0911,  0.0390,  0.0993,  0.0878,  0.0692, -0.0073,  0.0931,\n",
       "                        -0.0196, -0.0868, -0.0488, -0.0109, -0.0604, -0.0278,  0.0102,  0.0271,\n",
       "                         0.0236,  0.0214, -0.0345,  0.0415, -0.0659,  0.1011, -0.1154,  0.0654,\n",
       "                         0.0215,  0.0113, -0.0824,  0.0245,  0.0314, -0.0981, -0.0502,  0.0435,\n",
       "                        -0.0754,  0.0210, -0.0442,  0.0170, -0.0103,  0.0701, -0.0187, -0.0221,\n",
       "                         0.0434, -0.0577,  0.0640, -0.0512,  0.0239,  0.0901, -0.0216, -0.0686],\n",
       "                       [-0.0950, -0.0178, -0.1089,  0.0081,  0.1247,  0.0817,  0.0264,  0.1144,\n",
       "                         0.1293,  0.0485, -0.0447, -0.0231, -0.0375,  0.0189,  0.0263,  0.0733,\n",
       "                         0.0206,  0.0904, -0.0369, -0.1010, -0.0862, -0.0721,  0.0054, -0.0922,\n",
       "                         0.0203,  0.0882,  0.0489,  0.0124,  0.0614,  0.0291, -0.0082, -0.0236,\n",
       "                        -0.0256, -0.0255,  0.0348, -0.0408,  0.0680, -0.1020,  0.1167, -0.0657,\n",
       "                        -0.0237, -0.0083,  0.0815, -0.0216, -0.0307,  0.0991,  0.0503, -0.0429,\n",
       "                         0.0754, -0.0223,  0.0428, -0.0177,  0.0113, -0.0706,  0.0197,  0.0220,\n",
       "                        -0.0446,  0.0593, -0.0628,  0.0501, -0.0238, -0.0941,  0.0241,  0.0665]])),\n",
       "              ('action_net.bias', tensor([ 0.0139, -0.0139])),\n",
       "              ('value_net.weight',\n",
       "               tensor([[ 1.0202, -0.9497, -0.8406, -1.0738, -1.0216, -0.9515,  1.0179, -0.9739,\n",
       "                         1.0149, -1.1442,  1.0059, -0.9796,  0.9721, -1.1916, -0.9389,  0.9998,\n",
       "                         0.9295, -0.9889,  0.9631,  1.0372,  0.9846,  0.9949,  0.9557,  1.0415,\n",
       "                        -0.9690,  1.1154, -1.0924, -1.0362, -1.0015,  1.1228,  1.0029, -1.0098,\n",
       "                        -0.9979, -1.0992,  1.1623,  1.0894,  1.3194,  1.0328, -1.0114,  1.0285,\n",
       "                         1.0706, -0.9690, -0.9411, -1.0772, -0.9764, -1.0749, -0.9469, -1.0057,\n",
       "                         0.7926,  1.2111, -0.9162, -0.9370,  0.9512, -1.0824, -1.0189, -0.9686,\n",
       "                         1.0815,  0.9720,  0.9390,  0.9423,  0.8095, -1.0483,  1.0782, -1.1002]])),\n",
       "              ('value_net.bias', tensor([0.5799]))]),\n",
       " 'policy.optimizer': {'state': {},\n",
       "  'param_groups': [{'lr': 0.0003,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-05,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save('a2c_lunar_multiproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Params as JSON\n",
    "## Function to Convert Params Dict to Flattened List\n",
    "def flatten_list(params):\n",
    "    \"\"\"\n",
    "    :param params: (dict)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    params_ = {}\n",
    "    for key in params.keys():\n",
    "        params_[key] = params[key].tolist()\n",
    "    return params_\n",
    "## Write Parameters to JSON File\n",
    "import json\n",
    "\n",
    "all_params = global_model.get_parameters()\n",
    "pol_params = flatten_list(all_params['policy'])\n",
    "\n",
    "all_params['policy'] = pol_params\n",
    "\n",
    "with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "    json.dump(all_params, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loaded = ALGO(\n",
    "#     \"MlpPolicy\",\n",
    "#     env\n",
    "# )\n",
    "\n",
    "# evaluate(model_loaded,env, verbose=1)\n",
    "\n",
    "# import json\n",
    "# with open('a2c_lunar_multiproc.json', 'w') as f:\n",
    "#     new_params = json.load(f)\n",
    "\n",
    "# loaded_pol_params = new_params['policy']\n",
    "# for key in loaded_pol_params.keys():\n",
    "#     loaded_pol_params[key] = th.tensor(loaded_pol_params[key])\n",
    "\n",
    "# new_params['policy'] = loaded_pol_params\n",
    "\n",
    "# model_loaded.set_parameters(new_params)\n",
    "\n",
    "model_loaded = ALGO.load('a2c_lunar_multiproc', env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500.0 1 500.0 2 500.0 3 500.0 4 500.0 5 500.0 6 500.0 7 500.0 8 500.0 9 500.0 Type  Mean reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "evaluate(model_loaded,env, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe87c7677a9be80aab770929aa8f3d40850ac08a0f73ec246342c77c48f1c11"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('pydrl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
